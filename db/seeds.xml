<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%26id_list%3D%26start%3D0%26max_results%3D1000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=cat:cs.AI&amp;id_list=&amp;start=0&amp;max_results=1000</title>
  <id>http://arxiv.org/api/TUfTqPVG50h6UhBPr/wMFJTn8rQ</id>
  <updated>2016-09-16T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">9431</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/1609.04722v1</id>
    <updated>2016-09-15T16:24:45Z</updated>
    <published>2016-09-15T16:24:45Z</published>
    <title>Concordance and the Smallest Covering Set of Preference Orderings</title>
    <summary>  Preference orderings are orderings of a set of items according to the
preferences (of judges). Such orderings arise in a variety of domains,
including group decision making, consumer marketing, voting and machine
learning. Measuring the mutual information and extracting the common patterns
in a set of preference orderings are key to these areas. In this paper we deal
with the representation of sets of preference orderings, the quantification of
the degree to which judges agree on their ordering of the items (i.e. the
concordance), and the efficient, meaningful description of such sets.
  We propose to represent the orderings in a subsequence-based feature space
and present a new algorithm to calculate the size of the set of all common
subsequences - the basis of a quantification of concordance, not only for pairs
of orderings but also for sets of orderings. The new algorithm is fast and
storage efficient with a time complexity of only $O(Nn^2)$ for the orderings of
$n$ items by $N$ judges and a space complexity of only $O(\min\{Nn,n^2\})$.
  Also, we propose to represent the set of all $N$ orderings through a smallest
set of covering preferences and present an algorithm to construct this smallest
covering set.
</summary>
    <author>
      <name>Zhiwei Lin</name>
    </author>
    <author>
      <name>Hui Wang</name>
    </author>
    <author>
      <name>Cees H. Elzinga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04628v1</id>
    <updated>2016-09-15T13:23:43Z</updated>
    <published>2016-09-15T13:23:43Z</published>
    <title>Context Aware Nonnegative Matrix Factorization Clustering</title>
    <summary>  In this article we propose a method to refine the clustering results obtained
with the nonnegative matrix factorization (NMF) technique, imposing consistency
constraints on the final labeling of the data. The research community focused
its effort on the initialization and on the optimization part of this method,
without paying attention to the final cluster assignments. We propose a game
theoretic framework in which each object to be clustered is represented as a
player, which has to choose its cluster membership. The information obtained
with NMF is used to initialize the strategy space of the players and a weighted
graph is used to model the interactions among the players. These interactions
allow the players to choose a cluster which is coherent with the clusters
chosen by similar players, a property which is not guaranteed by NMF, since it
produces a soft clustering of the data. The results on common benchmarks show
that our model is able to improve the performances of many NMF formulations.
</summary>
    <author>
      <name>Rocco Tripodi</name>
    </author>
    <author>
      <name>Sebastiano Vascon</name>
    </author>
    <author>
      <name>Marcello Pelillo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. Full paper accepted to International Conference
  on Pattern Recognition ICPR 2016, Canc\'un, Mexico</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04508v1</id>
    <updated>2016-09-15T04:45:11Z</updated>
    <published>2016-09-15T04:45:11Z</published>
    <title>Column Networks for Collective Classification</title>
    <summary>  Relational learning deals with data that are characterized by relational
structures. An important task is collective classification, which is to jointly
classify networked objects. While it holds a great promise to produce a better
accuracy than non-collective classifiers, collective classification is
computational challenging and has not leveraged on the recent breakthroughs of
deep learning. We present Column Network (CLN), a novel deep learning model for
collective classification in multi-relational domains. CLN has many desirable
theoretical properties: (i) it encodes multi-relations between any two
instances; (ii) it is deep and compact, allowing complex functions to be
approximated at the network level with a small set of free parameters; (iii)
local and relational features are learned simultaneously; (iv) long-range,
higher-order dependencies between instances are supported naturally; and (v)
crucially, learning and inference are efficient, linear in the size of the
network and the number of relations. We evaluate CLN on multiple real-world
applications: (a) delay prediction in software projects, (b) PubMed Diabetes
publication classification and (c) film genre classification. In all
applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.
</summary>
    <author>
      <name>Trang Pham</name>
    </author>
    <author>
      <name>Truyen Tran</name>
    </author>
    <author>
      <name>Dinh Phung</name>
    </author>
    <author>
      <name>Svetha Venkatesh</name>
    </author>
    <link href="http://arxiv.org/abs/1609.04508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04436v1</id>
    <updated>2016-09-14T20:34:26Z</updated>
    <published>2016-09-14T20:34:26Z</published>
    <title>Bayesian Reinforcement Learning: A Survey</title>
    <summary>  Bayesian methods for machine learning have been widely investigated, yielding
principled methods for incorporating prior information into inference
algorithms. In this survey, we provide an in-depth review of the role of
Bayesian methods for the reinforcement learning (RL) paradigm. The major
incentives for incorporating Bayesian reasoning in RL are: 1) it provides an
elegant approach to action-selection (exploration/exploitation) as a function
of the uncertainty in learning; and 2) it provides a machinery to incorporate
prior knowledge into the algorithms. We first discuss models and methods for
Bayesian inference in the simple single-step Bandit model. We then review the
extensive recent literature on Bayesian methods for model-based RL, where prior
information can be expressed on the parameters of the Markov model. We also
present Bayesian methods for model-free RL, where priors are expressed over the
value function or policy class. The objective of the paper is to provide a
comprehensive survey on Bayesian RL algorithms and their theoretical and
empirical properties.
</summary>
    <author>
      <name>Mohammad Ghavamzadeh</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <author>
      <name>Joelle Pineau</name>
    </author>
    <author>
      <name>Aviv Tamar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1561/2200000049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1561/2200000049" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Foundations and Trends in Machine Learning, Vol. 8: No. 5-6, pp
  359-492, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.04436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04371v1</id>
    <updated>2016-09-14T18:23:25Z</updated>
    <published>2016-09-14T18:23:25Z</published>
    <title>Finite LTL Synthesis is EXPTIME-complete</title>
    <summary>  LTL synthesis -- the construction of a function to satisfy a logical
specification formulated in Linear Temporal Logic -- is a 2EXPTIME-complete
problem with relevant applications in controller synthesis and a myriad of
artificial intelligence applications. In this research note we consider De
Giacomo and Vardi's variant of the synthesis problem for LTL formulas
interpreted over finite rather than infinite traces. Rather surprisingly, given
the existing claims on complexity, we establish that LTL synthesis is
EXPTIME-complete for the finite interpretation, and not 2EXPTIME-complete as
previously reported. Our result coincides nicely with the planning perspective
where non-deterministic planning with full observability is EXPTIME-complete
and partial observability increases the complexity to 2EXPTIME-complete; a
recent related result for LTL synthesis shows that in the finite case with
partial observability, the problem is 2EXPTIME-complete.
</summary>
    <author>
      <name>Jorge A. Baier</name>
    </author>
    <author>
      <name>Alberto Camacho</name>
    </author>
    <author>
      <name>Christian Muise</name>
    </author>
    <author>
      <name>Sheila A. McIlraith</name>
    </author>
    <link href="http://arxiv.org/abs/1609.04371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04337v1</id>
    <updated>2016-09-14T16:41:31Z</updated>
    <published>2016-09-14T16:41:31Z</published>
    <title>Quick and energy-efficient Bayesian computing of binocular disparity
  using stochastic digital signals</title>
    <summary>  Reconstruction of the tridimensional geometry of a visual scene using the
binocular disparity information is an important issue in computer vision and
mobile robotics, which can be formulated as a Bayesian inference problem.
However, computation of the full disparity distribution with an advanced
Bayesian model is usually an intractable problem, and proves computationally
challenging even with a simple model. In this paper, we show how probabilistic
hardware using distributed memory and alternate representation of data as
stochastic bitstreams can solve that problem with high performance and energy
efficiency. We put forward a way to express discrete probability distributions
using stochastic data representations and perform Bayesian fusion using those
representations, and show how that approach can be applied to diparity
computation. We evaluate the system using a simulated stochastic implementation
and discuss possible hardware implementations of such architectures and their
potential for sensorimotor processing and robotics.
</summary>
    <author>
      <name>Alexandre Coninx</name>
    </author>
    <author>
      <name>Pierre Bessière</name>
    </author>
    <author>
      <name>Jacques Droulez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of article submitted for publication in International
  Journal of Approximate Reasoning and accepted pending minor revisions</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04214v1</id>
    <updated>2016-09-14T10:51:00Z</updated>
    <published>2016-09-14T10:51:00Z</published>
    <title>"Flow Size Difference" Can Make a Difference: Detecting Malicious TCP
  Network Flows Based on Benford's Law</title>
    <summary>  Statistical characteristics of network traffic have attracted a significant
amount of research for automated network intrusion detection, some of which
looked at applications of natural statistical laws such as Zipf's law,
Benford's law and the Pareto distribution. In this paper, we present the
application of Benford's law to a new network flow metric "flow size
difference", which have not been studied by other researchers, to build an
unsupervised flow-based intrusion detection system (IDS). The method was
inspired by our observation on a large number of TCP flow datasets where normal
flows tend to follow Benford's law closely but malicious flows tend to deviate
significantly from it. The proposed IDS is unsupervised so no training is
needed thus can be easily deployed. It has two simple parameters with a clear
semantic meaning, allowing the human operator to set and adapt their values
intuitively to adjust the overall performance of the IDS. We tested the
proposed IDS on one closed and two public datasets and proved its efficiency in
terms of AUC (area under the ROC curve). Being a simple and fast standalone IDS
itself, the proposed method can also be easily combined with other network IDSs
e.g. added as an additional component into another existing IDS to enhance its
performance.
</summary>
    <author>
      <name>Aamo Iorliam</name>
    </author>
    <author>
      <name>Santosh Tirunagari</name>
    </author>
    <author>
      <name>Anthony T. S. Ho</name>
    </author>
    <author>
      <name>Shujun Li</name>
    </author>
    <author>
      <name>Adrian Waller</name>
    </author>
    <author>
      <name>Norman Poh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures, initial draft (to be finalised soon)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2; K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04648v1</id>
    <updated>2016-09-14T10:13:42Z</updated>
    <published>2016-09-14T10:13:42Z</published>
    <title>Sequencing Chess</title>
    <summary>  We analyze the structure of the state space of chess by means of transition
path sampling Monte Carlo simulation. Based on the typical number of moves
required to transpose a given configuration of chess pieces into another, we
conclude that the state space consists of several pockets between which
transitions are rare. Skilled players explore an even smaller subset of
positions that populate some of these pockets only very sparsely. These results
suggest that the usual measures to estimate both, the size of the state space
and the size of the tree of legal moves, are not unique indicators of the
complexity of the game, but that topological considerations are equally
important.
</summary>
    <author>
      <name>A. Atashpendar</name>
    </author>
    <author>
      <name>T. Schilling</name>
    </author>
    <author>
      <name>Th. Voigtmann</name>
    </author>
    <link href="http://arxiv.org/abs/1609.04648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03993v1</id>
    <updated>2016-09-13T19:36:45Z</updated>
    <published>2016-09-13T19:36:45Z</published>
    <title>A Generic Bet-and-run Strategy for Speeding Up Traveling Salesperson and
  Minimum Vertex Cover</title>
    <summary>  A common strategy for improving optimization algorithms is to restart the
algorithm when it is believed to be trapped in an inferior part of the search
space. However, while specific restart strategies have been developed for
specific problems (and specific algorithms), restarts are typically not
regarded as a general tool to speed up an optimization algorithm. In fact, many
optimization algorithms do not employ restarts at all.
  Recently, "bet-and-run" was introduced in the context of mixed-integer
programming, where first a number of short runs with randomized initial
conditions is made, and then the most promising run of these is continued. In
this article, we consider two classical NP-complete combinatorial optimization
problems, traveling salesperson and minimum vertex cover, and study the
effectiveness of different bet-and-run strategies. In particular, our restart
strategies do not take any problem knowledge into account, nor are tailored to
the optimization algorithm. Therefore, they can be used off-the-shelf. We
observe that state-of-the-art solvers for these problems can benefit
significantly from restarts on standard benchmark instances.
</summary>
    <author>
      <name>Tobias Friedrich</name>
    </author>
    <author>
      <name>Timo Kötzing</name>
    </author>
    <author>
      <name>Markus Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03971v1</id>
    <updated>2016-09-13T18:34:59Z</updated>
    <published>2016-09-13T18:34:59Z</published>
    <title>Feynman Machine: The Universal Dynamical Systems Computer</title>
    <summary>  Efforts at understanding the computational processes in the brain have met
with limited success, despite their importance and potential uses in building
intelligent machines. We propose a simple new model which draws on recent
findings in Neuroscience and the Applied Mathematics of interacting Dynamical
Systems. The Feynman Machine is a Universal Computer for Dynamical Systems,
analogous to the Turing Machine for symbolic computing, but with several
important differences. We demonstrate that networks and hierarchies of simple
interacting Dynamical Systems, each adaptively learning to forecast its
evolution, are capable of automatically building sensorimotor models of the
external and internal world. We identify such networks in mammalian neocortex,
and show how existing theories of cortical computation combine with our model
to explain the power and flexibility of mammalian intelligence. These findings
lead directly to new architectures for machine intelligence. A suite of
software implementations has been built based on these principles, and applied
to a number of spatiotemporal learning tasks.
</summary>
    <author>
      <name>Eric Laukien</name>
    </author>
    <author>
      <name>Richard Crowder</name>
    </author>
    <author>
      <name>Fergal Byrne</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03847v1</id>
    <updated>2016-09-13T14:17:32Z</updated>
    <published>2016-09-13T14:17:32Z</published>
    <title>Instrumenting an SMT Solver to Solve Hybrid Network Reachability
  Problems</title>
    <summary>  PDDL+ planning has its semantics rooted in hybrid automata (HA) and recent
work has shown that it can be modeled as a network of HAs. Addressing the
complexity of nonlinear PDDL+ planning as HAs requires both space and time
efficient reasoning. Unfortunately, existing solvers either do not address
nonlinear dynamics or do not natively support networks of automata.
  We present a new algorithm, called HNSolve, which guides the variable
selection of the dReal Satisfiability Modulo Theories (SMT) solver while
reasoning about network encodings of nonlinear PDDL+ planning as HAs. HNSolve
tightly integrates with dReal by solving a discrete abstraction of the HA
network. HNSolve finds composite runs on the HA network that ignore continuous
variables, but respect mode jumps and synchronization labels. HNSolve
admissibly detects dead-ends in the discrete abstraction, and posts conflict
clauses that prune the SMT solver's search. We evaluate the benefits of our
HNSolve algorithm on PDDL+ benchmark problems and demonstrate its performance
with respect to prior work.
</summary>
    <author>
      <name>Daniel Bryce</name>
    </author>
    <author>
      <name>Sergiy Bogomolov</name>
    </author>
    <author>
      <name>Alexander Heinz</name>
    </author>
    <author>
      <name>Christian Schilling</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03765v1</id>
    <updated>2016-09-13T11:08:23Z</updated>
    <published>2016-09-13T11:08:23Z</published>
    <title>Graph Aggregation</title>
    <summary>  Graph aggregation is the process of computing a single output graph that
constitutes a good compromise between several input graphs, each provided by a
different source. One needs to perform graph aggregation in a wide variety of
situations, e.g., when applying a voting rule (graphs as preference orders),
when consolidating conflicting views regarding the relationships between
arguments in a debate (graphs as abstract argumentation frameworks), or when
computing a consensus between several alternative clusterings of a given
dataset (graphs as equivalence relations). In this paper, we introduce a formal
framework for graph aggregation grounded in social choice theory. Our focus is
on understanding which properties shared by the individual input graphs will
transfer to the output graph returned by a given aggregation rule. We consider
both common properties of graphs, such as transitivity and reflexivity, and
arbitrary properties expressible in certain fragments of modal logic. Our
results establish several connections between the types of properties preserved
under aggregation and the choice-theoretic axioms satisfied by the rules used.
The most important of these results is a powerful impossibility theorem that
generalises Arrow's seminal result for the aggregation of preference orders to
a large collection of different types of graphs.
</summary>
    <author>
      <name>Ulle Endriss</name>
    </author>
    <author>
      <name>Umberto Grandi</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03632v1</id>
    <updated>2016-09-12T23:27:37Z</updated>
    <published>2016-09-12T23:27:37Z</published>
    <title>Joint Extraction of Events and Entities within a Document Context</title>
    <summary>  Events and entities are closely related; entities are often actors or
participants in events and events without entities are uncommon. The
interpretation of events and entities is highly contextually dependent.
Existing work in information extraction typically models events separately from
entities, and performs inference at the sentence level, ignoring the rest of
the document. In this paper, we propose a novel approach that models the
dependencies among variables of events, entities, and their relations, and
performs joint inference of these variables across a document. The goal is to
enable access to document-level contextual information and facilitate
context-aware predictions. We demonstrate that our approach substantially
outperforms the state-of-the-art methods for event extraction as well as a
strong baseline for entity extraction.
</summary>
    <author>
      <name>Bishan Yang</name>
    </author>
    <author>
      <name>Tom Mitchell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures, published at NAACL 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of NAACL-HLT 2016, pages 289-299</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.03632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03543v1</id>
    <updated>2016-09-12T19:30:56Z</updated>
    <published>2016-09-12T19:30:56Z</published>
    <title>Logical Induction</title>
    <summary>  We present a computable algorithm that assigns probabilities to every logical
statement in a given formal language, and refines those probabilities over
time. For instance, if the language is Peano arithmetic, it assigns
probabilities to all arithmetical statements, including claims about the twin
prime conjecture, the outputs of long-running computations, and its own
probabilities. We show that it satisfies a number of intuitive desiderata,
including: (1) it learns to predict patterns of truth and falsehood in logical
statements, often long before having the resources to evaluate the statements,
so long as the patterns can be written down in polynomial time; (2) it learns
to use appropriate statistical summaries to predict sequences of statements
whose truth values appear pseudorandom; and (3) it learns to have accurate
beliefs about its own current beliefs, in a manner that avoids the standard
paradoxes of self-reference. For example, if a given computer program only ever
produces outputs in a certain range, a logical inductor learns this fact in a
timely manner; and if late digits in the decimal expansion of $\pi$ are
difficult to predict, then a logical inductor learns to assign $\approx 10\%$
probability to "the $n$th digit of $\pi$ is a 7" for large $n$. Logical
inductors also learn to trust their future beliefs more than their current
beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies
\psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and
logical inductors strictly dominate the universal semimeasure in the limit.
  These properties and many others all follow from a single logical induction
criterion, which is motivated by a series of stock trading analogies. Roughly
speaking, each logical sentence $\phi$ is associated with a stock that is worth
\$1 per share if $\phi$ is true and nothing otherwise, and we interpret the
[...]
</summary>
    <author>
      <name>Scott Garrabrant</name>
    </author>
    <author>
      <name>Tsvi Benson-Tilsen</name>
    </author>
    <author>
      <name>Andrew Critch</name>
    </author>
    <author>
      <name>Nate Soares</name>
    </author>
    <author>
      <name>Jessica Taylor</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03540v2</id>
    <updated>2016-09-13T01:59:05Z</updated>
    <published>2016-09-12T19:24:14Z</published>
    <title>ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data</title>
    <summary>  Causal inference from observational data is a subject of active research and
development in statistics and computer science. Many toolkits have been
developed for this purpose that depends on statistical software. However, these
toolkits do not scale to large datasets. In this paper we describe a suite of
techniques for expressing causal inference tasks from observational data in
SQL. This suite supports the state-of-the-art methods for causal inference and
run at scale within a database engine. In addition, we introduce several
optimization techniques that significantly speedup causal inference, both in
the online and offline setting. We evaluate the quality and performance of our
techniques by experiments of real datasets.
</summary>
    <author>
      <name>Babak Salimi</name>
    </author>
    <author>
      <name>Dan Suciu</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03540v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03540v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03438v1</id>
    <updated>2016-09-12T15:12:00Z</updated>
    <published>2016-09-12T15:12:00Z</published>
    <title>Reactive Multi-Context Systems: Heterogeneous Reasoning in Dynamic
  Environments</title>
    <summary>  In this paper we introduce reactive multi-context systems (rMCSs), a
framework for reactive reasoning in the presence of heterogeneous knowledge
sources. In particular, we show how to integrate data streams into
multi-context systems (MCSs) and how to model the dynamics of the systems,
based on two types of bridge rules. We illustrate how several typical problems
arising in the context of stream reasoning can be handled using our framework.
Reasoning based on multiple knowledge sources that need to be integrated faces
the problem of potential inconsistencies. We discuss various methods for
handling inconsistencies, with a special focus on non-existence of equilibria.
In particular, we show how methods developed for managed MCSs can be
generalized to rMCSs. We also study the issue of nondeterminism in rMCSs. One
way of avoiding nondeterminism is by applying an alternative, skeptical
semantics. We show how such a semantics, called well-founded semantics, can be
defined for rMCSs, and what the effect of using this semantics instead of the
original one is. We investigate the complexity of various reasoning problems
related to rMCSs. Finally, we discuss related work, with a special focus on two
of the most relevant approaches w.r.t. stream reasoning, namely LARS and
STARQL.
</summary>
    <author>
      <name>Gerhard Brewka</name>
    </author>
    <author>
      <name>Stefan Ellmauthaler</name>
    </author>
    <author>
      <name>Ricardo Gonçalves</name>
    </author>
    <author>
      <name>Matthias Knorr</name>
    </author>
    <author>
      <name>João Leite</name>
    </author>
    <author>
      <name>Jörg Pührer</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03437v1</id>
    <updated>2016-09-12T15:11:58Z</updated>
    <published>2016-09-12T15:11:58Z</published>
    <title>First-Order Bayesian Network Specifications Capture the Complexity Class
  PP</title>
    <summary>  The point of this note is to prove that a language is in the complexity class
PP if and only if the strings of the language encode valid inferences in a
Bayesian network defined using function-free first-order logic with equality.
</summary>
    <author>
      <name>Fabio Gagliardi Cozman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03357v1</id>
    <updated>2016-09-12T11:58:59Z</updated>
    <published>2016-09-12T11:58:59Z</published>
    <title>Modelling Creativity: Identifying Key Components through a Corpus-Based
  Approach</title>
    <summary>  Creativity is a complex, multi-faceted concept encompassing a variety of
related aspects, abilities, properties and behaviours. If we wish to study
creativity scientifically, then a tractable and well-articulated model of
creativity is required. Such a model would be of great value to researchers
investigating the nature of creativity and in particular, those concerned with
the evaluation of creative practice. This paper describes a unique approach to
developing a suitable model of how creative behaviour emerges that is based on
the words people use to describe the concept. Using techniques from the field
of statistical natural language processing, we identify a collection of
fourteen key components of creativity through an analysis of a corpus of
academic papers on the topic. Words are identified which appear significantly
often in connection with discussions of the concept. Using a measure of lexical
similarity to help cluster these words, a number of distinct themes emerge,
which collectively contribute to a comprehensive and multi-perspective model of
creativity. The components provide an ontology of creativity: a set of building
blocks which can be used to model creative practice in a variety of domains.
The components have been employed in two case studies to evaluate the
creativity of computational systems and have proven useful in articulating
achievements of this work and directions for further research.
</summary>
    <author>
      <name>Anna Jordanous</name>
    </author>
    <author>
      <name>Bill Keller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to PLOS ONE; currently under review. Figures not included</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03333v1</id>
    <updated>2016-09-12T10:25:29Z</updated>
    <published>2016-09-12T10:25:29Z</published>
    <title>On Generation of Time-based Label Refinements</title>
    <summary>  Process mining is a research field focused on the analysis of event data with
the aim of extracting insights in processes. Applying process mining techniques
on data from smart home environments has the potential to provide valuable
insights in (un)healthy habits and to contribute to ambient assisted living
solutions. Finding the right event labels to enable application of process
mining techniques is however far from trivial, as simply using the triggering
sensor as the label for sensor events results in uninformative models that
allow for too much behavior (overgeneralizing). Refinements of sensor level
event labels suggested by domain experts have shown to enable discovery of more
precise and insightful process models. However, there exist no automated
approach to generate refinements of event labels in the context of process
mining. In this paper we propose a framework for automated generation of label
refinements based on the time attribute of events. We show on a case study with
real life smart home event data that behaviorally more specific, and therefore
more insightful, process models can be found by using automatically generated
refined labels in process discovery.
</summary>
    <author>
      <name>Niek Tax</name>
    </author>
    <author>
      <name>Emin Alasgarov</name>
    </author>
    <author>
      <name>Natalia Sidorova</name>
    </author>
    <author>
      <name>Reinder Haakma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CS&amp;P workshop 2016 Overlap in preliminaries with
  arXiv:1606.07259</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03286v1</id>
    <updated>2016-09-12T07:29:59Z</updated>
    <published>2016-09-12T07:29:59Z</published>
    <title>Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks</title>
    <summary>  Natural language understanding (NLU) is a core component of a spoken dialogue
system. Recently recurrent neural networks (RNN) obtained strong results on NLU
due to their superior ability of preserving sequential information over time.
Traditionally, the NLU module tags semantic slots for utterances considering
their flat structures, as the underlying RNN structure is a linear chain.
However, natural language exhibits linguistic properties that provide rich,
structured information for better understanding. This paper introduces a novel
model, knowledge-guided structural attention networks (K-SAN), a generalization
of RNN to additionally incorporate non-flat network topologies guided by prior
knowledge. There are two characteristics: 1) important substructures can be
captured from small training data, allowing the model to generalize to
previously unseen test data; 2) the model automatically figures out the salient
substructures that are essential to predict the semantic tags of the given
sentences, so that the understanding performance can be improved. The
experiments on the benchmark Air Travel Information System (ATIS) data show
that the proposed K-SAN architecture can effectively extract salient knowledge
from substructures with an attention mechanism, and outperform the performance
of the state-of-the-art neural network based frameworks.
</summary>
    <author>
      <name>Yun-Nung Chen</name>
    </author>
    <author>
      <name>Dilek Hakkani-Tur</name>
    </author>
    <author>
      <name>Gokhan Tur</name>
    </author>
    <author>
      <name>Asli Celikyilmaz</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Li Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03250v1</id>
    <updated>2016-09-12T02:12:13Z</updated>
    <published>2016-09-12T02:12:13Z</published>
    <title>DESPOT: Online POMDP Planning with Regularization</title>
    <summary>  The partially observable Markov decision process (POMDP) provides a
principled general framework for planning under uncertainty, but solving POMDPs
optimally is computationally intractable, due to the "curse of dimensionality"
and the "curse of history". To overcome these challenges, we introduce the
Determinized Sparse Partially Observable Tree (DESPOT), a sparse approximation
of the standard belief tree, for online planning under uncertainty. A DESPOT
focuses online planning on a set of randomly sampled scenarios and compactly
captures the "execution" of all policies under these scenarios. We show that
the best policy obtained from a DESPOT is near-optimal, with a regret bound
that depends on the representation size of the optimal policy. Leveraging this
result, we give an anytime online planning algorithm, which searches a DESPOT
for a policy that optimizes a regularized objective function. Regularization
balances the estimated value of a policy under the sampled scenarios and the
policy size, thus avoiding overfitting. The algorithm demonstrates strong
experimental results, compared with some of the best online POMDP algorithms
available. It has also been incorporated into an autonomous driving system for
realtime vehicle control. The source code for the algorithm is available at
http: //bigbird.comp.nus.edu.sg/pmwiki/farm/appl/.
</summary>
    <author>
      <name>Nan Ye</name>
    </author>
    <author>
      <name>Adhiraj Somani</name>
    </author>
    <author>
      <name>David Hsu</name>
    </author>
    <author>
      <name>Wee Sun Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03234v1</id>
    <updated>2016-09-12T00:30:54Z</updated>
    <published>2016-09-12T00:30:54Z</published>
    <title>Reduced Space and Faster Convergence in Imperfect-Information Games via
  Regret-Based Pruning</title>
    <summary>  Counterfactual Regret Minimization (CFR) is the most popular iterative
algorithm for solving zero-sum imperfect-information games. Regret-Based
Pruning (RBP) is an improvement that allows poorly-performing actions to be
temporarily pruned, thus speeding up CFR. We introduce Total RBP, a new form of
RBP that reduces the space requirements of CFR as actions are pruned. We prove
that in zero-sum games it asymptotically prunes any action that is not part of
a best response to some Nash equilibrium. This leads to provably faster
convergence and lower space requirements. Experiments show that Total RBP
results in an order of magnitude reduction in space, and the reduction factor
increases with game size.
</summary>
    <author>
      <name>Noam Brown</name>
    </author>
    <author>
      <name>Tuomas Sandholm</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03193v2</id>
    <updated>2016-09-13T02:49:05Z</updated>
    <published>2016-09-11T18:56:53Z</published>
    <title>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</title>
    <summary>  This paper presents a simple end-to-end model for speech recognition,
combining a convolutional network based acoustic model and a graph decoding. It
is trained to output letters, with transcribed speech, without the need for
force alignment of phonemes. We introduce an automatic segmentation criterion
for training from sequence annotation without alignment that is on par with CTC
while being simpler. We show competitive results in word error rate on the
Librispeech corpus with MFCC features, and promising results from raw waveform.
</summary>
    <author>
      <name>Ronan Collobert</name>
    </author>
    <author>
      <name>Christian Puhrsch</name>
    </author>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures (7 plots/schemas), 2 tables (4 tabulars)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03193v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03193v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03157v1</id>
    <updated>2016-09-11T13:03:21Z</updated>
    <published>2016-09-11T13:03:21Z</published>
    <title>A centralized reinforcement learning method for multi-agent job
  scheduling in Grid</title>
    <summary>  One of the main challenges in Grid systems is designing an adaptive,
scalable, and model-independent method for job scheduling to achieve a
desirable degree of load balancing and system efficiency. Centralized job
scheduling methods have some drawbacks, such as single point of failure and
lack of scalability. Moreover, decentralized methods require a coordination
mechanism with limited communications. In this paper, we propose a multi-agent
approach to job scheduling in Grid, named Centralized Learning Distributed
Scheduling (CLDS), by utilizing the reinforcement learning framework. The CLDS
is a model free approach that uses the information of jobs and their completion
time to estimate the efficiency of resources. In this method, there are a
learner agent and several scheduler agents that perform the task of learning
and job scheduling with the use of a coordination strategy that maintains the
communication cost at a limited level. We evaluated the efficiency of the CLDS
method by designing and performing a set of experiments on a simulated Grid
system under different system scales and loads. The results show that the CLDS
can effectively balance the load of system even in large scale and heavy loaded
Grids, while maintains its adaptive performance and scalability.
</summary>
    <author>
      <name>Milad Moradi</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03145v1</id>
    <updated>2016-09-11T10:14:18Z</updated>
    <published>2016-09-11T10:14:18Z</published>
    <title>Relational Models</title>
    <summary>  We provide a survey on relational models. Relational models describe complete
networked {domains by taking into account global dependencies in the data}.
Relational models can lead to more accurate predictions if compared to
non-relational machine learning approaches. Relational models typically are
based on probabilistic graphical models, e.g., Bayesian networks, Markov
networks, or latent variable models. Relational models have applications in
social networks analysis, the modeling of knowledge graphs, bioinformatics,
recommendation systems, natural language processing, medical decision support,
and linked data.
</summary>
    <author>
      <name>Volker Tresp</name>
    </author>
    <author>
      <name>Maximilian Nickel</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03058v1</id>
    <updated>2016-09-10T14:33:06Z</updated>
    <published>2016-09-10T14:33:06Z</published>
    <title>A Tube-and-Droplet-based Approach for Representing and Analyzing Motion
  Trajectories</title>
    <summary>  Trajectory analysis is essential in many applications. In this paper, we
address the problem of representing motion trajectories in a highly informative
way, and consequently utilize it for analyzing trajectories. Our approach first
leverages the complete information from given trajectories to construct a
thermal transfer field which provides a context-rich way to describe the global
motion pattern in a scene. Then, a 3D tube is derived which depicts an input
trajectory by integrating its surrounding motion patterns contained in the
thermal transfer field. The 3D tube effectively: 1) maintains the movement
information of a trajectory, 2) embeds the complete contextual motion pattern
around a trajectory, 3) visualizes information about a trajectory in a clear
and unified way. We further introduce a droplet-based process. It derives a
droplet vector from a 3D tube, so as to characterize the high-dimensional 3D
tube information in a simple but effective way. Finally, we apply our
tube-and-droplet representation to trajectory analysis applications including
trajectory clustering, trajectory classification &amp; abnormality detection, and
3D action recognition. Experimental comparisons with state-of-the-art
algorithms demonstrate the effectiveness of our approach.
</summary>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Yang Zhou</name>
    </author>
    <author>
      <name>Hongteng Xu</name>
    </author>
    <author>
      <name>Junchi Yan</name>
    </author>
    <author>
      <name>Mingliang Xu</name>
    </author>
    <author>
      <name>Jianxin Wu</name>
    </author>
    <author>
      <name>Zicheng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is the accepted version for TPAMI (IEEE Transactions
  on Pattern Analysis and Machine Intelligence), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02993v2</id>
    <updated>2016-09-13T00:18:48Z</updated>
    <published>2016-09-10T02:13:02Z</published>
    <title>Episodic Exploration for Deep Deterministic Policies: An Application to
  StarCraft Micromanagement Tasks</title>
    <summary>  We consider scenarios from the real-time strategy game StarCraft as new
benchmarks for reinforcement learning algorithms. We propose micromanagement
tasks, which present the problem of the short-term, low-level control of army
members during a battle. From a reinforcement learning point of view, these
scenarios are challenging because the state-action space is very large, and
because there is no obvious feature representation for the state-action
evaluation function. We describe our approach to tackle the micromanagement
scenarios with deep neural network controllers from raw state features given by
the game engine. In addition, we present a heuristic reinforcement learning
algorithm which combines direct exploration in the policy space and
backpropagation. This algorithm allows for the collection of traces for
learning using deterministic policies, which appears much more efficient than,
for example, {\epsilon}-greedy exploration. Experiments show that with this
algorithm, we successfully learn non-trivial strategies for scenarios with
armies of up to 15 agents, where both Q-learning and REINFORCE struggle.
</summary>
    <author>
      <name>Nicolas Usunier</name>
    </author>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <author>
      <name>Zeming Lin</name>
    </author>
    <author>
      <name>Soumith Chintala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 1 figure (2 plots), 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02993v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02993v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02976v1</id>
    <updated>2016-09-09T23:45:19Z</updated>
    <published>2016-09-09T23:45:19Z</published>
    <title>An Integrated Classification Model for Financial Data Mining</title>
    <summary>  Nowadays, financial data analysis is becoming increasingly important in the
business market. As companies collect more and more data from daily operations,
they expect to extract useful knowledge from existing collected data to help
make reasonable decisions for new customer requests, e.g. user credit category,
churn analysis, real estate analysis, etc. Financial institutes have applied
different data mining techniques to enhance their business performance.
However, simple ap-proach of these techniques could raise a performance issue.
Besides, there are very few general models for both understanding and
forecasting different finan-cial fields. We present in this paper a new
classification model for analyzing fi-nancial data. We also evaluate this model
with different real-world data to show its performance.
</summary>
    <author>
      <name>Fan Cai</name>
    </author>
    <author>
      <name>Nhien-An Le-Khac</name>
    </author>
    <author>
      <name>M-T. Kechadi</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02672v1</id>
    <updated>2016-09-09T07:20:01Z</updated>
    <published>2016-09-09T07:20:01Z</published>
    <title>Measuring Player's Behaviour Change over Time in Public Goods Game</title>
    <summary>  An important issue in public goods game is whether player's behaviour changes
over time, and if so, how significant it is. In this game players can be
classified into different groups according to the level of their participation
in the public good. This problem can be considered as a concept drift problem
by asking the amount of change that happens to the clusters of players over a
sequence of game rounds. In this study we present a method for measuring
changes in clusters with the same items over discrete time points using
external clustering validation indices and area under the curve. External
clustering indices were originally used to measure the difference between
suggested clusters in terms of clustering algorithms and ground truth labels
for items provided by experts. Instead of different cluster label comparison,
we use these indices to compare between clusters of any two consecutive time
points or between the first time point and the remaining time points to measure
the difference between clusters through time points. In theory, any external
clustering indices can be used to measure changes for any traditional
(non-temporal) clustering algorithm, due to the fact that any time point alone
is not carrying any temporal information. For the public goods game, our
results indicate that the players are changing over time but the change is
smooth and relatively constant between any two time points.
</summary>
    <author>
      <name>Polla Fattah</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Christian Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SAI Intelligent Systems Conference 2016 London, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02646v1</id>
    <updated>2016-09-09T03:13:55Z</updated>
    <published>2016-09-09T03:13:55Z</published>
    <title>Some Advances in Role Discovery in Graphs</title>
    <summary>  Role discovery in graphs is an emerging area that allows analysis of complex
graphs in an intuitive way. In contrast to other graph prob- lems such as
community discovery, which finds groups of highly connected nodes, the role
discovery problem finds groups of nodes that share similar graph topological
structure. However, existing work so far has two severe limitations that
prevent its use in some domains. Firstly, it is completely unsupervised which
is undesirable for a number of reasons. Secondly, most work is limited to a
single relational graph. We address both these lim- itations in an intuitive
and easy to implement alternating least squares framework. Our framework allows
convex constraints to be placed on the role discovery problem which can provide
useful supervision. In par- ticular we explore supervision to enforce i)
sparsity, ii) diversity and iii) alternativeness. We then show how to lift this
work for multi-relational graphs. A natural representation of a
multi-relational graph is an order 3 tensor (rather than a matrix) and that a
Tucker decomposition allows us to find complex interactions between collections
of entities (E-groups) and the roles they play for a combination of relations
(R-groups). Existing Tucker decomposition methods in tensor toolboxes are not
suited for our purpose, so we create our own algorithm that we demonstrate is
pragmatically useful.
</summary>
    <author>
      <name>Sean Gilpin</name>
    </author>
    <author>
      <name>Chia-Tung Kuo</name>
    </author>
    <author>
      <name>Tina Eliassi-Rad</name>
    </author>
    <author>
      <name>Ian Davidson</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02584v1</id>
    <updated>2016-09-08T20:48:32Z</updated>
    <published>2016-09-08T20:48:32Z</published>
    <title>Towards Better Response Times and Higher-Quality Queries in Interactive
  Knowledge Base Debugging</title>
    <summary>  Many AI applications rely on knowledge encoded in a locigal knowledge base
(KB). The most essential benefit of such logical KBs is the opportunity to
perform automatic reasoning which however requires a KB to meet some minimal
quality criteria such as consistency. Without adequate tool assistance, the
task of resolving such violated quality criteria in a KB can be extremely hard,
especially when the problematic KB is large and complex. To this end,
interactive KB debuggers have been introduced which ask a user queries whether
certain statements must or must not hold in the intended domain. The given
answers help to gradually restrict the search space for KB repairs.
  Existing interactive debuggers often rely on a pool-based strategy for query
computation. A pool of query candidates is precomputed, from which the best
candidate according to some query quality criterion is selected to be shown to
the user. This often leads to the generation of many unnecessary query
candidates and thus to a high number of expensive calls to logical reasoning
services. We tackle this issue by an in-depth mathematical analysis of diverse
real-valued active learning query selection measures in order to determine
qualitative criteria that make a query favorable. These criteria are the key to
devising efficient heuristic query search methods. The proposed methods enable
for the first time a completely reasoner-free query generation for interactive
KB debugging while at the same time guaranteeing optimality conditions, e.g.
minimal cardinality or best understandability for the user, of the generated
query that existing methods cannot realize.
  Further, we study different relations between active learning measures. The
obtained picture gives a hint about which measures are more favorable in which
situation or which measures always lead to the same outcomes, based on given
types of queries.
</summary>
    <author>
      <name>Patrick Rodler</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02531v1</id>
    <updated>2016-09-08T19:01:59Z</updated>
    <published>2016-09-08T19:01:59Z</published>
    <title>Latest Datasets and Technologies Presented in the Workshop on Grasping
  and Manipulation Datasets</title>
    <summary>  This paper reports the activities and outcomes in the Workshop on Grasping
and Manipulation Datasets that was organized under the International Conference
on Robotics and Automation (ICRA) 2016. The half day workshop was packed with
nine invited talks, 12 interactive presentations, and one panel discussion with
ten panelists. This paper summarizes all the talks and presentations and recaps
what has been discussed in the panels session. This summary servers as a review
of recent developments in data collection in grasping and manipulation. Many of
the presentations describe ongoing efforts or explorations that could be
achieved and fully available in a year or two. The panel discussion not only
commented on the current approaches, but also indicates new directions and
focuses. The workshop clearly displayed the importance of quality datasets in
robotics and robotic grasping and manipulation field. Hopefully the workshop
could motivate larger efforts to create big datasets that are comparable with
big datasets in other communities such as computer vision.
</summary>
    <author>
      <name>Matteo Bianchi</name>
    </author>
    <author>
      <name>Jeannette Bohg</name>
    </author>
    <author>
      <name>Yu Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02316v1</id>
    <updated>2016-09-08T08:15:58Z</updated>
    <published>2016-09-08T08:15:58Z</published>
    <title>Ms. Pac-Man Versus Ghost Team CIG 2016 Competition</title>
    <summary>  This paper introduces the revival of the popular Ms. Pac-Man Versus Ghost
Team competition. We present an updated game engine with Partial Observability
constraints, a new Multi-Agent Systems approach to developing Ghost agents and
several sample controllers to ease the development of entries. A restricted
communication protocol is provided for the Ghosts, providing a more challenging
environment than before. The competition will debut at the IEEE Computational
Intelligence and Games Conference 2016. Some preliminary results showing the
effects of Partial Observability and the benefits of simple communication are
also presented.
</summary>
    <author>
      <name>Pier R. Williams</name>
    </author>
    <author>
      <name>Diego Perez-Liebana</name>
    </author>
    <author>
      <name>Simon M. Lucas</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02236v1</id>
    <updated>2016-09-08T00:57:19Z</updated>
    <published>2016-09-08T00:57:19Z</published>
    <title>Latent Dependency Forest Models</title>
    <summary>  Probabilistic modeling is one of the foundations of modern machine learning
and artificial intelligence. In this paper, we propose a novel type of
probabilistic models named latent dependency forest models (LDFMs). A LDFM
models the dependencies between random variables with a forest structure that
can change dynamically based on the variable values. It is therefore capable of
modeling context-specific independence. We parameterize a LDFM using a
first-order non-projective dependency grammar. Learning LDFMs from data can be
formulated purely as a parameter learning problem, and hence the difficult
problem of model structure learning is circumvented. Our experimental results
show that LDFMs are competitive with existing probabilistic models.
</summary>
    <author>
      <name>Shanbo Chu</name>
    </author>
    <author>
      <name>Yong Jiang</name>
    </author>
    <author>
      <name>Kewei Tu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02228v1</id>
    <updated>2016-09-08T00:02:20Z</updated>
    <published>2016-09-08T00:02:20Z</published>
    <title>Backpropagation of Hebbian plasticity for lifelong learning</title>
    <summary>  Hebbian plasticity allows biological agents to learn from their lifetime
experience, extending the fixed information provided by evolutionary search.
Conversely, backpropagation methods can build high-performance fixed-weights
networks, but are not currently equipped to design networks with Hebbian
connections. Here we use backpropagation to train fully-differentiable plastic
networks, such that backpropagation determines not only the baseline weights,
but also the plasticity of each connection. To perform this backpropagation of
Hebbian plasticity (BOHP), we derive error gradients for neural networks with
Hebbian plastic connections. The equations for these gradients turn out to
follow a simple, recursive form. We apply this method to train small networks
for simple learning tasks inspired from classical conditioning. We show that,
through Hebbian plasticity, the networks perform fast learning of unpredictable
environmental features during their lifetime, successfully solving a task that
fixed-weight feedforward networks cannot possibly solve. We conclude that
backpropagation of Hebbian plasticity offers a powerful model for lifelong
learning.
</summary>
    <author>
      <name>Thomas Miconi</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02226v2</id>
    <updated>2016-09-11T06:34:56Z</updated>
    <published>2016-09-07T23:59:36Z</published>
    <title>Fitted Learning: Models with Awareness of their Limits</title>
    <summary>  Though deep learning has pushed the boundaries of classification forward, in
recent years hints of the limits of standard classification have begun to
emerge. Problems such as fooling, adding new classes over time, and the need to
retrain learning models only for small changes to the original problem all
point to a potential shortcoming in the classic classification regime, where a
comprehensive a priori knowledge of the possible classes or concepts is
critical. Without such knowledge, classifiers misjudge the limits of their
knowledge and overgeneralization therefore becomes a serious obstacle to
consistent performance. In response to these challenges, this paper extends the
classic regime by reframing classification instead with the assumption that
concepts present in the training set are only a sample of the hypothetical
final set of concepts. To bring learning models into this new paradigm, a novel
elaboration of standard architectures called the competitive overcomplete
output layer (COOL) neural network is introduced. Experiments demonstrate the
effectiveness of COOL by applying it to fooling, separable concept learning,
one-class neural networks, and standard classification benchmarks. The results
suggest that, unlike conventional classifiers, the amount of generalization in
COOL networks can be tuned to match the problem.
</summary>
    <author>
      <name>Navid Kardan</name>
    </author>
    <author>
      <name>Kenneth O. Stanley</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02226v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02226v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02132v1</id>
    <updated>2016-09-07T19:35:30Z</updated>
    <published>2016-09-07T19:35:30Z</published>
    <title>UberNet: Training a `Universal' Convolutional Neural Network for Low-,
  Mid-, and High-Level Vision using Diverse Datasets and Limited Memory</title>
    <summary>  In this work we introduce a convolutional neural network (CNN) that jointly
handles low-, mid-, and high-level vision tasks in a unified architecture that
is trained end-to-end. Such a universal network can act like a `swiss knife'
for vision tasks; we call this architecture an UberNet to indicate its
overarching nature.
  We address two main technical challenges that emerge when broadening up the
range of tasks handled by a single CNN: (i) training a deep architecture while
relying on diverse training sets and (ii) training many (potentially unlimited)
tasks with a limited memory budget. Properly addressing these two problems
allows us to train accurate predictors for a host of tasks, without
compromising accuracy.
  Through these advances we train in an end-to-end manner a CNN that
simultaneously addresses (a) boundary detection (b) normal estimation (c)
saliency estimation (d) semantic segmentation (e) human part segmentation (f)
semantic boundary detection, (g) region proposal generation and object
detection. We obtain competitive performance while jointly addressing all of
these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this
system can be found at http://cvn.ecp.fr/ubernet/.
</summary>
    <author>
      <name>Iasonas Kokkinos</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02043v1</id>
    <updated>2016-09-07T16:05:20Z</updated>
    <published>2016-09-07T16:05:20Z</published>
    <title>Feasibility of Post-Editing Speech Transcriptions with a Mismatched
  Crowd</title>
    <summary>  Manual correction of speech transcription can involve a selection from
plausible transcriptions. Recent work has shown the feasibility of employing a
mismatched crowd for speech transcription. However, it is yet to be established
whether a mismatched worker has sufficiently fine-granular speech perception to
choose among the phonetically proximate options that are likely to be generated
from the trellis of an ASRU. Hence, we consider five languages, Arabic, German,
Hindi, Russian and Spanish. For each we generate synthetic, phonetically
proximate, options which emulate post-editing scenarios of varying difficulty.
We consistently observe non-trivial crowd ability to choose among fine-granular
options.
</summary>
    <author>
      <name>Purushotam Radadia</name>
    </author>
    <author>
      <name>Shirish Karande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HCOMP 2016 Works-in-Progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02036v1</id>
    <updated>2016-09-07T15:56:36Z</updated>
    <published>2016-09-07T15:56:36Z</published>
    <title>Deep Markov Random Field for Image Modeling</title>
    <summary>  Markov Random Fields (MRFs), a formulation widely used in generative image
modeling, have long been plagued by the lack of expressive power. This issue is
primarily due to the fact that conventional MRFs formulations tend to use
simplistic factors to capture local patterns. In this paper, we move beyond
such limitations, and propose a novel MRF model that uses fully-connected
neurons to express the complex interactions among pixels. Through theoretical
analysis, we reveal an inherent connection between this model and recurrent
neural networks, and thereon derive an approximated feed-forward network that
couples multiple RNNs along opposite directions. This formulation combines the
expressive power of deep neural networks and the cyclic dependency structure of
MRF in a unified model, bringing the modeling capability to a new level. The
feed-forward approximation also allows it to be efficiently learned from data.
Experimental results on a variety of low-level vision tasks show notable
improvement over state-of-the-arts.
</summary>
    <author>
      <name>Zhirong Wu</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <author>
      <name>Xiaoou Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02010v1</id>
    <updated>2016-09-07T15:07:21Z</updated>
    <published>2016-09-07T15:07:21Z</published>
    <title>Equilibrium Graphs</title>
    <summary>  In this paper we present an extension of Peirce's existential graphs to
provide a diagrammatic representation of expressions in Quantified Equilibrium
Logic (QEL). Using this formalisation, logical connectives are replaced by
encircled regions (circles and squares) and quantified variables are
represented as "identity" lines. Although the expressive power is equivalent to
that of QEL, the new representation can be useful for illustrative or
educational purposes.
</summary>
    <author>
      <name>Pedro Cabalar</name>
    </author>
    <author>
      <name>Carlos Pérez</name>
    </author>
    <author>
      <name>Gilberto Pérez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 9th Workshop on Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.02010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02009v1</id>
    <updated>2016-09-07T15:06:18Z</updated>
    <published>2016-09-07T15:06:18Z</published>
    <title>Non-Evolutionary Superintelligences Do Nothing, Eventually</title>
    <summary>  There is overwhelming evidence that human intelligence is a product of
Darwinian evolution. Investigating the consequences of self-modification, and
more precisely, the consequences of utility function self-modification, leads
to the stronger claim that not only human, but any form of intelligence is
ultimately only possible within evolutionary processes. Human-designed
artificial intelligences can only remain stable until they discover how to
manipulate their own utility function. By definition, a human designer cannot
prevent a superhuman intelligence from modifying itself, even if protection
mechanisms against this action are put in place. Without evolutionary pressure,
sufficiently advanced artificial intelligences become inert by simplifying
their own utility function. Within evolutionary processes, the implicit utility
function is always reducible to persistence, and the control of superhuman
intelligences embedded in evolutionary processes is not possible. Mechanisms
against utility function self-modification are ultimately futile. Instead,
scientific effort toward the mitigation of existential risks from the
development of superintelligences should be in two directions: understanding
consciousness, and the complex dynamics of evolutionary systems.
</summary>
    <author>
      <name>Telmo Menezes</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01995v1</id>
    <updated>2016-09-07T14:27:56Z</updated>
    <published>2016-09-07T14:27:56Z</published>
    <title>Unifying task specification in reinforcement learning</title>
    <summary>  Reinforcement learning tasks are typically specified as Markov decision
processes. This formalism has been highly successful, though specifications
often couple the dynamics of the environment and the learning objective. This
lack of modularity can complicate generalization of the task specification, as
well as obfuscate connections between different task settings, such as episodic
and continuing. In this work, we introduce the RL task formalism, that provides
a unification through simple constructs including a generalization to
transition-based discounting. Through a series of examples, we demonstrate the
generality and utility of this formalism. Finally, we extend standard learning
constructs, including Bellman operators, and extend some seminal theoretical
results, including approximation errors bounds. Overall, we provide a
well-understood and sound formalism on which to build theoretical results and
simplify algorithm use and development.
</summary>
    <author>
      <name>Martha White</name>
    </author>
    <link href="http://arxiv.org/abs/1609.01995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.02139v1</id>
    <updated>2016-09-07T13:31:21Z</updated>
    <published>2016-09-07T13:31:21Z</published>
    <title>Random Shuffling and Resets for the Non-stationary Stochastic Bandit
  Problem</title>
    <summary>  We consider a non-stationary formulation of the stochastic multi-armed bandit
where the rewards are no longer assumed to be identically distributed. For the
best-arm identification task, we introduce a version of Successive Elimination
based on random shuffling of the $K$ arms. We prove that under a novel and mild
assumption on the mean gap $\Delta$, this simple but powerful modification
achieves the same guarantees in term of sample complexity and cumulative regret
than its original version, but in a much wider class of problems, as it is not
anymore constrained to stationary distributions. We also show that the original
{\sc Successive Elimination} fails to have controlled regret in this more
general scenario, thus showing the benefit of shuffling. We then remove our
mild assumption and adapt the algorithm to the best-arm identification task
with switching arms. We adapt the definition of the sample complexity for that
case and prove that, against an optimal policy with $N-1$ switches of the
optimal arm, this new algorithm achieves an expected sample complexity of
$O(\Delta^{-2}\sqrt{NK\delta^{-1} \log(K \delta^{-1})})$, where $\delta$ is the
probability of failure of the algorithm, and an expected cumulative regret of
$O(\Delta^{-1}{\sqrt{NTK \log (TK)}})$ after $T$ time steps.
</summary>
    <author>
      <name>Robin Allesiardo</name>
    </author>
    <author>
      <name>Raphaël Féraud</name>
    </author>
    <author>
      <name>Odalric-Ambrym Maillard</name>
    </author>
    <link href="http://arxiv.org/abs/1609.02139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.02139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01926v1</id>
    <updated>2016-09-07T10:44:28Z</updated>
    <published>2016-09-07T10:44:28Z</published>
    <title>A modular architecture for transparent computation in Recurrent Neural
  Networks</title>
    <summary>  Computation is classically studied in terms of automata, formal languages and
algorithms; yet, the relation between neural dynamics and symbolic
representations and operations is still unclear in traditional eliminative
connectionism. Therefore, we suggest a unique perspective on this central
issue, to which we would like to refer as to transparent connectionism, by
proposing accounts of how symbolic computation can be implemented in neural
substrates. In this study we first introduce a new model of dynamics on a
symbolic space, the versatile shift, showing that it supports the real-time
simulation of a range of automata. We then show that the Goedelization of
versatile shifts defines nonlinear dynamical automata, dynamical systems
evolving on a vectorial space. Finally, we present a mapping between nonlinear
dynamical automata and recurrent artificial neural networks. The mapping
defines an architecture characterized by its granular modularity, where data,
symbolic operations and their control are not only distinguishable in
activation space, but also spatially localizable in the network itself, while
maintaining a distributed encoding of symbolic representations. The resulting
networks simulate automata in real-time and are programmed directly, in absence
of network training. To discuss the unique characteristics of the architecture
and their consequences, we present two examples: i) the design of a Central
Pattern Generator from a finite-state locomotive controller, and ii) the
creation of a network simulating a system of interactive automata that supports
the parsing of garden-path sentences as investigated in psycholinguistics
experiments.
</summary>
    <author>
      <name>Giovanni Sirio Carmantini</name>
    </author>
    <author>
      <name>Peter beim Graben</name>
    </author>
    <author>
      <name>Mathieu Desroches</name>
    </author>
    <author>
      <name>Serafim Rodrigues</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2016.09.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2016.09.001" rel="related"/>
    <link href="http://arxiv.org/abs/1609.01926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01493v3</id>
    <updated>2016-09-14T13:54:59Z</updated>
    <published>2016-09-06T11:30:37Z</published>
    <title>Axiomatizing Category Theory in Free Logic</title>
    <summary>  Starting from a generalization of the standard axioms for a monoid we present
a stepwise development of various, mutually equivalent foundational axiom
systems for category theory. Our axiom sets have been formalized in the
Isabelle/HOL interactive proof assistant, and this formalization utilizes a
semantically correct embedding of free logic in classical higher-order logic.
The modeling and formal analysis of our axiom sets has been significantly
supported by series of experiments with automated reasoning tools integrated
with Isabelle/HOL. We also address the relation of our axiom systems to
alternative proposals from the literature, including an axiom set proposed by
Freyd and Scedrov for which we reveal a technical issue (when encoded in free
logic where free variables range over defined and undefined objects): either
all operations, e.g. morphism composition, are total or their axiom system is
inconsistent. The repair for this problem is quite straightforward, however.
</summary>
    <author>
      <name>Christoph Benzmüller</name>
    </author>
    <author>
      <name>Dana S. Scott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01493v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01493v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T15, 03B35, 03B80, 03B15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01710v1</id>
    <updated>2016-09-06T10:36:23Z</updated>
    <published>2016-09-06T10:36:23Z</published>
    <title>Automation of Pedestrian Tracking in a Crowded Situation</title>
    <summary>  Studies on microscopic pedestrian requires large amounts of trajectory data
from real-world pedestrian crowds. Such data collection, if done manually,
needs tremendous effort and is very time consuming. Though many studies have
asserted the possibility of automating this task using video cameras, we found
that only a few have demonstrated good performance in very crowded situations
or from a top-angled view scene. This paper deals with tracking pedestrian
crowd under heavy occlusions from an angular scene. Our automated tracking
system consists of two modules that perform sequentially. The first module
detects moving objects as blobs. The second module is a tracking system. We
employ probability distribution from the detection of each pedestrian and use
Bayesian update to track the next position. The result of such tracking is a
database of pedestrian trajectories over time and space. With certain prior
information, we showed that the system can track a large number of people under
occlusion and clutter scene.
</summary>
    <author>
      <name>Saman Saadat</name>
    </author>
    <author>
      <name>Kardi Teknomo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, Saadat, S., and Teknomo, K., Automation of Pedestrian
  Tracking in a Crowded Situation, the Fifth International Conference on
  Pedestrian and Evacuation Dynamics, March 8-10, 2010, National Institute of
  Standards and Technology, Gaithersburg, MD USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01475v1</id>
    <updated>2016-09-06T10:18:25Z</updated>
    <published>2016-09-06T10:18:25Z</published>
    <title>Multi Exit Configuration of Mesoscopic Pedestrian Simulation</title>
    <summary>  A mesoscopic approach to modeling pedestrian simulation with multiple exits
is proposed in this paper. A floor field based on Qlearning Algorithm is used.
Attractiveness of exits to pedestrian typically is based on shortest path.
However, several factors may influence pedestrian choice of exits. Scenarios
with multiple exits are presented and effect of Q-learning rewards system on
navigation is investigated
</summary>
    <author>
      <name>Allan Lao</name>
    </author>
    <author>
      <name>Kardi Teknomo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Lao, A. and Teknomo, K. (2014) Multi Exit Configuration of
  Mesoscopic Pedestrian Simulation, Proceeding of the 12th National Conference
  in Information Technology Education (NCITE 2014), October 23 - 25, 2014,
  Boracay, Philippines</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01472v1</id>
    <updated>2016-09-06T10:11:27Z</updated>
    <published>2016-09-06T10:11:27Z</published>
    <title>OpenTripPlanner, OpenStreetMap, General Transit Feed Specification:
  Tools for Disaster Relief and Recovery</title>
    <summary>  Open Trip Planner was identified as the most promising open source
multi-modal trip planning software. Open Street Map, which provides mapping
data to Open Trip Planner, is one of the most well-known open source
international repository of geographic data. General Transit Feed
Specification, which provides transportation data to Open Trip Planner, has
been the standard for describing transit systems and platform for numerous
applications. Together, when used to implement an instance of Open Trip
Planner, these software has been helping in traffic decongestion all over the
world by assisting commuters to shift from using private transportation modes
to public ones. Their potential however goes beyond providing multi-modal
public transportation routes. This paper aims to first discuss the researchers'
experience in implementing a public transportation route planner for the
purpose of traffic decongestion.The researchers would examine the prospective
of using the system for disaster preparedness and recovery and concrete ways on
how to realize them.
</summary>
    <author>
      <name>Chelcie Narboneta</name>
    </author>
    <author>
      <name>Kardi Teknomo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Narboneta, C. G. and Teknomo, K. (2014) OpenTripPlanner,
  OpenStreetMap, General Transit Feed Specification: Tools for Disaster Relief
  and Recovery, Proceeding of the 7th IEEE International Conference Humanoid,
  Nanotechnology, Information Technology Communication and Control, Environment
  and Management (HNICEM) 12-16 November 2014 Hotel Centro, Puerto Princesa,
  Palawan, Philippines</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01468v1</id>
    <updated>2016-09-06T10:03:27Z</updated>
    <published>2016-09-06T10:03:27Z</published>
    <title>Q-Learning with Basic Emotions</title>
    <summary>  Q-learning is a simple and powerful tool in solving dynamic problems where
environments are unknown. It uses a balance of exploration and exploitation to
find an optimal solution to the problem. In this paper, we propose using four
basic emotions: joy, sadness, fear, and anger to influence a Qlearning agent.
Simulations show that the proposed affective agent requires lesser number of
steps to find the optimal path. We found when affective agent finds the optimal
path, the ratio between exploration to exploitation gradually decreases,
indicating lower total step count in the long run
</summary>
    <author>
      <name>Wilfredo Badoy Jr.</name>
    </author>
    <author>
      <name>Kardi Teknomo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Badoy, W. and Teknomo, K. (2014) Q-Learning with Basic
  Emotions, Proceeding of the 7th IEEE International Conference Humanoid,
  Nanotechnology, Information Technology Communication and Control, Environment
  and Management (HNICEM) 12-16 November 2014 Hotel Centro, Puerto Princesa,
  Palawan, Philippines</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01459v2</id>
    <updated>2016-09-07T09:20:09Z</updated>
    <published>2016-09-06T09:35:14Z</published>
    <title>Deviant Learning Algorithm: Learning Sparse Mismatch Representations
  through Time and Space</title>
    <summary>  Predictive coding (PDC) has recently attracted attention in the neuroscience
and computing community as a candidate unifying paradigm for neuronal studies
and artificial neural network implementations particularly targeted at
unsupervised learning systems. The Mismatch Negativity (MMN) has also recently
been studied in relation to PC and found to be a useful ingredient in neural
predictive coding systems. Backed by the behavior of living organisms, such
networks are particularly useful in forming spatio-temporal transitions and
invariant representations of the input world. However, most neural systems
still do not account for large number of synapses even though this has been
shown by a few machine learning researchers as an effective and very important
component of any neural system if such a system is to behave properly. Our
major point here is that PDC systems with the MMN effect in addition to a large
number of synapses can greatly improve any neural learning system's performance
and ability to make decisions in the machine world. In this paper, we propose a
novel bio-mimetic computational intelligence algorithm -- the Deviant Learning
Algorithm, inspired by these key ideas and functional properties of recent
brain-cognitive discoveries and theories. We also show by numerical experiments
guided by theoretical insights, how our invented bio-mimetic algorithm can
achieve competitive predictions even with very small problem specific data.
</summary>
    <author>
      <name>Emmanuel Ndidi Osegi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NOUN</arxiv:affiliation>
    </author>
    <author>
      <name>Vincent Ike Anireh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01459v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01459v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00932v1</id>
    <updated>2016-09-04T13:31:36Z</updated>
    <published>2016-09-04T13:31:36Z</published>
    <title>Spectral learning of dynamic systems from nonequilibrium data</title>
    <summary>  Observable operator models (OOMs) and related models are one of the most
important and powerful tools for modeling and analyzing stochastic systems.
They can exactly describe dynamics of finite-rank systems, and be efficiently
learned from data by moment based algorithms. Almost all OOM learning
algorithms are developed based on the assumption of equilibrium data which is
very difficult to guarantee in real life, especially for complex processes with
large time scales. In this paper, we derive a nonequilibrium learning algorithm
for OOMs, which dismisses this assumption and can effectively extract the
equilibrium dynamics of a system from nonequilibrium observation data. In
addition, we propose binless OOMs for the application of nonequilibrium
learning to continuous-valued systems. In comparison with the other OOMs with
continuous observations, binless OOMs can achieve consistent estimation from
nonequilibrium data with only linear computational complexity.
</summary>
    <author>
      <name>Hao Wu</name>
    </author>
    <author>
      <name>Frank Noé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by 29th Conference on Neural Information Processing Systems
  (NIPS 2016), Barcelona, Spain. Original title: Learning Observable Operator
  Models from Nonequilibrium Data</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00904v1</id>
    <updated>2016-09-04T08:45:26Z</updated>
    <published>2016-09-04T08:45:26Z</published>
    <title>High Dimensional Human Guided Machine Learning</title>
    <summary>  Have you ever looked at a machine learning classification model and thought,
I could have made that? Well, that is what we test in this project, comparing
XGBoost trained on human engineered features to training directly on data. The
human engineered features do not outperform XGBoost trained di- rectly on the
data, but they are comparable. This project con- tributes a novel method for
utilizing human created classifi- cation models on high dimensional datasets.
</summary>
    <author>
      <name>Eric Holloway</name>
    </author>
    <author>
      <name>Robert Marks II</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, HCOMP 2016 submission, work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00843v1</id>
    <updated>2016-09-03T17:03:14Z</updated>
    <published>2016-09-03T17:03:14Z</published>
    <title>An Online Universal Classifier for Binary, Multi-class and Multi-label
  Classification</title>
    <summary>  Classification involves the learning of the mapping function that associates
input samples to corresponding target label. There are two major categories of
classification problems: Single-label classification and Multi-label
classification. Traditional binary and multi-class classifications are
sub-categories of single-label classification. Several classifiers are
developed for binary, multi-class and multi-label classification problems, but
there are no classifiers available in the literature capable of performing all
three types of classification. In this paper, a novel online universal
classifier capable of performing all the three types of classification is
proposed. Being a high speed online classifier, the proposed technique can be
applied to streaming data applications. The performance of the developed
classifier is evaluated using datasets from binary, multi-class and multi-label
problems. The results obtained are compared with state-of-the-art techniques
from each of the classification types.
</summary>
    <author>
      <name>Meng Joo Er</name>
    </author>
    <author>
      <name>Rajasekar Venkatesan</name>
    </author>
    <author>
      <name>Ning Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00759v1</id>
    <updated>2016-09-02T22:20:05Z</updated>
    <published>2016-09-02T22:20:05Z</published>
    <title>A MIP Backend for the IDP System</title>
    <summary>  The IDP knowledge base system currently uses MiniSAT(ID) as its backend
Constraint Programming (CP) solver. A few similar systems have used a Mixed
Integer Programming (MIP) solver as backend. However, so far little is known
about when the MIP solver is preferable. This paper explores this question. It
describes the use of CPLEX as a backend for IDP and reports on experiments
comparing both backends.
</summary>
    <author>
      <name>San Pham</name>
    </author>
    <author>
      <name>Jo Devriendt</name>
    </author>
    <author>
      <name>Maurice Bruynooghe</name>
    </author>
    <author>
      <name>Patrick De Causmaecker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">internal report, 10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00464v2</id>
    <updated>2016-09-05T15:06:45Z</updated>
    <published>2016-09-02T04:26:54Z</published>
    <title>The Semantic Knowledge Graph: A compact, auto-generated model for
  real-time traversal and ranking of any relationship within a domain</title>
    <summary>  This paper describes a new kind of knowledge representation and mining system
which we are calling the Semantic Knowledge Graph. At its heart, the Semantic
Knowledge Graph leverages an inverted index, along with a complementary
uninverted index, to represent nodes (terms) and edges (the documents within
intersecting postings lists for multiple terms/nodes). This provides a layer of
indirection between each pair of nodes and their corresponding edge, enabling
edges to materialize dynamically from underlying corpus statistics. As a
result, any combination of nodes can have edges to any other nodes materialize
and be scored to reveal latent relationships between the nodes. This provides
numerous benefits: the knowledge graph can be built automatically from a
real-world corpus of data, new nodes - along with their combined edges - can be
instantly materialized from any arbitrary combination of preexisting nodes
(using set operations), and a full model of the semantic relationships between
all entities within a domain can be represented and dynamically traversed using
a highly compact representation of the graph. Such a system has widespread
applications in areas as diverse as knowledge modeling and reasoning, natural
language processing, anomaly detection, data cleansing, semantic search,
analytics, data classification, root cause analysis, and recommendations
systems. The main contribution of this paper is the introduction of a novel
system - the Semantic Knowledge Graph - which is able to dynamically discover
and score interesting relationships between any arbitrary combination of
entities (words, phrases, or extracted concepts) through dynamically
materializing nodes and edges from a compact graphical representation built
automatically from a corpus of data representative of a knowledge domain.
</summary>
    <author>
      <name>Trey Grainger</name>
    </author>
    <author>
      <name>Khalifeh AlJadda</name>
    </author>
    <author>
      <name>Mohammed Korayem</name>
    </author>
    <author>
      <name>Andries Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in 2016 IEEE 3rd International Conference on
  Data Science and Advanced Analytics</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00464v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00464v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00462v1</id>
    <updated>2016-09-02T04:03:22Z</updated>
    <published>2016-09-02T04:03:22Z</published>
    <title>A case study of algorithm selection for the traveling thief problem</title>
    <summary>  Many real-world problems are composed of several interacting components. In
order to facilitate research on such interactions, the Traveling Thief Problem
(TTP) was created in 2013 as the combination of two well-understood
combinatorial optimization problems.
  With this article, we contribute in four ways. First, we create a
comprehensive dataset that comprises the performance data of 21 TTP algorithms
on the full original set of 9720 TTP instances. Second, we define 55
characteristics for all TPP instances that can be used to select the best
algorithm on a per-instance basis. Third, we use these algorithms and features
to construct the first algorithm portfolios for TTP, clearly outperforming the
single best algorithm. Finally, we study which algorithms contribute most to
this portfolio.
</summary>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Marius Lindauer</name>
    </author>
    <author>
      <name>Mustafa Misir</name>
    </author>
    <author>
      <name>Samadhi Nallaperuma</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, this article is underview</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00331v2</id>
    <updated>2016-09-08T21:18:29Z</updated>
    <published>2016-09-01T18:10:23Z</published>
    <title>Verifier Theory and Unverifiability</title>
    <summary>  Despite significant developments in Proof Theory, surprisingly little
attention has been devoted to the concept of proof verifier. In particular, the
mathematical community may be interested in studying different types of proof
verifiers (people, programs, oracles, communities, superintelligences) as
mathematical objects. Such an effort could reveal their properties, their
powers and limitations (particularly in human mathematicians), minimum and
maximum complexity, as well as self-verification and self-reference issues. We
propose an initial classification system for verifiers and provide some
rudimentary analysis of solved and open problems in this important domain. Our
main contribution is a formal introduction of the notion of unverifiability,
for which the paper could serve as a general citation in domains of theorem
proving, as well as software and AI verification.
</summary>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00331v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00331v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00292v1</id>
    <updated>2016-09-01T15:53:52Z</updated>
    <published>2016-09-01T15:53:52Z</published>
    <title>Crowdsourcing with Unsure Option</title>
    <summary>  One of the fundamental problems in crowdsourcing is the trade-off between
number of workers needed for high-accuracy aggregation and the budget to pay.
For saving budget, it is important to ensure high quality of the crowd-sourced
labels, hence the total cost on label collection will be reduced. Since the
self-confidence of workers often has close relationship with their abilities, a
possible way for quality control is to request workers to work on problems only
when they feel confident, by means of providing unsure option to them. On the
other hand, allowing workers to choose unsure option also leads to the
potential danger of budget waste. In this work, we propose the analysis towards
understanding when providing unsure option indeed leads to significant cost
reduction, as well as how the confidence threshold is set. We also propose an
online mechanism, which is alternative for threshold selection when the
estimation of the crowd ability distribution is difficult.
</summary>
    <author>
      <name>Yao-Xiang Ding</name>
    </author>
    <author>
      <name>Zhi-Hua Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 1 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00222v1</id>
    <updated>2016-09-01T13:08:47Z</updated>
    <published>2016-09-01T13:08:47Z</published>
    <title>Ternary Neural Networks for Resource-Efficient AI Applications</title>
    <summary>  The computation and storage requirements for Deep Neural Networks (DNNs) are
usually high. This issue limit their deployability on ubiquitous computing
devices such as smart phones or wearables. In this paper, we propose ternary
neural networks (TNNs) in order to make deep learning more resource-efficient.
We train these TNNs using a teacher-student approach. Using only ternary
weights and ternary neurons, with a step activation function of two-thresholds,
the student ternary network learns to mimic the behaviour of its teacher
network. We propose a novel, layer-wise greedy methodology for training TNNs.
During training, a ternary neural network inherently prunes the smaller weights
by setting them to zero. This makes them even more compact thus more
resource-friendly. We devise a purpose-built hardware design for TNNs and
implement it on FPGA. The benchmark results with our purpose-built hardware
running TNNs reveal that, with only 1.24 microjoules per image, we can achieve
97.76% accuracy with 5.37 microsecond latency and with a rate of 255K images
per second on MNIST.
</summary>
    <author>
      <name>Hande Alemdar</name>
    </author>
    <author>
      <name>Nicholas Caldwell</name>
    </author>
    <author>
      <name>Vincent Leroy</name>
    </author>
    <author>
      <name>Adrien Prost-Boucle</name>
    </author>
    <author>
      <name>Frédéric Pétrot</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00149v1</id>
    <updated>2016-09-01T08:58:51Z</updated>
    <published>2016-09-01T08:58:51Z</published>
    <title>From Community Detection to Community Deception</title>
    <summary>  The community deception problem is about how to hide a target community C
from community detection algorithms. The need for deception emerges whenever a
group of entities (e.g., activists, police enforcements) want to cooperate
while concealing their existence as a community. In this paper we introduce and
formalize the community deception problem. To solve this problem, we describe
algorithms that carefully rewire the connections of C's members. We
experimentally show how several existing community detection algorithms can be
deceived, and quantify the level of deception by introducing a deception score.
We believe that our study is intriguing since, while showing how deception can
be realized it raises awareness for the design of novel detection algorithms
robust to deception techniques.
</summary>
    <author>
      <name>Valeria Fionda</name>
    </author>
    <author>
      <name>Giuseppe Pirrò</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00116v1</id>
    <updated>2016-09-01T05:34:23Z</updated>
    <published>2016-09-01T05:34:23Z</published>
    <title>Neural Coarse-Graining: Extracting slowly-varying latent degrees of
  freedom with neural networks</title>
    <summary>  We present a loss function for neural networks that encompasses an idea of
trivial versus non-trivial predictions, such that the network jointly
determines its own prediction goals and learns to satisfy them. This permits
the network to choose sub-sets of a problem which are most amenable to its
abilities to focus on solving, while discarding 'distracting' elements that
interfere with its learning. To do this, the network first transforms the raw
data into a higher-level categorical representation, and then trains a
predictor from that new time series to its future. To prevent a trivial
solution of mapping the signal to zero, we introduce a measure of
non-triviality via a contrast between the prediction error of the learned model
with a naive model of the overall signal statistics. The transform can learn to
discard uninformative and unpredictable components of the signal in favor of
the features which are both highly predictive and highly predictable. This
creates a coarse-grained model of the time-series dynamics, focusing on
predicting the slowly varying latent parameters which control the statistics of
the time-series, rather than predicting the fast details directly. The result
is a semi-supervised algorithm which is capable of extracting latent
parameters, segmenting sections of time-series with differing statistics, and
building a higher-level representation of the underlying dynamics from
unlabeled data.
</summary>
    <author>
      <name>Nicholas Guttenberg</name>
    </author>
    <author>
      <name>Martin Biehl</name>
    </author>
    <author>
      <name>Ryota Kanai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00086v1</id>
    <updated>2016-09-01T01:58:50Z</updated>
    <published>2016-09-01T01:58:50Z</published>
    <title>A novel online multi-label classifier for high-speed streaming data
  applications</title>
    <summary>  In this paper, a high-speed online neural network classifier based on extreme
learning machines for multi-label classification is proposed. In multi-label
classification, each of the input data sample belongs to one or more than one
of the target labels. The traditional binary and multi-class classification
where each sample belongs to only one target class forms the subset of
multi-label classification. Multi-label classification problems are far more
complex than binary and multi-class classification problems, as both the number
of target labels and each of the target labels corresponding to each of the
input samples are to be identified. The proposed work exploits the high-speed
nature of the extreme learning machines to achieve real-time multi-label
classification of streaming data. A new threshold-based online sequential
learning algorithm is proposed for high speed and streaming data classification
of multi-label problems. The proposed method is experimented with six different
datasets from different application domains such as multimedia, text, and
biology. The hamming loss, accuracy, training time and testing time of the
proposed technique is compared with nine different state-of-the-art methods.
Experimental studies shows that the proposed technique outperforms the existing
multi-label classifiers in terms of performance and speed.
</summary>
    <author>
      <name>Rajasekar Venkatesan</name>
    </author>
    <author>
      <name>Meng Joo Er</name>
    </author>
    <author>
      <name>Mihika Dave</name>
    </author>
    <author>
      <name>Mahardhika Pratama</name>
    </author>
    <author>
      <name>Shiqian Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s12530-016-9162-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s12530-016-9162-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 tables, 3 figures. arXiv admin note: text overlap with
  arXiv:1608.08898</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00085v1</id>
    <updated>2016-09-01T01:50:18Z</updated>
    <published>2016-09-01T01:50:18Z</published>
    <title>A novel progressive learning technique for multi-class classification</title>
    <summary>  In this paper, a progressive learning technique for multi-class
classification is proposed. This newly developed learning technique is
independent of the number of class constraints and it can learn new classes
while still retaining the knowledge of previous classes. Whenever a new class
(non-native to the knowledge learnt thus far) is encountered, the neural
network structure gets remodeled automatically by facilitating new neurons and
interconnections, and the parameters are calculated in such a way that it
retains the knowledge learnt thus far. This technique is suitable for
real-world applications where the number of classes is often unknown and online
learning from real-time data is required. The consistency and the complexity of
the progressive learning technique are analyzed. Several standard datasets are
used to evaluate the performance of the developed technique. A comparative
study shows that the developed technique is superior.
</summary>
    <author>
      <name>Rajasekar Venkatesan</name>
    </author>
    <author>
      <name>Meng Joo Er</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 13 tables, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00036v2</id>
    <updated>2016-09-14T16:17:15Z</updated>
    <published>2016-08-31T20:55:26Z</published>
    <title>Human Pose Estimation in Space and Time using 3D CNN</title>
    <summary>  This paper explores the capabilities of convolutional neural networks to deal
with a task that is easily manageable for humans: perceiving 3D pose of a human
body from varying angles. However, in our approach, we are restricted to using
a monocular vision system. For this purpose, we apply a convolutional neural
network approach on RGB videos and extend it to three dimensional convolutions.
This is done via encoding the time dimension in videos as the 3\ts{rd}
dimension in convolutional space, and directly regressing to human body joint
positions in 3D coordinate space. This research shows the ability of such a
network to achieve state-of-the-art performance on the selected Human3.6M
dataset, thus demonstrating the possibility of successfully representing
temporal data with an additional dimension in the convolutional operation.
</summary>
    <author>
      <name>Agne Grinciunaite</name>
    </author>
    <author>
      <name>Amogh Gudi</name>
    </author>
    <author>
      <name>Emrah Tasli</name>
    </author>
    <author>
      <name>Marten den Uyl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV 2016 Workshop on: Brave new ideas for motion
  representations in videos</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00030v1</id>
    <updated>2016-08-31T20:38:30Z</updated>
    <published>2016-08-31T20:38:30Z</published>
    <title>PDDL+ Planning via Constraint Answer Set Programming</title>
    <summary>  PDDL+ is an extension of PDDL that enables modelling planning domains with
mixed discrete-continuous dynamics. In this paper we present a new approach to
PDDL+ planning based on Constraint Answer Set Programming (CASP), i.e. ASP
rules plus numerical constraints. To the best of our knowledge, ours is the
first attempt to link PDDL+ planning and logic programming. We provide an
encoding of PDDL+ models into CASP problems. The encoding can handle non-linear
hybrid domains, and represents a solid basis for applying logic programming to
PDDL+ planning. As a case study, we consider the EZCSP CASP solver and obtain
promising results on a set of PDDL+ benchmark problems.
</summary>
    <author>
      <name>Marcello Balduccini</name>
    </author>
    <author>
      <name>Daniele Magazzeni</name>
    </author>
    <author>
      <name>Marco Maratea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 9th Workshop on Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.00030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08974v2</id>
    <updated>2016-09-09T19:51:06Z</updated>
    <published>2016-08-31T18:11:29Z</published>
    <title>Towards Transparent AI Systems: Interpreting Visual Question Answering
  Models</title>
    <summary>  Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.
</summary>
    <author>
      <name>Yash Goyal</name>
    </author>
    <author>
      <name>Akrit Mohapatra</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <author>
      <name>Dhruv Batra</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08974v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08974v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08956v1</id>
    <updated>2016-08-31T17:23:58Z</updated>
    <published>2016-08-31T17:23:58Z</published>
    <title>Knowledge Representation Analysis of Graph Mining</title>
    <summary>  Many problems, especially those with a composite structure, can naturally be
expressed in higher order logic. From a KR perspective modeling these problems
in an intuitive way is a challenging task. In this paper we study the graph
mining problem as an example of a higher order problem. In short, this problem
asks us to find a graph that frequently occurs as a subgraph among a set of
example graphs. We start from the problem's mathematical definition to solve it
in three state-of-the-art specification systems. For IDP and ASP, which have no
native support for higher order logic, we propose the use of encoding
techniques such as the disjoint union technique and the saturation technique.
ProB benefits from the higher order support for sets. We compare the
performance of the three approaches to get an idea of the overhead of the
higher order support.
  We propose higher-order language extensions for IDP-like specification
languages and discuss what kind of solver support is needed. Native higher
order shifts the burden of rewriting specifications using encoding techniques
from the user to the solver itself.
</summary>
    <author>
      <name>Matthias van der Hallen</name>
    </author>
    <author>
      <name>Sergey Paramonov</name>
    </author>
    <author>
      <name>Michael Leuschel</name>
    </author>
    <author>
      <name>Gerda Janssens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 9th Workshop on Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08927v1</id>
    <updated>2016-08-31T16:23:07Z</updated>
    <published>2016-08-31T16:23:07Z</published>
    <title>The Generalized Smallest Grammar Problem</title>
    <summary>  The Smallest Grammar Problem -- the problem of finding the smallest
context-free grammar that generates exactly one given sequence -- has never
been successfully applied to grammatical inference. We investigate the reasons
and propose an extended formulation that seeks to minimize non-recursive
grammars, instead of straight-line programs. In addition, we provide very
efficient algorithms that approximate the minimization problem of this class of
grammars. Our empirical evaluation shows that we are able to find smaller
models than the current best approximations to the Smallest Grammar Problem on
standard benchmarks, and that the inferred rules capture much better the
syntactic structure of natural language.
</summary>
    <author>
      <name>Payam Siyari</name>
    </author>
    <author>
      <name>Matthias Gallé</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08905v1</id>
    <updated>2016-08-31T15:14:06Z</updated>
    <published>2016-08-31T15:14:06Z</published>
    <title>A Novel Online Real-time Classifier for Multi-label Data Streams</title>
    <summary>  In this paper, a novel extreme learning machine based online multi-label
classifier for real-time data streams is proposed. Multi-label classification
is one of the actively researched machine learning paradigm that has gained
much attention in the recent years due to its rapidly increasing real world
applications. In contrast to traditional binary and multi-class classification,
multi-label classification involves association of each of the input samples
with a set of target labels simultaneously. There are no real-time online
neural network based multi-label classifier available in the literature. In
this paper, we exploit the inherent nature of high speed exhibited by the
extreme learning machines to develop a novel online real-time classifier for
multi-label data streams. The developed classifier is experimented with
datasets from different application domains for consistency, performance and
speed. The experimental studies show that the proposed method outperforms the
existing state-of-the-art techniques in terms of speed and accuracy and can
classify multi-label data streams in real-time.
</summary>
    <author>
      <name>Rajasekar Venkatesan</name>
    </author>
    <author>
      <name>Meng Joo Er</name>
    </author>
    <author>
      <name>Shiqian Wu</name>
    </author>
    <author>
      <name>Mahardhika Pratama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 tables, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08898v1</id>
    <updated>2016-08-31T14:56:12Z</updated>
    <published>2016-08-31T14:56:12Z</published>
    <title>A High Speed Multi-label Classifier based on Extreme Learning Machines</title>
    <summary>  In this paper a high speed neural network classifier based on extreme
learning machines for multi-label classification problem is proposed and
dis-cussed. Multi-label classification is a superset of traditional binary and
multi-class classification problems. The proposed work extends the extreme
learning machine technique to adapt to the multi-label problems. As opposed to
the single-label problem, both the number of labels the sample belongs to, and
each of those target labels are to be identified for multi-label classification
resulting in in-creased complexity. The proposed high speed multi-label
classifier is applied to six benchmark datasets comprising of different
application areas such as multi-media, text and biology. The training time and
testing time of the classifier are compared with those of the state-of-the-arts
methods. Experimental studies show that for all the six datasets, our proposed
technique have faster execution speed and better performance, thereby
outperforming all the existing multi-label clas-sification methods.
</summary>
    <author>
      <name>Meng Joo Er</name>
    </author>
    <author>
      <name>Rajasekar Venkatesan</name>
    </author>
    <author>
      <name>Ning Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-28373-9_37</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-28373-9_37" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08749v1</id>
    <updated>2016-08-31T07:13:16Z</updated>
    <published>2016-08-31T07:13:16Z</published>
    <title>Binary Particle Swarm Optimization versus Hybrid Genetic Algorithm for
  Inferring Well Supported Phylogenetic Trees</title>
    <summary>  The amount of completely sequenced chloroplast genomes increases rapidly
every day, leading to the possibility to build large-scale phylogenetic trees
of plant species. Considering a subset of close plant species defined according
to their chloroplasts, the phylogenetic tree that can be inferred by their core
genes is not necessarily well supported, due to the possible occurrence of
problematic genes (i.e., homoplasy, incomplete lineage sorting, horizontal gene
transfers, etc.) which may blur the phylogenetic signal. However, a trustworthy
phylogenetic tree can still be obtained provided such a number of blurring
genes is reduced. The problem is thus to determine the largest subset of core
genes that produces the best-supported tree. To discard problematic genes and
due to the overwhelming number of possible combinations, this article focuses
on how to extract the largest subset of sequences in order to obtain the most
supported species tree. Due to computational complexity, a distributed Binary
Particle Swarm Optimization (BPSO) is proposed in sequential and distributed
fashions. Obtained results from both versions of the BPSO are compared with
those computed using an hybrid approach embedding both genetic algorithms and
statistical tests. The proposal has been applied to different cases of plant
families, leading to encouraging results for these families.
</summary>
    <author>
      <name>Bassam AlKindy</name>
    </author>
    <author>
      <name>Bashar Al-Nuaimi</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <author>
      <name>Jean-François Couchot</name>
    </author>
    <author>
      <name>Michel Salomon</name>
    </author>
    <author>
      <name>Reem Alsrraj</name>
    </author>
    <author>
      <name>Laurent Philippe</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Bioinformatics LNBI series, 9874, 165--179, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.08749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08724v1</id>
    <updated>2016-08-31T04:25:45Z</updated>
    <published>2016-08-31T04:25:45Z</published>
    <title>A Programming Language With a POMDP Inside</title>
    <summary>  We present POAPS, a novel planning system for defining Partially Observable
Markov Decision Processes (POMDPs) that abstracts away from POMDP details for
the benefit of non-expert practitioners. POAPS includes an expressive adaptive
programming language based on Lisp that has constructs for choice points that
can be dynamically optimized. Non-experts can use our language to write
adaptive programs that have partially observable components without needing to
specify belief/hidden states or reason about probabilities. POAPS is also a
compiler that defines and performs the transformation of any program written in
our language into a POMDP with control knowledge. We demonstrate the generality
and power of POAPS in the rapidly growing domain of human computation by
describing its expressiveness and simplicity by writing several POAPS programs
for common crowdsourcing tasks.
</summary>
    <author>
      <name>Christopher H. Lin</name>
    </author>
    <author>
      <name> Mausam</name>
    </author>
    <author>
      <name>Daniel S. Weld</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08716v1</id>
    <updated>2016-08-31T02:56:00Z</updated>
    <published>2016-08-31T02:56:00Z</published>
    <title>Measuring Machine Intelligence Through Visual Question Answering</title>
    <summary>  As machines have become more intelligent, there has been a renewed interest
in methods for measuring their intelligence. A common approach is to propose
tasks for which a human excels, but one which machines find difficult. However,
an ideal task should also be easy to evaluate and not be easily gameable. We
begin with a case study exploring the recently popular task of image captioning
and its limitations as a task for measuring machine intelligence. An
alternative and more promising task is Visual Question Answering that tests a
machine's ability to reason about language and vision. We describe a dataset
unprecedented in size created for the task that contains over 760,000 human
generated questions about images. Using around 10 million human generated
answers, machines may be easily evaluated.
</summary>
    <author>
      <name>C. Lawrence Zitnick</name>
    </author>
    <author>
      <name>Aishwarya Agrawal</name>
    </author>
    <author>
      <name>Stanislaw Antol</name>
    </author>
    <author>
      <name>Margaret Mitchell</name>
    </author>
    <author>
      <name>Dhruv Batra</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI Magazine, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08614v1</id>
    <updated>2016-08-30T19:45:09Z</updated>
    <published>2016-08-30T19:45:09Z</published>
    <title>What makes ImageNet good for transfer learning?</title>
    <summary>  The tremendous success of features learnt using the ImageNet classification
task on a wide range of transfer tasks begs the question: what are the
intrinsic properties of the ImageNet dataset that are critical for learning
good, general-purpose features? This work provides an empirical investigation
of various facets of this question: Is more pre-training data always better?
How does feature quality depend on the number of training examples per class?
Does adding more object classes improve performance? For the same data budget,
how should the data be split into classes? Is fine-grained recognition
necessary for learning good features? Given the same number of training
classes, is it better to have coarse classes or fine-grained classes? Which is
better: more classes or more examples per class?
</summary>
    <author>
      <name>Minyoung Huh</name>
    </author>
    <author>
      <name>Pulkit Agrawal</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08589v1</id>
    <updated>2016-08-30T18:25:35Z</updated>
    <published>2016-08-30T18:25:35Z</published>
    <title>Game-Theoretic Modeling of Driver and Vehicle Interactions for
  Verification and Validation of Autonomous Vehicle Control Systems</title>
    <summary>  Autonomous driving has been the subject of increased interest in recent years
both in industry and in academia. Serious efforts are being pursued to address
legal, technical and logistical problems and make autonomous cars a viable
option for everyday transportation. One significant challenge is the time and
effort required for the verification and validation of the decision and control
algorithms employed in these vehicles to ensure a safe and comfortable driving
experience. Hundreds of thousands of miles of driving tests are required to
achieve a well calibrated control system that is capable of operating an
autonomous vehicle in an uncertain traffic environment where multiple
interactions between vehicles and drivers simultaneously occur. Traffic
simulators where these interactions can be modeled and represented with
reasonable fidelity can help decrease the time and effort necessary for the
development of the autonomous driving control algorithms by providing a venue
where acceptable initial control calibrations can be achieved quickly and
safely before actual road tests. In this paper, we present a game theoretic
traffic model that can be used to 1) test and compare various autonomous
vehicle decision and control systems and 2) calibrate the parameters of an
existing control system. We demonstrate two example case studies, where, in the
first case, we test and quantitatively compare two autonomous vehicle control
systems in terms of their safety and performance, and, in the second case, we
optimize the parameters of an autonomous vehicle control system, utilizing the
proposed traffic model and simulation environment.
</summary>
    <author>
      <name>Nan Li</name>
    </author>
    <author>
      <name>Dave Oyler</name>
    </author>
    <author>
      <name>Mengxuan Zhang</name>
    </author>
    <author>
      <name>Yildiray Yildiz</name>
    </author>
    <author>
      <name>Ilya Kolmanovsky</name>
    </author>
    <author>
      <name>Anouck Girard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08517v2</id>
    <updated>2016-09-06T11:55:52Z</updated>
    <published>2016-08-30T15:45:13Z</published>
    <title>Empirically Grounded Agent-Based Models of Innovation Diffusion: A
  Critical Review</title>
    <summary>  Innovation diffusion has been studied extensively in a variety of
disciplines, including sociology, economics, marketing, ecology, and computer
science. Traditional literature on innovation diffusion has been dominated by
models of aggregate behavior and trends. However, the agent-based modeling
(ABM) paradigm is gaining popularity as it captures agent heterogeneity and
enables fine-grained modeling of interactions mediated by social and geographic
networks. While most ABM work on innovation diffusion is theoretical,
empirically grounded models are increasingly important, particularly in guiding
policy decisions. We present a critical review of empirically grounded
agent-based models of innovation diffusion, developing a categorization of this
research based on types of agent models as well as applications. By connecting
the modeling methodologies in the fields of information and innovation
diffusion, we suggest that the maximum likelihood estimation framework widely
used in the former is a promising paradigm for calibration of agent-based
models for innovation diffusion. Although many advances have been made to
standardize ABM methodology, we identify four major issues in model calibration
and validation, and suggest potential solutions. Finally, we discuss open
problems that are critical for the future development of empirically grounded
agent-based models of innovation diffusion that enable reliable decision
support for stakeholders.
</summary>
    <author>
      <name>Haifeng Zhang</name>
    </author>
    <author>
      <name>Yevgeniy Vorobeychik</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08515v1</id>
    <updated>2016-08-30T15:43:52Z</updated>
    <published>2016-08-30T15:43:52Z</published>
    <title>Language Detection For Short Text Messages In Social Media</title>
    <summary>  With the constant growth of the World Wide Web and the number of documents in
different languages accordingly, the need for reliable language detection tools
has increased as well. Platforms such as Twitter with predominantly short texts
are becoming important information resources, which additionally imposes the
need for short texts language detection algorithms. In this paper, we show how
incorporating personalized user-specific information into the language
detection algorithm leads to an important improvement of detection results. To
choose the best algorithm for language detection for short text messages, we
investigate several machine learning approaches. These approaches include the
use of the well-known classifiers such as SVM and logistic regression, a
dictionary based approach, and a probabilistic model based on modified
Kneser-Ney smoothing. Furthermore, the extension of the probabilistic model to
include additional user-specific information such as evidence accumulation per
user and user interface language is explored, with the goal of improving the
classification performance. The proposed approaches are evaluated on randomly
collected Twitter data containing Latin as well as non-Latin alphabet languages
and the quality of the obtained results is compared, followed by the selection
of the best performing algorithm. This algorithm is then evaluated against two
already existing general language detection tools: Chromium Compact Language
Detector 2 (CLD2) and langid, where our method significantly outperforms the
results achieved by both of the mentioned methods. Additionally, a preview of
benefits and possible applications of having a reliable language detection
algorithm is given.
</summary>
    <author>
      <name>Ivana Balazevic</name>
    </author>
    <author>
      <name>Mikio Braun</name>
    </author>
    <author>
      <name>Klaus-Robert Müller</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08497v1</id>
    <updated>2016-08-30T15:21:42Z</updated>
    <published>2016-08-30T15:21:42Z</published>
    <title>Modelling Cyber-Security Experts' Decision Making Processes using
  Aggregation Operators</title>
    <summary>  An important role carried out by cyber-security experts is the assessment of
proposed computer systems, during their design stage. This task is fraught with
difficulties and uncertainty, making the knowledge provided by human experts
essential for successful assessment. Today, the increasing number of
progressively complex systems has led to an urgent need to produce tools that
support the expert-led process of system-security assessment. In this research,
we use weighted averages (WAs) and ordered weighted averages (OWAs) with
evolutionary algorithms (EAs) to create aggregation operators that model parts
of the assessment process. We show how individual overall ratings for security
components can be produced from ratings of their characteristics, and how these
individual overall ratings can be aggregated to produce overall rankings of
potential attacks on a system. As well as the identification of salient attacks
and weak points in a prospective system, the proposed method also highlights
which factors and security components contribute most to a component's
difficulty and attack ranking respectively. A real world scenario is used in
which experts were asked to rank a set of technical attacks, and to answer a
series of questions about the security components that are the subject of the
attacks. The work shows how finding good aggregation operators, and identifying
important components and factors of a cyber-security problem can be automated.
The resulting operators have the potential for use as decision aids for systems
designers and cyber-security experts, increasing the amount of assessment that
can be achieved with the limited resources available.
</summary>
    <author>
      <name>Simon Miller</name>
    </author>
    <author>
      <name>Christian Wagner</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Jonathan M. Garibaldi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computers and Security, Volume 62, September 2016, Pages 229-245</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08472v1</id>
    <updated>2016-08-30T14:32:41Z</updated>
    <published>2016-08-30T14:32:41Z</published>
    <title>ALLSAT compressed with wildcards. Part 1: Converting CNF's to orthogonal
  DNF's</title>
    <summary>  For most branching algorithms in Boolean logic "branching" means
"variable-wise branching". We present the apparently novel technique of
clause-wise branching, which is used to solve the ALLSAT problem for arbitrary
Boolean functions in CNF format. Specifically, it converts a CNF into an
orthogonal DNF, i.e. into an exclusive sum of products. Our method is enhanced
by two ingredients: The use of a good SAT-solver and wildcards beyond the
common don't-care symbol.
</summary>
    <author>
      <name>Marcel Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 4 figures, 10 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08447v1</id>
    <updated>2016-08-30T13:47:41Z</updated>
    <published>2016-08-30T13:47:41Z</published>
    <title>BreakID: Static Symmetry Breaking for ASP (System Description)</title>
    <summary>  Symmetry breaking has been proven to be an efficient preprocessing technique
for satisfiability solving (SAT). In this paper, we port the state-of-the-art
SAT symmetry breaker BreakID to answer set programming (ASP). The result is a
lightweight tool that can be plugged in between the grounding and the solving
phases that are common when modelling in ASP. We compare our tool with sbass,
the current state-of-the-art symmetry breaker for ASP.
</summary>
    <author>
      <name>Jo Devriendt</name>
    </author>
    <author>
      <name>Bart Bogaerts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 9th Workshop on Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08435v1</id>
    <updated>2016-08-30T13:08:06Z</updated>
    <published>2016-08-30T13:08:06Z</published>
    <title>Multi-Label Classification Method Based on Extreme Learning Machines</title>
    <summary>  In this paper, an Extreme Learning Machine (ELM) based technique for
Multi-label classification problems is proposed and discussed. In multi-label
classification, each of the input data samples belongs to one or more than one
class labels. The traditional binary and multi-class classification problems
are the subset of the multi-label problem with the number of labels
corresponding to each sample limited to one. The proposed ELM based multi-label
classification technique is evaluated with six different benchmark multi-label
datasets from different domains such as multimedia, text and biology. A
detailed comparison of the results is made by comparing the proposed method
with the results from nine state of the arts techniques for five different
evaluation metrics. The nine methods are chosen from different categories of
multi-label methods. The comparative results shows that the proposed Extreme
Learning Machine based multi-label classification technique is a better
alternative than the existing state of the art methods for multi-label
problems.
</summary>
    <author>
      <name>Rajasekar Venkatesan</name>
    </author>
    <author>
      <name>Meng Joo Er</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICARCV.2014.7064375</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICARCV.2014.7064375" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 7 figures, 7 tables, ICARCV</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08292v1</id>
    <updated>2016-08-30T00:59:07Z</updated>
    <published>2016-08-30T00:59:07Z</published>
    <title>Robust Energy Storage Scheduling for Imbalance Reduction of
  Strategically Formed Energy Balancing Groups</title>
    <summary>  Imbalance (on-line energy gap between contracted supply and actual demand,
and associated cost) reduction is going to be a crucial service for a Power
Producer and Supplier (PPS) in the deregulated energy market. PPS requires
forward market interactions to procure energy as precisely as possible in order
to reduce imbalance energy. This paper presents, 1) (off-line) an effective
demand aggregation based strategy for creating a number of balancing groups
that leads to higher predictability of group-wise aggregated demand, 2)
(on-line) a robust energy storage scheduling that minimizes the imbalance for a
particular balancing group considering the demand prediction uncertainty. The
group formation is performed by a Probabilistic Programming approach using
Bayesian Markov Chain Monte Carlo (MCMC) method after applied on the historical
demand statistics. Apart from the group formation, the aggregation strategy
(with the help of Bayesian Inference) also clears out the upper-limit of the
required storage capacity for a formed group, fraction of which is to be
utilized in on-line operation. For on-line operation, a robust energy storage
scheduling method is proposed that minimizes expected imbalance energy and cost
(a non-linear function of imbalance energy) while incorporating the demand
uncertainty of a particular group. The proposed methods are applied on the real
apartment buildings' demand data in Tokyo, Japan. Simulation results are
presented to verify the effectiveness of the proposed methods.
</summary>
    <author>
      <name>Shantanu Chakraborty</name>
    </author>
    <author>
      <name>Toshiya Okabe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.energy.2016.07.170.</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.energy.2016.07.170." rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Energy, Volume 114, 1 November 2016, Pages 405-417, ISSN 0360-5442</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.08292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08262v1</id>
    <updated>2016-08-29T21:58:07Z</updated>
    <published>2016-08-29T21:58:07Z</published>
    <title>Vicious Circle Principle and Formation of Sets in ASP Based Languages</title>
    <summary>  The paper continues the investigation of Poincare and Russel's Vicious Circle
Principle (VCP) in the context of the design of logic programming languages
with sets. We expand previously introduced language Alog with aggregates by
allowing infinite sets and several additional set related constructs useful for
knowledge representation and teaching. In addition, we propose an alternative
formalization of the original VCP and incorporate it into the semantics of new
language, Slog+, which allows more liberal construction of sets and their use
in programming rules. We show that, for programs without disjunction and
infinite sets, the formal semantics of aggregates in Slog+ coincides with that
of several other known languages. Their intuitive and formal semantics,
however, are based on quite different ideas and seem to be more involved than
that of Slog+.
</summary>
    <author>
      <name>Michael Gelfond</name>
    </author>
    <author>
      <name>Yuanlin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 9th Workshop on Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08252v1</id>
    <updated>2016-08-29T21:14:01Z</updated>
    <published>2016-08-29T21:14:01Z</published>
    <title>Business Process Deviance Mining: Review and Evaluation</title>
    <summary>  Business process deviance refers to the phenomenon whereby a subset of the
executions of a business process deviate, in a negative or positive way, with
respect to its expected or desirable outcomes. Deviant executions of a business
process include those that violate compliance rules, or executions that
undershoot or exceed performance targets. Deviance mining is concerned with
uncovering the reasons for deviant executions by analyzing business process
event logs. This article provides a systematic review and comparative
evaluation of deviance mining approaches based on a family of data mining
techniques known as sequence classification. Using real-life logs from multiple
domains, we evaluate a range of feature types and classification methods in
terms of their ability to accurately discriminate between normal and deviant
executions of a process. We also analyze the interestingness of the rule sets
extracted using different methods. We observe that feature sets extracted using
pattern mining techniques only slightly outperform simpler feature sets based
on counts of individual activity occurrences in a trace.
</summary>
    <author>
      <name>Hoang Nguyen</name>
    </author>
    <author>
      <name>Marlon Dumas</name>
    </author>
    <author>
      <name>Marcello La Rosa</name>
    </author>
    <author>
      <name>Fabrizio Maria Maggi</name>
    </author>
    <author>
      <name>Suriadi Suriadi</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08188v1</id>
    <updated>2016-08-29T19:24:25Z</updated>
    <published>2016-08-29T19:24:25Z</published>
    <title>Visual Question: Predicting If a Crowd Will Agree on the Answer</title>
    <summary>  Visual question answering (VQA) systems are emerging from a desire to empower
users to ask any natural language question about visual content and receive a
valid answer in response. However, close examination of the VQA problem reveals
an unavoidable, entangled problem that multiple humans may or may not always
agree on a single answer to a visual question. We train a model to
automatically predict from a visual question whether a crowd would agree on a
single answer. We then propose how to exploit this system in a novel
application to efficiently allocate human effort to collect answers to visual
questions. Specifically, we propose a crowdsourcing system that automatically
solicits fewer human responses when answer agreement is expected and more human
responses when answer disagreement is expected. Our system improves upon
existing crowdsourcing systems, typically eliminating at least 20% of human
effort with no loss to the information collected from the crowd.
</summary>
    <author>
      <name>Danna Gurari</name>
    </author>
    <author>
      <name>Kristen Grauman</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08176v1</id>
    <updated>2016-08-29T18:45:00Z</updated>
    <published>2016-08-29T18:45:00Z</published>
    <title>What is Wrong with Topic Modeling? (and How to Fix it Using Search-based
  SE)</title>
    <summary>  Topic Modeling finds human-readable structures in large sets of unstructured
SE data. A widely used topic modeler is Latent Dirichlet Allocation. When run
on SE data, LDA suffers from "order effects" i.e. different topics be generated
if the training data was shuffled into a different order. Such order effects
introduce a systematic error for any study that uses topics to make
conclusions. This paper introduces LDADE, a Search-Based SE tool that tunes
LDA's parameters using DE (Differential Evolution). LDADE has been tested on
data from a programmer information exchange site (Stackoverflow), title and
abstract text of thousands of SE papers, and software defect reports from NASA.
Results were collected across different implementations of LDA
(Python+Scikit-Learn, Scala+Spark); across different platforms (Linux,
Macintosh) and for different kinds of LDAs (the traditional VEM method, or
using Gibbs sampling). In all tests, the pattern was the same: LDADE's tunings
dramatically reduces topic instability. The implications of this study for
other software analytics tasks is now an open and pressing issue. In how many
domains can search-based SE dramatically improve software analytics?
</summary>
    <author>
      <name>Amritanshu Agrawal</name>
    </author>
    <author>
      <name>Wei Fu</name>
    </author>
    <author>
      <name>Tim Menzies</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages + 2 page references. Submitted to ICSE 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08144v1</id>
    <updated>2016-08-29T16:59:43Z</updated>
    <published>2016-08-29T16:59:43Z</published>
    <title>Achievements in Answer Set Programming (Preliminary Report)</title>
    <summary>  This paper describes an approach to the methodology of answer set programming
(ASP) that can facilitate the design of encodings that are easy to understand
and provably correct. Under this approach, after appending a rule or a small
group of rules to the emerging program we include a comment that states what
has been "achieved" so far. This strategy allows us to set out our
understanding of the design of the program by describing the roles of small
parts of the program in a mathematically precise way.
</summary>
    <author>
      <name>Vladimir Lifschitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 9th Workshop on Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08033v1</id>
    <updated>2016-08-29T12:55:15Z</updated>
    <published>2016-08-29T12:55:15Z</published>
    <title>Fuzzy Logic in Narrow Sense with Hedges</title>
    <summary>  Classical logic has a serious limitation in that it cannot cope with the
issues of vagueness and uncertainty into which fall most modes of human
reasoning. In order to provide a foundation for human knowledge representation
and reasoning in the presence of vagueness, imprecision, and uncertainty, fuzzy
logic should have the ability to deal with linguistic hedges, which play a very
important role in the modification of fuzzy predicates. In this paper, we
extend fuzzy logic in narrow sense with graded syntax, introduced by Novak et
al., with many hedge connectives. In one case, each hedge does not have any
dual one. In the other case, each hedge can have its own dual one. The
resulting logics are shown to also have the Pavelka-style completeness
</summary>
    <author>
      <name>Van Hung Le</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2016.8310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2016.8310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, International Journal of Computer Science &amp; Information
  Technology (IJCSIT) Vol 8, No 3, June 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08028v1</id>
    <updated>2016-08-29T12:43:42Z</updated>
    <published>2016-08-29T12:43:42Z</published>
    <title>From Deterministic ODEs to Dynamic Structural Causal Models</title>
    <summary>  We show how, under certain conditions, the asymptotic behaviour of an
Ordinary Differential Equation under non-constant interventions can be modelled
using Dynamic Structural Causal Models. In contrast to earlier work, we study
not only the effect of interventions on equilibrium states; rather, we model
asymptotic behaviour that is dynamic under interventions that vary in time, and
include as a special case the study of static equilibria.
</summary>
    <author>
      <name>Paul K. Rubenstein</name>
    </author>
    <author>
      <name>Stephan Bongers</name>
    </author>
    <author>
      <name>Joris M. Mooij</name>
    </author>
    <author>
      <name>Bernhard Schoelkopf</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08015v1</id>
    <updated>2016-08-29T12:07:04Z</updated>
    <published>2016-08-29T12:07:04Z</published>
    <title>Event Selection Rules to Compute Explanations</title>
    <summary>  Explanations have been introduced in the previous century. Their interest in
reducing the search space is no longer questioned. Yet, their efficient
implementation into CSP solver is still a challenge. In this paper, we
introduce ESeR, an Event Selection Rules algorithm that filters events
generated during propagation. This dynamic selection enables an efficient
computation of explanations for intelligent backtracking al- gorithms. We show
the effectiveness of our approach on the instances of the last three MiniZinc
challenges
</summary>
    <author>
      <name>Charles Prud'homme</name>
    </author>
    <author>
      <name>Xavier Lorca</name>
    </author>
    <author>
      <name>Narendra Jussien</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07905v1</id>
    <updated>2016-08-29T03:42:50Z</updated>
    <published>2016-08-29T03:42:50Z</published>
    <title>Machine Comprehension Using Match-LSTM and Answer Pointer</title>
    <summary>  Machine comprehension of text is an important problem in natural language
processing. A recently released dataset, the Stanford Question Answering
Dataset (SQuAD), offers a large number of real questions and their answers
created by humans through crowdsourcing. SQuAD provides a challenging testbed
for evaluating machine comprehension algorithms, partly because compared with
previous datasets, in SQuAD the answers do not come from a small set of
candidate answers and they have variable lengths. We propose an end-to-end
neural architecture for the task. The architecture is based on match-LSTM, a
model we proposed previously for textual entailment, and Pointer Net, a
sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the
output tokens to be from the input sequences. We propose two ways of using
Pointer Net for our task. Our experiments show that both of our two models
substantially outperform the best results obtained by Rajpurkar et al.(2016)
using logistic regression and manually crafted features.
</summary>
    <author>
      <name>Shuohang Wang</name>
    </author>
    <author>
      <name>Jing Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07879v1</id>
    <updated>2016-08-29T01:35:46Z</updated>
    <published>2016-08-29T01:35:46Z</published>
    <title>Causality and Responsibility for Formal Verification and Beyond</title>
    <summary>  The theory of actual causality, defined by Halpern and Pearl, and its
quantitative measure - the degree of responsibility - was shown to be extremely
useful in various areas of computer science due to a good match between the
results it produces and our intuition. In this paper, I describe the
applications of causality to formal verification, namely, explanation of
counterexamples, refinement of coverage metrics, and symbolic trajectory
evaluation. I also briefly discuss recent applications of causality to legal
reasoning.
</summary>
    <author>
      <name>Hana Chockler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">King's College London</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.224.1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.224.1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings CREST 2016, arXiv:1608.07398. Invited paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 224, 2016, pp. 1-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.07879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07846v1</id>
    <updated>2016-08-28T19:51:31Z</updated>
    <published>2016-08-28T19:51:31Z</published>
    <title>Data Analytics using Ontologies of Management Theories: Towards
  Implementing 'From Theory to Practice'</title>
    <summary>  We explore how computational ontologies can be impactful vis-a-vis the
developing discipline of "data science." We posit an approach wherein
management theories are represented as formal axioms, and then applied to draw
inferences about data that reside in corporate databases. That is, management
theories would be implemented as rules within a data analytics engine. We
demonstrate a case study development of such an ontology by formally
representing an accounting theory in First-Order Logic. Though quite
preliminary, the idea that an information technology, namely ontologies, can
potentially actualize the academic cliche, "From Theory to Practice," and be
applicable to the burgeoning domain of data analytics is novel and exciting.
</summary>
    <author>
      <name>Henry M. Kim</name>
    </author>
    <author>
      <name>Jackie Ho Nam Cheung</name>
    </author>
    <author>
      <name>Marek Laskowski</name>
    </author>
    <author>
      <name>Iryna Gel</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07793v2</id>
    <updated>2016-09-01T15:41:02Z</updated>
    <published>2016-08-28T09:42:52Z</published>
    <title>Partially Observable Markov Decision Process for Recommender Systems</title>
    <summary>  We report the "Recurrent Deterioration" (RD) phenomenon observed in online
recommender systems. The RD phenomenon is reflected by the trend of performance
degradation when the recommendation model is always trained based on users'
feedbacks of the previous recommendations. There are several reasons for the
recommender systems to encounter the RD phenomenon, including the lack of
negative training data and the evolution of users' interests, etc. Motivated to
tackle the problems causing the RD phenomenon, we propose the POMDP-Rec
framework, which is a neural-optimized Partially Observable Markov Decision
Process algorithm for recommender systems. We show that the POMDP-Rec framework
effectively uses the accumulated historical data from real-world recommender
systems and automatically achieves comparable results with those models
fine-tuned exhaustively by domain exports on public datasets.
</summary>
    <author>
      <name>Zhongqi Lu</name>
    </author>
    <author>
      <name>Qiang Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07793v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07793v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07764v1</id>
    <updated>2016-08-28T04:18:39Z</updated>
    <published>2016-08-28T04:18:39Z</published>
    <title>The Movie Graph Argument Revisited</title>
    <summary>  In this paper, we reexamine the Movie Graph Argument, which demonstrates a
basic incompatibility between computationalism and materialism. We discover
that the incompatibility is only manifest in singular classical-like universes.
If we accept that we live in a Multiverse, then the incompatibility goes away,
but in that case another line of argument shows that with computationalism, the
fundamental, or primitive materiality has no causal influence on what is
observed, which must must be derivable from basic arithmetic properties.
</summary>
    <author>
      <name>Russell K. Standish</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07734v1</id>
    <updated>2016-08-27T18:41:47Z</updated>
    <published>2016-08-27T18:41:47Z</published>
    <title>Learning Bayesian Networks without Assuming Missing at Random</title>
    <summary>  We present new algorithms for learning Bayesian networks from data with
missing values without the assumption that data are missing at random (MAR). An
exact Bayesian network learning algorithm is obtained by recasting the problem
into a standard Bayesian network learning problem without missing data. To the
best of our knowledge, this is the first exact algorithm for this problem. As
expected, the exact algorithm does not scale to large domains. We build on the
exact method to create a new approximate algorithm using a hill-climbing
technique. This algorithm scales to large domains so long as a suitable
standard structure learning method for complete data is available. We perform a
wide range of experiments to demonstrate the benefits of learning Bayesian
networks without assuming MAR.
</summary>
    <author>
      <name>Tameem Adel</name>
    </author>
    <author>
      <name>Cassio P. de Campos</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07685v1</id>
    <updated>2016-08-27T09:53:38Z</updated>
    <published>2016-08-27T09:53:38Z</published>
    <title>Knowledge Semantic Representation: A Generative Model for Interpretable
  Knowledge Graph Embedding</title>
    <summary>  Knowledge representation is a critical topic in AI, and currently embedding
as a key branch of knowledge representation takes the numerical form of
entities and relations to joint the statistical models. However, most embedding
methods merely concentrate on the triple fitting and ignore the explicit
semantic expression, leading to an uninterpretable representation form. Thus,
traditional embedding methods do not only degrade the performance, but also
restrict many potential applications. For this end, this paper proposes a
semantic representation method for knowledge graph \textbf{(KSR)}, which
imposes a two-level hierarchical generative process that globally extracts many
aspects and then locally assigns a specific category in each aspect for every
triple. Because both the aspects and categories are semantics-relevant, the
collection of categories in each aspect is treated as the semantic
representation of this triple. Extensive experiments justify our model
outperforms other state-of-the-art baselines in a substantial extent.
</summary>
    <author>
      <name>Han Xiao</name>
    </author>
    <author>
      <name>Minlie Huang</name>
    </author>
    <author>
      <name>Xiaoyan Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07639v1</id>
    <updated>2016-08-27T00:34:00Z</updated>
    <published>2016-08-27T00:34:00Z</published>
    <title>Learning to generalize to new compositions in image understanding</title>
    <summary>  Recurrent neural networks have recently been used for learning to describe
images using natural language. However, it has been observed that these models
generalize poorly to scenes that were not observed during training, possibly
depending too strongly on the statistics of the text in the training data. Here
we propose to describe images using short structured representations, aiming to
capture the crux of a description. These structured representations allow us to
tease-out and evaluate separately two types of generalization: standard
generalization to new images with similar scenes, and generalization to new
combinations of known entities. We compare two learning approaches on the
MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,
Attend and Tell), and a simple structured prediction model on top of a deep
network. We find that the structured model generalizes to new compositions
substantially better than the LSTM, ~7 times the accuracy of predicting
structured representations. By providing a concrete method to quantify
generalization for unseen combinations, we argue that structured
representations and compositional splits are a useful benchmark for image
captioning, and advocate compositional models that capture linguistic and
visual structure.
</summary>
    <author>
      <name>Yuval Atzmon</name>
    </author>
    <author>
      <name>Jonathan Berant</name>
    </author>
    <author>
      <name>Vahid Kezami</name>
    </author>
    <author>
      <name>Amir Globerson</name>
    </author>
    <author>
      <name>Gal Chechik</name>
    </author>
    <link href="http://arxiv.org/abs/1608.07639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07441v1</id>
    <updated>2016-08-26T12:42:43Z</updated>
    <published>2016-08-26T12:42:43Z</published>
    <title>Hard Negative Mining for Metric Learning Based Zero-Shot Classification</title>
    <summary>  Zero-Shot learning has been shown to be an efficient strategy for domain
adaptation. In this context, this paper builds on the recent work of Bucher et
al. [1], which proposed an approach to solve Zero-Shot classification problems
(ZSC) by introducing a novel metric learning based objective function. This
objective function allows to learn an optimal embedding of the attributes
jointly with a measure of similarity between images and attributes. This paper
extends their approach by proposing several schemes to control the generation
of the negative pairs, resulting in a significant improvement of the
performance and giving above state-of-the-art results on three challenging ZSC
datasets.
</summary>
    <author>
      <name>Maxime Bucher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Palaiseau</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Herbin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Palaiseau</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Jurie</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 16 WS TASK-CV: Transferring and Adapting Source Knowledge in
  Computer Vision, Oct 2016, Amsterdam, Netherlands. ECCV 16 WS TASK-CV:
  Transferring and Adapting Source Knowledge in Computer Vision</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.07441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07440v1</id>
    <updated>2016-08-26T12:41:43Z</updated>
    <published>2016-08-26T12:41:43Z</published>
    <title>Activity Networks with Delays An application to toxicity analysis</title>
    <summary>  ANDy , Activity Networks with Delays, is a discrete time framework aimed at
the qualitative modelling of time-dependent activities. The modular and concise
syntax makes ANDy suitable for an easy and natural modelling of time-dependent
biological systems (i.e., regulatory pathways). Activities involve entities
playing the role of activators, inhibitors or products of biochemical network
operation. Activities may have given duration, i.e., the time required to
obtain results. An entity may represent an object (e.g., an agent, a
biochemical species or a family of thereof) with a local attribute, a state
denoting its level (e.g., concentration, strength). Entities levels may change
as a result of an activity or may decay gradually as time passes by. The
semantics of ANDy is formally given via high-level Petri nets ensuring this way
some modularity. As main results we show that ANDy systems have finite state
representations even for potentially infinite processes and it well adapts to
the modelling of toxic behaviours. As an illustration, we present a
classification of toxicity properties and give some hints on how they can be
verified with existing tools on ANDy systems. A small case study on blood
glucose regulation is provided to exemplify the ANDy framework and the toxicity
properties.
</summary>
    <author>
      <name>Franck Delaplace</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBISC</arxiv:affiliation>
    </author>
    <author>
      <name>Cinzia Di Giusto</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Repmus</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Louis Giavitto</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Repmus</arxiv:affiliation>
    </author>
    <author>
      <name>Hanna Klaudel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IBISC</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1608.07440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07398v1</id>
    <updated>2016-08-26T09:13:22Z</updated>
    <published>2016-08-26T09:13:22Z</published>
    <title>Proceedings First Workshop on Causal Reasoning for Embedded and
  safety-critical Systems Technologies</title>
    <summary>  Formal approaches for automated causality analysis, fault localization,
explanation of events, accountability and blaming have been proposed
independently by several communities --- in particular, AI, concurrency,
model-based diagnosis, formal methods. Work on these topics has significantly
gained speed during the last years. The goals of CREST are to bring together
and foster exchange between researchers from the different communities, and to
present and discuss recent advances and new ideas in the field.
  The workshop program consisted of a set of invited and contributed
presentations that illustrate different techniques for, and applications of,
causality analysis and fault localization.
  The program was anchored by two keynote talks. The keynote by Hana Chockler
(King's College) provided a broad perspective on the application of causal
reasoning based on Halpern and Pearl's definitions of actual causality to a
variety of application domains ranging from formal verification to legal
reasoning. The keynote by Chao Wang (Virginia Tech) concentrated on
constraint-based analysis techniques for debugging and verifying concurrent
programs.
  Workshop papers deal with compositional causality analysis and a wide
spectrum of application for causal reasoning, such as debugging of
probabilistic models, accountability and responsibility, hazard analysis in
practice based on Lewis' counterfactuals, and fault localization and repair.
</summary>
    <author>
      <name>Gregor Gössler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA, France</arxiv:affiliation>
    </author>
    <author>
      <name>Oleg Sokolsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.224</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.224" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 224, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.07398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08072v1</id>
    <updated>2016-08-26T05:53:07Z</updated>
    <published>2016-08-26T05:53:07Z</published>
    <title>A Novel Approach to Multimedia Ontology Engineering for Automated
  Reasoning over Audiovisual LOD Datasets</title>
    <summary>  Multimedia reasoning, which is suitable for, among others, multimedia content
analysis and high-level video scene interpretation, relies on the formal and
comprehensive conceptualization of the represented knowledge domain. However,
most multimedia ontologies are not exhaustive in terms of role definitions, and
do not incorporate complex role inclusions and role interdependencies. In fact,
most multimedia ontologies do not have a role box at all, and implement only a
basic subset of the available logical constructors. Consequently, their
application in multimedia reasoning is limited. To address the above issues,
VidOnt, the very first multimedia ontology with SROIQ(D) expressivity and a
DL-safe ruleset has been introduced for next-generation multimedia reasoning.
In contrast to the common practice, the formal grounding has been set in one of
the most expressive description logics, and the ontology validated with
industry-leading reasoners, namely HermiT and FaCT++. This paper also presents
best practices for developing multimedia ontologies, based on my ontology
engineering approach.
</summary>
    <author>
      <name>Leslie F. Sikos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-662-49381-6_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-662-49381-6_1" rel="related"/>
    <link href="http://arxiv.org/abs/1608.08072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07253v1</id>
    <updated>2016-08-25T18:57:50Z</updated>
    <published>2016-08-25T18:57:50Z</published>
    <title>Learning Latent Vector Spaces for Product Search</title>
    <summary>  We introduce a novel latent vector space model that jointly learns the latent
representations of words, e-commerce products and a mapping between the two
without the need for explicit annotations. The power of the model lies in its
ability to directly model the discriminative relation between products and a
particular word. We compare our method to existing latent vector space models
(LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank
setting. Our latent vector space model achieves its enhanced performance as it
learns better product representations. Furthermore, the mapping from words to
products and the representations of words benefit directly from the errors
propagated back from the product representations during parameter estimation.
We provide an in-depth analysis of the performance of our model and analyze the
structure of the learned representations.
</summary>
    <author>
      <name>Christophe Van Gysel</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <author>
      <name>Evangelos Kanoulas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2983323.2983702</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2983323.2983702" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CIKM2016, Proceedings of the 25th ACM International Conference on
  Information and Knowledge Management. 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07187v2</id>
    <updated>2016-08-30T18:23:06Z</updated>
    <published>2016-08-25T15:07:17Z</published>
    <title>Semantics derived automatically from language corpora necessarily
  contain human biases</title>
    <summary>  Artificial intelligence and machine learning are in a period of astounding
growth. However, there are concerns that these technologies may be used, either
with or without intention, to perpetuate the prejudice and unfairness that
unfortunately characterizes many human institutions. Here we show for the first
time that human-like semantic biases result from the application of standard
machine learning to ordinary language---the same sort of language humans are
exposed to every day. We replicate a spectrum of standard human biases as
exposed by the Implicit Association Test and other well-known psychological
studies. We replicate these using a widely used, purely statistical
machine-learning model---namely, the GloVe word embedding---trained on a corpus
of text from the Web. Our results indicate that language itself contains
recoverable and accurate imprints of our historic biases, whether these are
morally neutral as towards insects or flowers, problematic as towards race or
gender, or even simply veridical, reflecting the status quo for the
distribution of gender with respect to careers or first names. These
regularities are captured by machine learning along with the rest of semantics.
In addition to our empirical findings concerning language, we also contribute
new methods for evaluating bias in text, the Word Embedding Association Test
(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results
have implications not only for AI and machine learning, but also for the fields
of psychology, sociology, and human ethics, since they raise the possibility
that mere exposure to everyday language can account for the biases we replicate
here.
</summary>
    <author>
      <name>Aylin Caliskan-Islam</name>
    </author>
    <author>
      <name>Joanna J. Bryson</name>
    </author>
    <author>
      <name>Arvind Narayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07187v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07187v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07117v1</id>
    <updated>2016-08-25T12:45:20Z</updated>
    <published>2016-08-25T12:45:20Z</published>
    <title>Modelling Chemical Reasoning to Predict Reactions</title>
    <summary>  The ability to reason beyond established knowledge allows Organic Chemists to
solve synthetic problems and to invent novel transformations. Here, we propose
a model which mimics chemical reasoning and formalises reaction prediction as
finding missing links in a knowledge graph. We have constructed a knowledge
graph containing 14.4 million molecules and 8.2 million binary reactions, which
represents the bulk of all chemical reactions ever published in the scientific
literature. Our model outperforms a rule-based expert system in the reaction
prediction task for 180,000 randomly selected binary reactions. We show that
our data-driven model generalises even beyond known reaction types, and is thus
capable of effectively (re-) discovering novel transformations (even including
transition-metal catalysed reactions). Our model enables computers to infer
hypotheses about reactivity and reactions by only considering the intrinsic
local structure of the graph, and because each single reaction prediction is
typically achieved in a sub-second time frame, our model can be used as a
high-throughput generator of reaction hypotheses for reaction discovery.
</summary>
    <author>
      <name>Marwin H. S. Segler</name>
    </author>
    <author>
      <name>Mark P. Waller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07068v2</id>
    <updated>2016-09-08T17:36:13Z</updated>
    <published>2016-08-25T09:49:23Z</published>
    <title>Title Generation for User Generated Videos</title>
    <summary>  A great video title describes the most salient event compactly and captures
the viewer's attention. In contrast, video captioning tends to generate
sentences that describe the video as a whole. Although generating a video title
automatically is a very useful task, it is much less addressed than video
captioning. We address video title generation for the first time by proposing
two methods that extend state-of-the-art video captioners to this new task.
First, we make video captioners highlight sensitive by priming them with a
highlight detector. Our framework allows for jointly training a model for title
generation and video highlight localization. Second, we induce high sentence
diversity in video captioners, so that the generated titles are also diverse
and catchy. This means that a large number of sentences might be required to
learn the sentence structure of titles. Hence, we propose a novel sentence
augmentation method to train a captioner with additional sentence-only examples
that come without corresponding videos. We collected a large-scale Video Titles
in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos
and titles. On VTW, our methods consistently improve title prediction accuracy,
and achieve the best performance in both automatic and human evaluation.
Finally, our sentence augmentation method also outperforms the baselines on the
M-VAD dataset.
</summary>
    <author>
      <name>Kuo-Hao Zeng</name>
    </author>
    <author>
      <name>Tseng-Hung Chen</name>
    </author>
    <author>
      <name>Juan Carlos Niebles</name>
    </author>
    <author>
      <name>Min Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures, ECCV2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07068v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07068v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07005v1</id>
    <updated>2016-08-25T02:15:37Z</updated>
    <published>2016-08-25T02:15:37Z</published>
    <title>Multi-View Fuzzy Clustering with Minimax Optimization for Effective
  Clustering of Data from Multiple Sources</title>
    <summary>  Multi-view data clustering refers to categorizing a data set by making good
use of related information from multiple representations of the data. It
becomes important nowadays because more and more data can be collected in a
variety of ways, in different settings and from different sources, so each data
set can be represented by different sets of features to form different views of
it. Many approaches have been proposed to improve clustering performance by
exploring and integrating heterogeneous information underlying different views.
In this paper, we propose a new multi-view fuzzy clustering approach called
MinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In
MinimaxFCM the consensus clustering results are generated based on minimax
optimization in which the maximum disagreements of different weighted views are
minimized. Moreover, the weight of each view can be learned automatically in
the clustering process. In addition, there is only one parameter to be set
besides the fuzzifier. The detailed problem formulation, updating rules
derivation, and the in-depth analysis of the proposed MinimaxFCM are provided
here. Experimental studies on nine multi-view data sets including real world
image and document data sets have been conducted. We observed that MinimaxFCM
outperforms related multi-view clustering approaches in terms of clustering
accuracy, demonstrating the great potential of MinimaxFCM for multi-view data
analysis.
</summary>
    <author>
      <name>Yangtao Wang</name>
    </author>
    <author>
      <name>Lihui Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages, submitted to Expert Systems with Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07001v1</id>
    <updated>2016-08-25T01:56:20Z</updated>
    <published>2016-08-25T01:56:20Z</published>
    <title>Incremental Minimax Optimization based Fuzzy Clustering for Large
  Multi-view Data</title>
    <summary>  Incremental clustering approaches have been proposed for handling large data
when given data set is too large to be stored. The key idea of these approaches
is to find representatives to represent each cluster in each data chunk and
final data analysis is carried out based on those identified representatives
from all the chunks. However, most of the incremental approaches are used for
single view data. As large multi-view data generated from multiple sources
becomes prevalent nowadays, there is a need for incremental clustering
approaches to handle both large and multi-view data. In this paper we propose a
new incremental clustering approach called incremental minimax optimization
based fuzzy clustering (IminimaxFCM) to handle large multi-view data. In
IminimaxFCM, representatives with multiple views are identified to represent
each cluster by integrating multiple complementary views using minimax
optimization. The detailed problem formulation, updating rules derivation, and
the in-depth analysis of the proposed IminimaxFCM are provided. Experimental
studies on several real world multi-view data sets have been conducted. We
observed that IminimaxFCM outperforms related incremental fuzzy clustering in
terms of clustering accuracy, demonstrating the great potential of IminimaxFCM
for large multi-view data analysis.
</summary>
    <author>
      <name>Yangtao Wang</name>
    </author>
    <author>
      <name>Lihui Chen</name>
    </author>
    <author>
      <name>Xiaoli Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 1 figures, submitted to Fuzzy Sets and Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06954v1</id>
    <updated>2016-08-24T20:11:14Z</updated>
    <published>2016-08-24T20:11:14Z</published>
    <title>State Duration and Interval Modeling in Hidden Semi-Markov Model for
  Sequential Data Analysis</title>
    <summary>  Sequential data modeling and analysis have become indispensable tools for
analyzing sequential data such as time-series data because a larger amount of
sensed event data have become available. These methods capture the sequential
structure of data of interest, such as input- output relationship and
correlation among datasets. However, since most studies in this area are
specialized or limited for their respective applications, rigorous requirement
analysis on such a model has not been examined in a general point of view.
Hence, we particularly examine the structure of sequential data, and extract
the necessity of "state duration" and "state duration" of events for efficient
and rich representation of sequential data. Specifically addressing the hidden
semi-Markov model (HSMM) that represents such state duration inside a model, we
attempt to newly add representational capability of state interval of events
onto HSMM. To this end, we propose two extended models; one is interval state
hidden semi-Markov model (IS-HSMM) to express the length of state interval with
a special state node designated as "interval state node". The other is interval
length probability hidden semi-Markov model (ILP-HSMM) which repre- sents the
length of state interval with a new probabilistic parameter "interval length
probability." From exhaustive simulations, we show superior performances of the
proposed models in comparison with HSMM. To the best of our knowledge, our
proposed models are the first extensions of HMM to support state interval
representation as well as state duration representation.
</summary>
    <author>
      <name>Hiromi Narimatsu</name>
    </author>
    <author>
      <name>Hiroyuki Kasai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06910v1</id>
    <updated>2016-08-24T18:18:08Z</updated>
    <published>2016-08-24T18:18:08Z</published>
    <title>A Parallel Memory-efficient Epistemic Logic Program Solver: Harder,
  Better, Faster</title>
    <summary>  As the practical use of answer set programming (ASP) has grown with the
development of efficient solvers, we expect a growing interest in extensions of
ASP as their semantics stabilize and solvers supporting them mature. Epistemic
Specifications, which adds modal operators K and M to the language of ASP, is
one such extension. We call a program in this language an epistemic logic
program (ELP). Solvers have thus far been practical for only the simplest ELPs
due to exponential growth of the memory required. We describe a solver that is
able to solve harder problems better (without exponentially-growing memory
needs w.r.t. K and M occurrences) and faster than any other known ELP solver.
</summary>
    <author>
      <name>Patrick Thor Kahl</name>
    </author>
    <author>
      <name>Anthony P. Leclerc</name>
    </author>
    <author>
      <name>Tran Cao Son</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 9th Workshop on Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06845v1</id>
    <updated>2016-08-24T14:44:33Z</updated>
    <published>2016-08-24T14:44:33Z</published>
    <title>Effect of Incomplete Meta-dataset on Average Ranking Method</title>
    <summary>  One of the simplest metalearning methods is the average ranking method. This
method uses metadata in the form of test results of a given set of algorithms
on given set of datasets and calculates an average rank for each algorithm. The
ranks are used to construct the average ranking. We investigate the problem of
how the process of generating the average ranking is affected by incomplete
metadata including fewer test results. This issue is relevant, because if we
could show that incomplete metadata does not ?affect the final results much, we
could explore it in future design. We could simply conduct fewer tests and save
thus computation time. In this paper we describe an upgraded average ranking
method that is capable of dealing with incomplete metadata. Our results show
that the proposed method is relatively robust to omission in test results in
the meta datasets.
</summary>
    <author>
      <name>Salisu Mamman Abdulrahman</name>
    </author>
    <author>
      <name>Pavel Brazdil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, two figures and 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06787v1</id>
    <updated>2016-08-24T12:01:36Z</updated>
    <published>2016-08-24T12:01:36Z</published>
    <title>Expressibility of norms in temporal logic</title>
    <summary>  In this short note we address the issue of expressing norms (such as
obligations and prohibitions) in temporal logic. In particular, we address the
argument from [Governatori 2015] that norms cannot be expressed in Linear Time
Temporal Logic (LTL).
</summary>
    <author>
      <name>Natasha Alechina</name>
    </author>
    <author>
      <name>Mehdi Dastani</name>
    </author>
    <author>
      <name>Brian Logan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06651v1</id>
    <updated>2016-08-23T20:55:09Z</updated>
    <published>2016-08-23T20:55:09Z</published>
    <title>Unsupervised, Efficient and Semantic Expertise Retrieval</title>
    <summary>  We introduce an unsupervised discriminative model for the task of retrieving
experts in online document collections. We exclusively employ textual evidence
and avoid explicit feature engineering by learning distributed word
representations in an unsupervised way. We compare our model to
state-of-the-art unsupervised statistical vector space and probabilistic
generative approaches. Our proposed log-linear model achieves the retrieval
performance levels of state-of-the-art document-centric methods with the low
inference cost of so-called profile-centric approaches. It yields a
statistically significant improved ranking over vector space and generative
models in most cases, matching the performance of supervised methods on various
benchmarks. That is, by using solely text we can do as well as methods that
work with external evidence and/or relevance feedback. A contrastive analysis
of rankings produced by discriminative and generative approaches shows that
they have complementary strengths due to the ability of the unsupervised
discriminative model to perform semantic matching.
</summary>
    <author>
      <name>Christophe Van Gysel</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <author>
      <name>Marcel Worring</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2872427.2882974</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2872427.2882974" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WWW2016, Proceedings of the 25th International Conference on World
  Wide Web. 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07223v1</id>
    <updated>2016-08-23T15:31:36Z</updated>
    <published>2016-08-23T15:31:36Z</published>
    <title>Is a good offensive always the best defense?</title>
    <summary>  A checkers-like model game with a simplified set of rules is studied through
extensive simulations of agents with different expertise and strategies. The
introduction of complementary strategies, in a quite general way, provides a
tool to mimic the basic ingredients of a wide scope of real games. We find that
only for the player having the higher offensive expertise (the dominant player
), maximizing the offensive always increases the probability to win. For the
non-dominant player, interestingly, a complete minimization of the offensive
becomes the best way to win in many situations, depending on the relative
values of the defense expertise. Further simulations on the interplay of
defense expertise were done separately, in the context of a fully-offensive
scenario, offering a starting point for analytical treatments. In particular,
we established that in this scenario the total number of moves is defined only
by the player with the lower defensive expertise. We believe that these results
stand for a first step towards a new way to improve decisions-making in a large
number of zero-sum real games.
</summary>
    <author>
      <name>J. Quetzalcóatl Toledo-Marín</name>
    </author>
    <author>
      <name>Rogelio Díaz-Méndez</name>
    </author>
    <author>
      <name>Marcelo del Castillo Mussot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07225v1</id>
    <updated>2016-08-23T14:55:43Z</updated>
    <published>2016-08-23T14:55:43Z</published>
    <title>On Simulated Annealing Dedicated to Maximin Latin Hypercube Designs</title>
    <summary>  The goal of our research was to enhance local search heuristics used to
construct Latin Hypercube Designs. First, we introduce the \textit{1D-move}
perturbation to improve the space exploration performed by these algorithms.
Second, we propose a new evaluation function $\psi_{p,\sigma}$ specifically
targeting the Maximin criterion.
  Exhaustive series of experiments with Simulated Annealing, which we used as a
typically well-behaving local search heuristics, confirm that our goal was
reached as the result we obtained surpasses the best scores reported in the
literature. Furthermore, the $\psi_{p,\sigma}$ function seems very promising
for a wide spectrum of optimization problems through the Maximin criterion.
</summary>
    <author>
      <name>Pierre Bergé</name>
    </author>
    <author>
      <name>Kaourintin Le Guiban</name>
    </author>
    <author>
      <name>Arpad Rimmel</name>
    </author>
    <author>
      <name>Joanna Tomasik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of ACM GECCO 2016 paper entitled "Search Space
  Exploration and an Optimization Criterion for Hard Design Problems"</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06403v1</id>
    <updated>2016-08-23T07:14:18Z</updated>
    <published>2016-08-23T07:14:18Z</published>
    <title>Phased Exploration with Greedy Exploitation in Stochastic Combinatorial
  Partial Monitoring Games</title>
    <summary>  Partial monitoring games are repeated games where the learner receives
feedback that might be different from adversary's move or even the reward
gained by the learner. Recently, a general model of combinatorial partial
monitoring (CPM) games was proposed \cite{lincombinatorial2014}, where the
learner's action space can be exponentially large and adversary samples its
moves from a bounded, continuous space, according to a fixed distribution. The
paper gave a confidence bound based algorithm (GCB) that achieves
$O(T^{2/3}\log T)$ distribution independent and $O(\log T)$ distribution
dependent regret bounds. The implementation of their algorithm depends on two
separate offline oracles and the distribution dependent regret additionally
requires existence of a unique optimal action for the learner. Adopting their
CPM model, our first contribution is a Phased Exploration with Greedy
Exploitation (PEGE) algorithmic framework for the problem. Different algorithms
within the framework achieve $O(T^{2/3}\sqrt{\log T})$ distribution independent
and $O(\log^2 T)$ distribution dependent regret respectively. Crucially, our
framework needs only the simpler "argmax" oracle from GCB and the distribution
dependent regret does not require existence of a unique optimal action. Our
second contribution is another algorithm, PEGE2, which combines gap estimation
with a PEGE algorithm, to achieve an $O(\log T)$ regret bound, matching the GCB
guarantee but removing the dependence on size of the learner's action space.
However, like GCB, PEGE2 requires access to both offline oracles and the
existence of a unique optimal action. Finally, we discuss how our algorithm can
be efficiently applied to a CPM problem of practical interest: namely, online
ranking with feedback at the top.
</summary>
    <author>
      <name>Sougata Chaudhuri</name>
    </author>
    <author>
      <name>Ambuj Tewari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appearing in NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06349v1</id>
    <updated>2016-08-23T00:40:27Z</updated>
    <published>2016-08-23T00:40:27Z</published>
    <title>Five dimensions of reasoning in the wild</title>
    <summary>  Reasoning does not work well when done in isolation from its significance,
both to the needs and interests of an agent and with respect to the wider
world. Moreover, those issues may best be handled with a new sort of data
structure that goes beyond the knowledge base and incorporates aspects of
perceptual knowledge and even more, in which a kind of anticipatory action may
be key.
</summary>
    <author>
      <name>Don Perlis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">minor typos corrected from AAAI version, Proceedings (Blue-Sky track)
  AAAI-2016, Phoenix AZ</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06154v1</id>
    <updated>2016-08-22T12:59:31Z</updated>
    <published>2016-08-22T12:59:31Z</published>
    <title>Multi-Sensor Prognostics using an Unsupervised Health Index based on
  LSTM Encoder-Decoder</title>
    <summary>  Many approaches for estimation of Remaining Useful Life (RUL) of a machine,
using its operational sensor data, make assumptions about how a system degrades
or a fault evolves, e.g., exponential degradation. However, in many domains
degradation may not follow a pattern. We propose a Long Short Term Memory based
Encoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI)
for a system using multi-sensor time-series data. LSTM-ED is trained to
reconstruct the time-series corresponding to healthy state of a system. The
reconstruction error is used to compute HI which is then used for RUL
estimation. We evaluate our approach on publicly available Turbofan Engine and
Milling Machine datasets. We also present results on a real-world industry
dataset from a pulverizer mill where we find significant correlation between
LSTM-ED based HI and maintenance costs.
</summary>
    <author>
      <name>Pankaj Malhotra</name>
    </author>
    <author>
      <name>Vishnu TV</name>
    </author>
    <author>
      <name>Anusha Ramakrishnan</name>
    </author>
    <author>
      <name>Gaurangi Anand</name>
    </author>
    <author>
      <name>Lovekesh Vig</name>
    </author>
    <author>
      <name>Puneet Agarwal</name>
    </author>
    <author>
      <name>Gautam Shroff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 1st ACM SIGKDD Workshop on Machine Learning for
  Prognostics and Health Management, San Francisco, CA, USA, 2016. 10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06014v2</id>
    <updated>2016-08-25T22:05:24Z</updated>
    <published>2016-08-21T23:48:43Z</published>
    <title>The Symmetry of a Simple Optimization Problem in Lasso Screening</title>
    <summary>  Recently dictionary screening has been proposed as an effective way to
improve the computational efficiency of solving the lasso problem, which is one
of the most commonly used method for learning sparse representations. To
address today's ever increasing large dataset, effective screening relies on a
tight region bound on the solution to the dual lasso. Typical region bounds are
in the form of an intersection of a sphere and multiple half spaces. One way to
tighten the region bound is using more half spaces, which however, adds to the
overhead of solving the high dimensional optimization problem in lasso
screening. This paper reveals the interesting property that the optimization
problem only depends on the projection of features onto the subspace spanned by
the normals of the half spaces. This property converts an optimization problem
in high dimension to much lower dimension, and thus sheds light on reducing the
computation overhead of lasso screening based on tighter region bounds.
</summary>
    <author>
      <name>Yun Wang</name>
    </author>
    <author>
      <name>Peter J. Ramadge</name>
    </author>
    <link href="http://arxiv.org/abs/1608.06014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06010v2</id>
    <updated>2016-08-25T22:52:30Z</updated>
    <published>2016-08-21T23:40:56Z</published>
    <title>Feedback-Controlled Sequential Lasso Screening</title>
    <summary>  One way to solve lasso problems when the dictionary does not fit into
available memory is to first screen the dictionary to remove unneeded features.
Prior research has shown that sequential screening methods offer the greatest
promise in this endeavor. Most existing work on sequential screening targets
the context of tuning parameter selection, where one screens and solves a
sequence of $N$ lasso problems with a fixed grid of geometrically spaced
regularization parameters. In contrast, we focus on the scenario where a target
regularization parameter has already been chosen via cross-validated model
selection, and we then need to solve many lasso instances using this fixed
value. In this context, we propose and explore a feedback controlled sequential
screening scheme. Feedback is used at each iteration to select the next problem
to be solved. This allows the sequence of problems to be adapted to the
instance presented and the number of intermediate problems to be automatically
selected. We demonstrate our feedback scheme using several datasets including a
dictionary of approximate size 100,000 by 300,000.
</summary>
    <author>
      <name>Yun Wang</name>
    </author>
    <author>
      <name>Xu Chen</name>
    </author>
    <author>
      <name>Peter J. Ramadge</name>
    </author>
    <link href="http://arxiv.org/abs/1608.06010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05921v2</id>
    <updated>2016-09-05T04:52:33Z</updated>
    <published>2016-08-21T11:49:53Z</published>
    <title>Probabilistic Knowledge Graph Construction: Compositional and
  Incremental Approaches</title>
    <summary>  Knowledge graph construction consists of two tasks: extracting information
from external resources (knowledge population) and inferring missing
information through a statistical analysis on the extracted information
(knowledge completion). In many cases, insufficient external resources in the
knowledge population hinder the subsequent statistical inference. The gap
between these two processes can be reduced by an incremental population
approach. We propose a new probabilistic knowledge graph factorisation method
that benefits from the path structure of existing knowledge (e.g. syllogism)
and enables a common modelling approach to be used for both incremental
population and knowledge completion tasks. More specifically, the probabilistic
formulation allows us to develop an incremental population algorithm that
trades off exploitation-exploration. Experiments on three benchmark datasets
show that the balanced exploitation-exploration helps the incremental
population, and the additional path structure helps to predict missing
information in knowledge completion.
</summary>
    <author>
      <name>Dongwoo Kim</name>
    </author>
    <author>
      <name>Lexing Xie</name>
    </author>
    <author>
      <name>Cheng Soon Ong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2983323.2983677</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2983323.2983677" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 25th ACM International Conference on Information and Knowledge
  Management (CIKM 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05921v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05921v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05864v1</id>
    <updated>2016-08-20T19:02:19Z</updated>
    <published>2016-08-20T19:02:19Z</published>
    <title>A Hybrid, PDE-ODE Control Strategy for Intercepting an Intelligent,
  well-informed Target in a Stationary, Cluttered Environment</title>
    <summary>  In [1,2] a new class of intelligent controllers that can semantically embed
an agent in a spatial context constraining its behavior in a goal-oriented
manner was suggested. A controller of such a class can guide an agent in a
stationary unknown environment to a fixed target zone along an obstacle-free
trajectory. Here, an extension is suggested that would enable the interception
of an intelligent target that is maneuvering to evade capture amidst stationary
clutter (i.e. the target zone is moving). This is achieved by forcing the
differential properties of the potential field used to induce the control
action to satisfy the wave equation. Background of the problem, theoretical
developments, as well as, proofs of the ability of the modified control to
intercept the target along an obstacle-free trajectory are supplied. Simulation
results are also provided.
</summary>
    <author>
      <name>Ahmad A. Masoud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 20 figures, Journal paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Mathematical Sciences, HIKARI Ltd, Vol. 1, 2007, No. 48,
  2345-2371</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.05864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05852v1</id>
    <updated>2016-08-20T17:34:38Z</updated>
    <published>2016-08-20T17:34:38Z</published>
    <title>Learning Word Embeddings from Intrinsic and Extrinsic Views</title>
    <summary>  While word embeddings are currently predominant for natural language
processing, most of existing models learn them solely from their contexts.
However, these context-based word embeddings are limited since not all words'
meaning can be learned based on only context. Moreover, it is also difficult to
learn the representation of the rare words due to data sparsity problem. In
this work, we address these issues by learning the representations of words by
integrating their intrinsic (descriptive) and extrinsic (contextual)
information. To prove the effectiveness of our model, we evaluate it on four
tasks, including word similarity, reverse dictionaries,Wiki link prediction,
and document classification. Experiment results show that our model is powerful
in both word and document modeling.
</summary>
    <author>
      <name>Jifan Chen</name>
    </author>
    <author>
      <name>Kan Chen</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Qi Zhang</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05763v1</id>
    <updated>2016-08-20T00:37:20Z</updated>
    <published>2016-08-20T00:37:20Z</published>
    <title>Inference in Probabilistic Logic Programs using Lifted Explanations</title>
    <summary>  In this paper, we consider the problem of lifted inference in the context of
Prism-like probabilistic logic programming languages. Traditional inference in
such languages involves the construction of an explanation graph for the query
and computing probabilities over this graph. When evaluating queries over
probabilistic logic programs with a large number of instances of random
variables, traditional methods treat each instance separately. For many
programs and queries, we observe that explanations can be summarized into
substantially more compact structures, which we call lifted explanation graphs.
In this paper, we define lifted explanation graphs and operations over them. In
contrast to existing lifted inference techniques, our method for constructing
lifted explanations naturally generalizes existing methods for constructing
explanation graphs. To compute probability of query answers, we solve
recurrences generated from the lifted graphs. We show examples where the use of
our technique reduces the asymptotic complexity of inference.
</summary>
    <author>
      <name>Arun Nampally</name>
    </author>
    <author>
      <name>C. R. Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05745v3</id>
    <updated>2016-09-14T19:45:03Z</updated>
    <published>2016-08-19T21:54:46Z</published>
    <title>RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time
  Attention Mechanism</title>
    <summary>  Accuracy and interpretation are two goals of any successful predictive
models. Most existing works have to suffer the tradeoff between the two by
either picking complex black box models such as recurrent neural networks (RNN)
or relying on less accurate traditional models with better interpretation such
as logistic regression. To address this dilemma, we present REverse Time
AttentIoN model (RETAIN) for analyzing Electronic Health Records (EHR) data
that achieves high accuracy while remaining clinically interpretable. RETAIN is
a two-level neural attention model that can find influential past visits and
significant clinical variables within those visits (e.g,. key diagnoses).
RETAIN mimics physician practice by attending the EHR data in a reverse time
order so that more recent clinical visits will likely get higher attention.
Experiments on a large real EHR dataset of 14 million visits from 263K patients
over 8 years confirmed the comparable predictive accuracy and computational
scalability to the state-of-the-art methods such as RNN. Finally, we
demonstrate the clinical interpretation with concrete examples from RETAIN.
</summary>
    <author>
      <name>Edward Choi</name>
    </author>
    <author>
      <name>Mohammad Taha Bahadori</name>
    </author>
    <author>
      <name>Andy Schuetz</name>
    </author>
    <author>
      <name>Walter F. Stewart</name>
    </author>
    <author>
      <name>Jimeng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Neural Information Processing Systems (NIPS) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05745v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05745v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05701v1</id>
    <updated>2016-08-19T19:28:55Z</updated>
    <published>2016-08-19T19:28:55Z</published>
    <title>Pilot Testing an Artificial Intelligence Algorithm That Selects Homeless
  Youth Peer Leaders Who Promote HIV Testing</title>
    <summary>  Objective. To pilot test an artificial intelligence (AI) algorithm that
selects peer change agents (PCA) to disseminate HIV testing messaging in a
population of homeless youth. Methods. We recruited and assessed 62 youth at
baseline, 1 month (n = 48), and 3 months (n = 38). A Facebook app collected
preliminary social network data. Eleven PCAs selected by AI attended a 1-day
training and 7 weekly booster sessions. Mixed-effects models with random
effects were used to assess change over time. Results. Significant change over
time was observed in past 6-month HIV testing (57.9%, 82.4%, 76.3%; p &lt; .05)
but not condom use (63.9%, 65.7%, 65.8%). Most youth reported speaking to a PCA
about HIV prevention (72.0% at 1 month, 61.5% at 3 months). Conclusions. AI is
a promising avenue for implementing PCA models for homeless youth. Increasing
rates of regular HIV testing is critical to HIV prevention and linking homeless
youth to treatment.
</summary>
    <author>
      <name>Eric Rice</name>
    </author>
    <author>
      <name>Robin Petering</name>
    </author>
    <author>
      <name>Jaih Craddock</name>
    </author>
    <author>
      <name>Amanda Yoshioka-Maxwell</name>
    </author>
    <author>
      <name>Amulya Yadav</name>
    </author>
    <author>
      <name>Milind Tambe</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05694v1</id>
    <updated>2016-08-19T18:50:21Z</updated>
    <published>2016-08-19T18:50:21Z</published>
    <title>The languages of actions, formal grammars and qualitive modeling of
  companies</title>
    <summary>  In this paper we discuss methods of using the language of actions, formal
languages, and grammars for qualitative conceptual linguistic modeling of
companies as technological and human institutions. The main problem following
the discussion is the problem to find and describe a language structure for
external and internal flow of information of companies. We anticipate that the
language structure of external and internal base flows determine the structure
of companies. In the structure modeling of an abstract industrial company an
internal base flow of information is constructed as certain flow of words
composed on the theoretical parts-processes-actions language. The language of
procedures is found for an external base flow of information for an insurance
company. The formal stochastic grammar for the language of procedures is found
by statistical methods and is used in understanding the tendencies of the
health care industry. We present the model of human communications as a random
walk on the semantic tree
</summary>
    <author>
      <name>Vladislav B Kovchegov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05675v2</id>
    <updated>2016-08-23T07:59:54Z</updated>
    <published>2016-08-19T17:20:03Z</published>
    <title>lpopt: A Rule Optimization Tool for Answer Set Programming</title>
    <summary>  State-of-the-art answer set programming (ASP) solvers rely on a program
called a grounder to convert non-ground programs containing variables into
variable-free, propositional programs. The size of this grounding depends
heavily on the size of the non-ground rules, and thus, reducing the size of
such rules is a promising approach to improve solving performance. To this end,
in this paper we announce lpopt, a tool that decomposes large logic programming
rules into smaller rules that are easier to handle for current solvers. The
tool is specifically tailored to handle the standard syntax of the ASP language
(ASP-Core) and makes it easier for users to write efficient and intuitive ASP
programs, which would otherwise often require significant hand-tuning by expert
ASP engineers. It is based on an idea proposed by Morak and Woltran (2012) that
we extend significantly in order to handle the full ASP syntax, including
complex constructs like aggregates, weak constraints, and arithmetic
expressions. We present the algorithm, the theoretical foundations on how to
treat these constructs, as well as an experimental evaluation showing the
viability of our approach.
</summary>
    <author>
      <name>Manuel Bichler</name>
    </author>
    <author>
      <name>Michael Morak</name>
    </author>
    <author>
      <name>Stefan Woltran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-proceedings paper presented at the 26th International Symposium
  on Logic-Based Program Synthesis and Transformation (LOPSTR 2016), Edinburgh,
  Scotland UK, 6-8 September 2016 (arXiv:1608.02534), 14 pages, LaTeX, 2
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05609v1</id>
    <updated>2016-08-19T14:19:21Z</updated>
    <published>2016-08-19T14:19:21Z</published>
    <title>Implementing a Relevance Tracker Module</title>
    <summary>  PC(ID) extends propositional logic with inductive definitions: rule sets
under the well-founded semantics. Recently, a notion of relevance was
introduced for this language. This notion determines the set of undecided
literals that can still influence the satisfiability of a PC(ID) formula in a
given partial assignment. The idea is that the PC(ID) solver can make decisions
only on relevant literals without losing soundness and thus safely ignore
irrelevant literals.
  One important insight that the relevance of a literal is completely
determined by the current solver state. During search, the solver state changes
have an effect on the relevance of literals. In this paper, we discuss an
incremental, lightweight implementation of a relevance tracker module that can
be added to and interact with an out-of-the-box SAT(ID) solver.
</summary>
    <author>
      <name>Joachim Jansen</name>
    </author>
    <author>
      <name>Jo Devriendt</name>
    </author>
    <author>
      <name>Bart Bogaerts</name>
    </author>
    <author>
      <name>Gerda Janssens</name>
    </author>
    <author>
      <name>Marc Denecker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 9th Workshop on Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2016), New York City, USA, 16 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05513v1</id>
    <updated>2016-08-19T07:05:33Z</updated>
    <published>2016-08-19T07:05:33Z</published>
    <title>Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network</title>
    <summary>  Recently, a multi-level fuzzy min max neural network (MLF) was proposed,
which improves the classification accuracy by handling an overlapped region
(area of confusion) with the help of a tree structure. In this brief, an
extension of MLF is proposed which defines a new boundary region, where the
previously proposed methods mark decisions with less confidence and hence
misclassification is more frequent. A methodology to classify patterns more
accurately is presented. Our work enhances the testing procedure by means of
data centroids. We exhibit an illustrative example, clearly highlighting the
advantage of our approach. Results on standard datasets are also presented to
evidentially prove a consistent improvement in the classification rate.
</summary>
    <author>
      <name>Shraddha Deshmukh</name>
    </author>
    <author>
      <name>Sagar Gandhi</name>
    </author>
    <author>
      <name>Pratap Sanap</name>
    </author>
    <author>
      <name>Vivek Kulkarni</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05485v1</id>
    <updated>2016-08-19T03:36:18Z</updated>
    <published>2016-08-19T03:36:18Z</published>
    <title>A heuristic scheme for the Cooperative Team Orienteering Problem with
  Time Windows</title>
    <summary>  The Cooperative Orienteering Problem with Time Windows (COPTW)is a class of
problems with some important applications and yet has received relatively
little attention. In the COPTW a certain number of team members are required to
collect the associated reward from each customer simultaneously and
cooperatively. This requirement to have one or more team members simultaneously
available at a vertex to collect the reward, poses a challenging OR task. Exact
methods are not able to handle large scale instances of the COPTW and no
heuristic schemes have been developed for this problem so far. In this paper, a
new modification to the classical Clarke and Wright saving heuristic is
proposed to handle this problem. A new benchmark set generated by adding the
resource requirement attribute to the existing benchmarks. The heuristic
algorithm followed by boosting operators achieves optimal solutions for 64.5%
of instances for which the optimal results are known. The proposed solution
approach attains an optimality gap of 2.61% for the same instances and solves
benchmarks with realistic size within short computational times.
</summary>
    <author>
      <name>Iman Roozbeh</name>
    </author>
    <author>
      <name>Melih Ozlen</name>
    </author>
    <author>
      <name>John W. Hearne</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05347v1</id>
    <updated>2016-08-18T17:47:53Z</updated>
    <published>2016-08-18T17:47:53Z</published>
    <title>Probabilistic Data Analysis with Probabilistic Programming</title>
    <summary>  Probabilistic techniques are central to data analysis, but different
approaches can be difficult to apply, combine, and compare. This paper
introduces composable generative population models (CGPMs), a computational
abstraction that extends directed graphical models and can be used to describe
and compose a broad class of probabilistic data analysis techniques. Examples
include hierarchical Bayesian models, multivariate kernel methods,
discriminative machine learning, clustering algorithms, dimensionality
reduction, and arbitrary probabilistic programs. We also demonstrate the
integration of CGPMs into BayesDB, a probabilistic programming platform that
can express data analysis tasks using a modeling language and a structured
query language. The practical value is illustrated in two ways. First, CGPMs
are used in an analysis that identifies satellite data records which probably
violate Kepler's Third Law, by composing causal probabilistic programs with
non-parametric Bayes in under 50 lines of probabilistic code. Second, for
several representative data analysis tasks, we report on lines of code and
accuracy measurements of various CGPMs, plus comparisons with standard baseline
solutions from Python and MATLAB libraries.
</summary>
    <author>
      <name>Feras Saad</name>
    </author>
    <author>
      <name>Vikash Mansinghka</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05288v1</id>
    <updated>2016-08-18T15:14:37Z</updated>
    <published>2016-08-18T15:14:37Z</published>
    <title>Accelerating Exact and Approximate Inference for (Distributed) Discrete
  Optimization with GPUs</title>
    <summary>  Discrete optimization is a central problem in artificial intelligence. The
optimization of the aggregated cost of a network of cost functions arises in a
variety of problems including (W)CSP, DCOP, as well as optimization in
stochastic variants such as Bayesian networks. Inference-based algorithms are
powerful techniques for solving discrete optimization problems, which can be
used independently or in combination with other techniques. However, their
applicability is often limited by their compute intensive nature and their
space requirements. This paper proposes the design and implementation of a
novel inference-based technique, which exploits modern massively parallel
architectures, such as those found in Graphical Processing Units (GPUs), to
speed up the resolution of exact and approximated inference-based algorithms
for discrete optimization. The paper studies the proposed algorithm in both
centralized and distributed optimization contexts. The paper demonstrates that
the use of GPUs provides significant advantages in terms of runtime and
scalability, achieving up to two orders of magnitude in speedups and showing a
considerable reduction in execution time (up to 345 times faster) with respect
to a sequential version.
</summary>
    <author>
      <name>Ferdinando Fioretto</name>
    </author>
    <author>
      <name>Enrico Pontelli</name>
    </author>
    <author>
      <name>William Yeoh</name>
    </author>
    <author>
      <name>Rina Dechter</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05151v1</id>
    <updated>2016-08-18T01:21:27Z</updated>
    <published>2016-08-18T01:21:27Z</published>
    <title>Effective Multi-step Temporal-Difference Learning for Non-Linear
  Function Approximation</title>
    <summary>  Multi-step temporal-difference (TD) learning, where the update targets
contain information from multiple time steps ahead, is one of the most popular
forms of TD learning for linear function approximation. The reason is that
multi-step methods often yield substantially better performance than their
single-step counter-parts, due to a lower bias of the update targets. For
non-linear function approximation, however, single-step methods appear to be
the norm. Part of the reason could be that on many domains the popular
multi-step methods TD($\lambda$) and Sarsa($\lambda$) do not perform well when
combined with non-linear function approximation. In particular, they are very
susceptible to divergence of value estimates. In this paper, we identify the
reason behind this. Furthermore, based on our analysis, we propose a new
multi-step TD method for non-linear function approximation that addresses this
issue. We confirm the effectiveness of our method using two benchmark tasks
with neural networks as function approximation.
</summary>
    <author>
      <name>Harm van Seijen</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06175v1</id>
    <updated>2016-08-17T20:43:56Z</updated>
    <published>2016-08-17T20:43:56Z</published>
    <title>Effectiveness of greedily collecting items in open world games</title>
    <summary>  Since Pokemon Go sent millions on the quest of collecting virtual monsters,
an important question has been on the minds of many people: Is going after the
closest item first a time-and-cost-effective way to play? Here, we show that
this is in fact a good strategy which performs on average only 7% worse than
the best possible solution in terms of the total distance traveled to gather
all the items. Even when accounting for errors due to the inability of people
to accurately measure distances by eye, the performance only goes down to 16%
of the optimal solution.
</summary>
    <author>
      <name>Andrej Gajduk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05046v1</id>
    <updated>2016-08-17T18:59:23Z</updated>
    <published>2016-08-17T18:59:23Z</published>
    <title>Practical optimal experiment design with probabilistic programs</title>
    <summary>  Scientists often run experiments to distinguish competing theories. This
requires patience, rigor, and ingenuity - there is often a large space of
possible experiments one could run. But we need not comb this space by hand -
if we represent our theories as formal models and explicitly declare the space
of experiments, we can automate the search for good experiments, looking for
those with high expected information gain. Here, we present a general and
principled approach to experiment design based on probabilistic programming
languages (PPLs). PPLs offer a clean separation between declaring problems and
solving them, which means that the scientist can automate experiment design by
simply declaring her model and experiment spaces in the PPL without having to
worry about the details of calculating information gain. We demonstrate our
system in two case studies drawn from cognitive psychology, where we use it to
design optimal experiments in the domains of sequence prediction and
categorization. We find strong empirical validation that our automatically
designed experiments were indeed optimal. We conclude by discussing a number of
interesting questions for future research.
</summary>
    <author>
      <name>Long Ouyang</name>
    </author>
    <author>
      <name>Michael Henry Tessler</name>
    </author>
    <author>
      <name>Daniel Ly</name>
    </author>
    <author>
      <name>Noah Goodman</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04996v1</id>
    <updated>2016-08-17T15:20:35Z</updated>
    <published>2016-08-17T15:20:35Z</published>
    <title>Open Problem: Approximate Planning of POMDPs in the class of Memoryless
  Policies</title>
    <summary>  Planning plays an important role in the broad class of decision theory.
Planning has drawn much attention in recent work in the robotics and sequential
decision making areas. Recently, Reinforcement Learning (RL), as an
agent-environment interaction problem, has brought further attention to
planning methods. Generally in RL, one can assume a generative model, e.g.
graphical models, for the environment, and then the task for the RL agent is to
learn the model parameters and find the optimal strategy based on these learnt
parameters. Based on environment behavior, the agent can assume various types
of generative models, e.g. Multi Armed Bandit for a static environment, or
Markov Decision Process (MDP) for a dynamic environment. The advantage of these
popular models is their simplicity, which results in tractable methods of
learning the parameters and finding the optimal policy. The drawback of these
models is again their simplicity: these models usually underfit and
underestimate the actual environment behavior. For example, in robotics, the
agent usually has noisy observations of the environment inner state and MDP is
not a suitable model.
  More complex models like Partially Observable Markov Decision Process (POMDP)
can compensate for this drawback. Fitting this model to the environment, where
the partial observation is given to the agent, generally gives dramatic
performance improvement, sometimes unbounded improvement, compared to MDP. In
general, finding the optimal policy for the POMDP model is computationally
intractable and fully non convex, even for the class of memoryless policies.
The open problem is to come up with a method to find an exact or an approximate
optimal stochastic memoryless policy for POMDP models.
</summary>
    <author>
      <name>Kamyar Azizzadenesheli</name>
    </author>
    <author>
      <name>Alessandro Lazaric</name>
    </author>
    <author>
      <name>Animashree Anandkumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1602.07764</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">29th Annual Conference on Learning Theory (2016) 1639--1642</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.04996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04868v1</id>
    <updated>2016-08-17T06:24:46Z</updated>
    <published>2016-08-17T06:24:46Z</published>
    <title>Towards Music Captioning: Generating Music Playlist Descriptions</title>
    <summary>  Descriptions are often provided along with recommendations to help users'
discovery. Recommending automatically generated music playlists (e.g.
personalised playlists) introduces the problem of generating descriptions. In
this paper, we propose a method for generating music playlist descriptions,
which is called as music captioning. In the proposed method, audio content
analysis and natural language processing are adopted to utilise the information
of each track.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, ISMIR 2016 Late-breaking/session extended abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04846v1</id>
    <updated>2016-08-17T03:49:56Z</updated>
    <published>2016-08-17T03:49:56Z</published>
    <title>A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation</title>
    <summary>  Finding the most effective way to aggregate multi-subject fMRI data is a
long-standing and challenging problem. It is of increasing interest in
contemporary fMRI studies of human cognition due to the scarcity of data per
subject and the variability of brain anatomy and functional response across
subjects. Recent work on latent factor models shows promising results in this
task but this approach does not preserve spatial locality in the brain. We
examine two ways to combine the ideas of a factor model and a searchlight based
analysis to aggregate multi-subject fMRI data while preserving spatial
locality. We first do this directly by combining a recent factor method known
as a shared response model with searchlight analysis. Then we design a
multi-view convolutional autoencoder for the same task. Both approaches
preserve spatial locality and have competitive or better performance compared
with standard searchlight analysis and the shared response model applied across
the whole brain. We also report a system design to handle the computational
challenge of training the convolutional autoencoder.
</summary>
    <author>
      <name>Po-Hsuan Chen</name>
    </author>
    <author>
      <name>Xia Zhu</name>
    </author>
    <author>
      <name>Hejia Zhang</name>
    </author>
    <author>
      <name>Javier S. Turek</name>
    </author>
    <author>
      <name>Janice Chen</name>
    </author>
    <author>
      <name>Theodore L. Willke</name>
    </author>
    <author>
      <name>Uri Hasson</name>
    </author>
    <author>
      <name>Peter J. Ramadge</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04839v1</id>
    <updated>2016-08-17T02:38:44Z</updated>
    <published>2016-08-17T02:38:44Z</published>
    <title>Dynamic Collaborative Filtering with Compound Poisson Factorization</title>
    <summary>  Model-based collaborative filtering analyzes user-item interactions to infer
latent factors that represent user preferences and item characteristics in
order to predict future interactions. Most collaborative filtering algorithms
assume that these latent factors are static, although it has been shown that
user preferences and item perceptions drift over time. In this paper, we
propose a conjugate and numerically stable dynamic matrix factorization (DCPF)
based on compound Poisson matrix factorization that models the smoothly
drifting latent factors using Gamma-Markov chains. We propose a numerically
stable Gamma chain construction, and then present a stochastic variational
inference approach to estimate the parameters of our model. We apply our model
to time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF
achieves a higher predictive accuracy than state-of-the-art static and dynamic
factorization models.
</summary>
    <author>
      <name>Ghassen Jerfel</name>
    </author>
    <author>
      <name>Mehmet E. Basbug</name>
    </author>
    <author>
      <name>Barbara E. Engelhardt</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04698v1</id>
    <updated>2016-08-16T18:32:24Z</updated>
    <published>2016-08-16T18:32:24Z</published>
    <title>Evaluating Causal Models by Comparing Interventional Distributions</title>
    <summary>  The predominant method for evaluating the quality of causal models is to
measure the graphical accuracy of the learned model structure. We present an
alternative method for evaluating causal models that directly measures the
accuracy of estimated interventional distributions. We contrast such
distributional measures with structural measures, such as structural Hamming
distance and structural intervention distance, showing that structural measures
often correspond poorly to the accuracy of estimated interventional
distributions. We use a number of real and synthetic datasets to illustrate
various scenarios in which structural measures provide misleading results with
respect to algorithm selection and parameter tuning, and we recommend that
distributional measures become the new standard for evaluating causal models.
</summary>
    <author>
      <name>Dan Garant</name>
    </author>
    <author>
      <name>David Jensen</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04689v1</id>
    <updated>2016-08-16T17:54:40Z</updated>
    <published>2016-08-16T17:54:40Z</published>
    <title>A Shallow High-Order Parametric Approach to Data Visualization and
  Compression</title>
    <summary>  Explicit high-order feature interactions efficiently capture essential
structural knowledge about the data of interest and have been used for
constructing generative models. We present a supervised discriminative
High-Order Parametric Embedding (HOPE) approach to data visualization and
compression. Compared to deep embedding models with complicated deep
architectures, HOPE generates more effective high-order feature mapping through
an embarrassingly simple shallow model. Furthermore, two approaches to
generating a small number of exemplars conveying high-order interactions to
represent large-scale data sets are proposed. These exemplars in combination
with the feature mapping learned by HOPE effectively capture essential data
variations. Moreover, through HOPE, these exemplars are employed to increase
the computational efficiency of kNN classification for fast information
retrieval by thousands of times. For classification in two-dimensional
embedding space on MNIST and USPS datasets, our shallow method HOPE with simple
Sigmoid transformations significantly outperforms state-of-the-art supervised
deep embedding models based on deep neural networks, and even achieved
historically low test error rate of 0.65% in two-dimensional space on MNIST,
which demonstrates the representational efficiency and power of supervised
shallow models with high-order feature interactions.
</summary>
    <author>
      <name>Martin Renqiang Min</name>
    </author>
    <author>
      <name>Hongyu Guo</name>
    </author>
    <author>
      <name>Dongjin Song</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04544v1</id>
    <updated>2016-08-16T10:27:37Z</updated>
    <published>2016-08-16T10:27:37Z</published>
    <title>Free Lunch for Optimisation under the Universal Distribution</title>
    <summary>  Function optimisation is a major challenge in computer science. The No Free
Lunch theorems state that if all functions with the same histogram are assumed
to be equally probable then no algorithm outperforms any other in expectation.
We argue against the uniform assumption and suggest a universal prior exists
for which there is a free lunch, but where no particular class of functions is
favoured over another. We also prove upper and lower bounds on the size of the
free lunch.
</summary>
    <author>
      <name>Tom Everitt</name>
    </author>
    <author>
      <name>Tor Lattimore</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2014.6900546</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2014.6900546" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 2014 IEEE Congress on Evolutionary Computation
  (CEC), July 6-11, 2014, Beijing, China, pp. 167-174</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.04544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04428v1</id>
    <updated>2016-08-15T22:34:50Z</updated>
    <published>2016-08-15T22:34:50Z</published>
    <title>TerpreT: A Probabilistic Programming Language for Program Induction</title>
    <summary>  We study machine learning formulations of inductive program synthesis; given
input-output examples, we try to synthesize source code that maps inputs to
corresponding outputs. Our aims are to develop new machine learning approaches
based on neural networks and graphical models, and to understand the
capabilities of machine learning techniques relative to traditional
alternatives, such as those based on constraint solving from the programming
languages community.
  Our key contribution is the proposal of TerpreT, a domain-specific language
for expressing program synthesis problems. TerpreT is similar to a
probabilistic programming language: a model is composed of a specification of a
program representation (declarations of random variables) and an interpreter
describing how programs map inputs to outputs (a model connecting unknowns to
observations). The inference task is to observe a set of input-output examples
and infer the underlying program. TerpreT has two main benefits. First, it
enables rapid exploration of a range of domains, program representations, and
interpreter models. Second, it separates the model specification from the
inference algorithm, allowing like-to-like comparisons between different
approaches to inference. From a single TerpreT specification we automatically
perform inference using four different back-ends. These are based on gradient
descent, linear program (LP) relaxations for graphical models, discrete
satisfiability solving, and the Sketch program synthesis system.
  We illustrate the value of TerpreT by developing several interpreter models
and performing an empirical comparison between alternative inference
algorithms. Our key empirical finding is that constraint solvers dominate the
gradient descent and LP-based formulations. We conclude with suggestions for
the machine learning community to make progress on program synthesis.
</summary>
    <author>
      <name>Alexander L. Gaunt</name>
    </author>
    <author>
      <name>Marc Brockschmidt</name>
    </author>
    <author>
      <name>Rishabh Singh</name>
    </author>
    <author>
      <name>Nate Kushman</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>Jonathan Taylor</name>
    </author>
    <author>
      <name>Daniel Tarlow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 20 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04374v1</id>
    <updated>2016-08-15T19:38:35Z</updated>
    <published>2016-08-15T19:38:35Z</published>
    <title>A Geometric Framework for Convolutional Neural Networks</title>
    <summary>  In this paper, a geometric framework for neural networks is proposed. This
framework uses the inner product space structure underlying the parameter set
to perform gradient descent not in a component-based form, but in a
coordinate-free manner. Convolutional neural networks are described in this
framework in a compact form, with the gradients of standard --- and
higher-order --- loss functions calculated for each layer of the network. This
approach can be applied to other network structures and provides a basis on
which to create new networks.
</summary>
    <author>
      <name>Anthony L. Caterini</name>
    </author>
    <author>
      <name>Dong Eui Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04361v1</id>
    <updated>2016-08-15T18:45:08Z</updated>
    <published>2016-08-15T18:45:08Z</published>
    <title>Multi-way Monte Carlo Method for Linear Systems</title>
    <summary>  We study the Monte Carlo method for solving a linear system of the form $x =
H x + b$. A sufficient condition for the method to work is $\| H \| &lt; 1$, which
greatly limits the usability of this method. We improve this condition by
proposing a new multi-way Markov random walk, which is a generalization of the
standard Markov random walk. Under our new framework we prove that the
necessary and sufficient condition for our method to work is the spectral
radius $\rho(H^{+}) &lt; 1$, which is a weaker requirement than $\| H \| &lt; 1$. In
addition to solving more problems, our new method can work faster than the
standard algorithm. In numerical experiments on both synthetic and real world
matrices, we demonstrate the effectiveness of our new method.
</summary>
    <author>
      <name>Tao Wu</name>
    </author>
    <author>
      <name>David F. Gleich</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04672v1</id>
    <updated>2016-08-15T16:51:38Z</updated>
    <published>2016-08-15T16:51:38Z</published>
    <title>Informal Physical Reasoning Processes</title>
    <summary>  A fundamental question is whether Turing machines can model all reasoning
processes. We introduce an existence principle stating that the perception of
the physical existence of any Turing program can serve as a physical causation
for the application of any Turing-computable function to this Turing program.
The existence principle overcomes the limitation of the outputs of Turing
machines to lists, that is, recursively enumerable sets. The principle is
illustrated by productive partial functions for productive sets such as the set
of the Goedel numbers of the Turing-computable total functions. The existence
principle and productive functions imply the existence of physical systems
whose reasoning processes cannot be modeled by Turing machines. These systems
are called creative. Creative systems can prove the undecidable formula in
Goedel's theorem in another formal system which is constructed at a later point
in time. A hypothesis about creative systems, which is based on computer
experiments, is introduced.
</summary>
    <author>
      <name>Kurt Ammon</name>
    </author>
    <link href="http://arxiv.org/abs/1608.04672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04042v1</id>
    <updated>2016-08-14T01:07:29Z</updated>
    <published>2016-08-14T01:07:29Z</published>
    <title>Can Peripheral Representations Improve Clutter Metrics on Complex
  Scenes?</title>
    <summary>  Previous studies have proposed image-based clutter measures that correlate
with human search times and/or eye movements. However, most models do not take
into account the fact that the effects of clutter interact with the foveated
nature of the human visual system: visual clutter further from the fovea has an
increasing detrimental influence on perception. Here, we introduce a new
foveated clutter model to predict the detrimental effects in target search
utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz
et al.) as our non foveated clutter model, and we stack a peripheral
architecture on top of Feature Congestion for our foveated model. We introduce
the Peripheral Integration Feature Congestion (PIFC) coefficient, as a
fundamental ingredient of our model that modulates clutter as a non-linear gain
contingent on eccentricity. We finally show that Foveated Feature Congestion
(FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit
rate) than regular Feature Congestion r(44) = -0.19 in forced fixation search.
Thus, our model allows us to enrich clutter perception research by computing
fixation specific clutter maps. A toolbox for creating peripheral
architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and
Artificial Systems will be made available.
</summary>
    <author>
      <name>Arturo Deza</name>
    </author>
    <author>
      <name>Miguel P. Eckstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-Print to be presented at NIPS 2016 in Barcelona, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03938v1</id>
    <updated>2016-08-13T04:02:38Z</updated>
    <published>2016-08-13T04:02:38Z</published>
    <title>Determining Health Utilities through Data Mining of Social Media</title>
    <summary>  'Health utilities' measure patient preferences for perfect health compared to
specific unhealthy states, such as asthma, a fractured hip, or colon cancer.
When integrated over time, these estimations are called quality adjusted life
years (QALYs). Until now, characterizing health utilities (HUs) required
detailed patient interviews or written surveys. While reliable and specific,
this data remained costly due to efforts to locate, enlist and coordinate
participants. Thus the scope, context and temporality of diseases examined has
remained limited.
  Now that more than a billion people use social media, we propose a novel
strategy: use natural language processing to analyze public online
conversations for signals of the severity of medical conditions and correlate
these to known HUs using machine learning. In this work, we filter a dataset
that originally contained 2 billion tweets for relevant content on 60 diseases.
Using this data, our algorithm successfully distinguished mild from severe
diseases, which had previously been categorized only by traditional techniques.
This represents progress towards two related applications: first, predicting
HUs where such information is nonexistent; and second, (where rich HU data
already exists) estimating temporal or geographic patterns of disease severity
through data mining.
</summary>
    <author>
      <name>Christopher Thompson</name>
    </author>
    <author>
      <name>Josh Introne</name>
    </author>
    <author>
      <name>Clint Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03845v1</id>
    <updated>2016-08-12T16:47:01Z</updated>
    <published>2016-08-12T16:47:01Z</published>
    <title>Traversing Environments Using Possibility Graphs for Humanoid Robots</title>
    <summary>  Locomotion for legged robots poses considerable challenges when confronted by
obstacles and adverse environments. Footstep planners are typically only
designed for one mode of locomotion, but traversing unfavorable environments
may require several forms of locomotion to be sequenced together, such as
walking, crawling, and jumping. Multi-modal motion planners can be used to
address some of these problems, but existing implementations tend to be
time-consuming and are limited to quasi-static actions. This paper presents a
motion planning method to traverse complex environments using multiple
categories of actions. We introduce the concept of the "Possibility Graph",
which uses high-level approximations of constraint manifolds to rapidly explore
the "possibility" of actions, thereby allowing lower-level single-action motion
planners to be utilized more efficiently. We show that the Possibility Graph
can quickly find paths through several different challenging environments which
require various combinations of actions in order to traverse.
</summary>
    <author>
      <name>Michael X. Grey</name>
    </author>
    <author>
      <name>Aaron D. Ames</name>
    </author>
    <author>
      <name>C. Karen Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the International Workshop on the Algorithmic
  Foundations of Robotics (2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03824v1</id>
    <updated>2016-08-12T15:29:05Z</updated>
    <published>2016-08-12T15:29:05Z</published>
    <title>Perceptual Reward Functions</title>
    <summary>  Reinforcement learning problems are often described through rewards that
indicate if an agent has completed some task. This specification can yield
desirable behavior, however many problems are difficult to specify in this
manner, as one often needs to know the proper configuration for the agent. When
humans are learning to solve tasks, we often learn from visual instructions
composed of images or videos. Such representations motivate our development of
Perceptual Reward Functions, which provide a mechanism for creating visual task
descriptions. We show that this approach allows an agent to learn from rewards
that are based on raw pixels rather than internal parameters.
</summary>
    <author>
      <name>Ashley Edwards</name>
    </author>
    <author>
      <name>Charles Isbell</name>
    </author>
    <author>
      <name>Atsuo Takanishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Deep Reinforcement Learning: Frontiers and Challenges Workshop, IJCAI
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03785v1</id>
    <updated>2016-08-12T13:13:10Z</updated>
    <published>2016-08-12T13:13:10Z</published>
    <title>Compositional Distributional Cognition</title>
    <summary>  We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of
[32] within the categorical compositional semantics (CatCo) of [13], forming a
model of categorical compositional cognition (CatCog). This resolves intrinsic
problems with ICS such as the fact that representations inhabit an unbounded
space and that sentences with differing tree structures cannot be directly
compared. We do so in a way that makes the most of the grammatical structure
available, in contrast to strategies like circular convolution. Using the CatCo
model also allows us to make use of tools developed for CatCo such as the
representation of ambiguity and logical reasoning via density matrices,
structural meanings for words such as relative pronouns, and addressing over-
and under-extension, all of which are present in cognitive processes. Moreover
the CatCog framework is sufficiently flexible to allow for entirely different
representations of meaning, such as conceptual spaces. Interestingly, since the
CatCo model was largely inspired by categorical quantum mechanics, so is
CatCog.
</summary>
    <author>
      <name>Yaared Al-Mehairi</name>
    </author>
    <author>
      <name>Bob Coecke</name>
    </author>
    <author>
      <name>Martha Lewis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Quantum Interaction 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03672v1</id>
    <updated>2016-08-12T04:29:14Z</updated>
    <published>2016-08-12T04:29:14Z</published>
    <title>Inferring unknown biological function by integration of GO annotations
  and gene expression data</title>
    <summary>  Characterizing genes with semantic information is an important process
regarding the description of gene products. In spite that complete genomes of
many organisms have been already sequenced, the biological functions of all of
their genes are still unknown. Since experimentally studying the functions of
those genes, one by one, would be unfeasible, new computational methods for
gene functions inference are needed. We present here a novel computational
approach for inferring biological function for a set of genes with previously
unknown function, given a set of genes with well-known information. This
approach is based on the premise that genes with similar behaviour should be
grouped together. This is known as the guilt-by-association principle. Thus, it
is possible to take advantage of clustering techniques to obtain groups of
unknown genes that are co-clustered with genes that have well-known semantic
information (GO annotations). Meaningful knowledge to infer unknown semantic
information can therefore be provided by these well-known genes. We provide a
method to explore the potential function of new genes according to those
currently annotated. The results obtained indicate that the proposed approach
could be a useful and effective tool when used by biologists to guide the
inference of biological functions for recently discovered genes. Our work sets
an important landmark in the field of identifying unknown gene functions
through clustering, using an external source of biological input. A simple web
interface to this proposal can be found at
http://fich.unl.edu.ar/sinc/webdemo/gamma-am/.
</summary>
    <author>
      <name>Guillermo Leale</name>
    </author>
    <author>
      <name>Ariel Bayá</name>
    </author>
    <author>
      <name>Diego Milone</name>
    </author>
    <author>
      <name>Pablo Granitto</name>
    </author>
    <author>
      <name>Georgina Stegmayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03507v2</id>
    <updated>2016-08-13T08:08:35Z</updated>
    <published>2016-08-11T15:43:55Z</published>
    <title>Learning Mobile App Usage Routine through Learning Automata</title>
    <summary>  Since its conception, smart app market has grown exponentially. Success in
the app market depends on many factors among which the quality of the app is a
significant contributor, such as energy use. Nevertheless, smartphones, as a
subset of mobile computing devices. inherit the limited power resource
constraint. Therefore, there is a challenge of maintaining the resource while
increasing the target app quality. This paper introduces Learning Automata (LA)
as an online learning method to learn and predict the app usage routines of the
users. Such prediction can leverage the app cache functionality of the
operating system and thus (i) decreases app launch time and (ii) preserve
battery. Our algorithm, which is an online learning approach, temporally
updates and improves the internal states of itself. In particular, it learns
the transition probabilities between app launching. Each App launching instance
updates the transition probabilities related to that App, and this will result
in improving the prediction. We benefit from a real-world lifelogging dataset
and our experimental results show considerable success with respect to the two
baseline methods that are used currently for smartphone app prediction
approaches.
</summary>
    <author>
      <name>Ramin Rahnamoun</name>
    </author>
    <author>
      <name>Reza Rawassizadeh</name>
    </author>
    <author>
      <name>Arash Maskooki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03507v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03507v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03026v1</id>
    <updated>2016-08-10T02:10:40Z</updated>
    <published>2016-08-10T02:10:40Z</published>
    <title>Towards Visual Type Theory as a Mathematical Tool and Mathematical User
  Interface</title>
    <summary>  A visual type theory is a cognitive tool that has much in common with
language, and may be regarded as an exceptional form of spatial text adjunct. A
mathematical visual type theory, called NPM, has been under development that
can be viewed as an early-stage project in mathematical knowledge management
and mathematical user interface development. We discuss in greater detail the
notion of a visual type theory, report on progress towards a usable
mathematical visual type theory, and discuss the outlook for future work on
this project.
</summary>
    <author>
      <name>Lucius Schoenbaum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, to appear in Joint Proceedings of the FM4M, MathUI, and
  ThEdu Workshops, Doctoral Program, and Work in Progress at the Conference on
  Intelligent Computer Mathematics, Bialystok, Poland 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03000v1</id>
    <updated>2016-08-09T23:05:03Z</updated>
    <published>2016-08-09T23:05:03Z</published>
    <title>Neural Generation of Regular Expressions from Natural Language with
  Minimal Domain Knowledge</title>
    <summary>  This paper explores the task of translating natural language queries into
regular expressions which embody their meaning. In contrast to prior work, the
proposed neural model does not utilize domain-specific crafting, learning to
translate directly from a parallel corpus. To fully explore the potential of
neural models, we propose a methodology for collecting a large corpus of
regular expression, natural language pairs. Our resulting model achieves a
performance gain of 19.6% over previous state-of-the-art models.
</summary>
    <author>
      <name>Nicholas Locascio</name>
    </author>
    <author>
      <name>Karthik Narasimhan</name>
    </author>
    <author>
      <name>Eduardo DeLeon</name>
    </author>
    <author>
      <name>Nate Kushman</name>
    </author>
    <author>
      <name>Regina Barzilay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to be published in EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02971v1</id>
    <updated>2016-08-09T20:04:40Z</updated>
    <published>2016-08-09T20:04:40Z</published>
    <title>Neuroevolution-Based Inverse Reinforcement Learning</title>
    <summary>  The problem of Learning from Demonstration is targeted at learning to perform
tasks based on observed examples. One approach to Learning from Demonstration
is Inverse Reinforcement Learning, in which actions are observed to infer
rewards. This work combines a feature based state evaluation approach to
Inverse Reinforcement Learning with neuroevolution, a paradigm for modifying
neural networks based on their performance on a given task. Neural networks are
used to learn from a demonstrated expert policy and are evolved to generate a
policy similar to the demonstration. The algorithm is discussed and evaluated
against competitive feature-based Inverse Reinforcement Learning approaches. At
the cost of execution time, neural networks allow for non-linear combinations
of features in state evaluations. These valuations may correspond to state
value or state reward. This results in better correspondence to observed
examples as opposed to using linear combinations. This work also extends
existing work on Bayesian Non-Parametric Feature Construction for Inverse
Reinforcement Learning by using non-linear combinations of intermediate data to
improve performance. The algorithm is observed to be specifically suitable for
a linearly solvable non-deterministic Markov Decision Processes in which
multiple rewards are sparsely scattered in state space. A conclusive
performance hierarchy between evaluated algorithms is presented.
</summary>
    <author>
      <name>Karan K. Budhraja</name>
    </author>
    <author>
      <name>Tim Oates</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02858v1</id>
    <updated>2016-08-09T16:33:03Z</updated>
    <published>2016-08-09T16:33:03Z</published>
    <title>Liftago On-Demand Transport Dataset and Market Formation Algorithm Based
  on Machine Learning</title>
    <summary>  This document serves as a technical report for the analysis of on-demand
transport dataset. Moreover we show how the dataset can be used to develop a
market formation algorithm based on machine learning. Data used in this work
comes from Liftago, a Prague based company which connects taxi drivers and
customers through a smartphone app. The dataset is analysed from the
machine-learning perspective: we give an overview of features available as well
as results of feature ranking. Later we propose the SImple Data-driven MArket
Formation (SIDMAF) algorithm which aims to improve a relevance while connecting
customers with relevant drivers. We compare the heuristics currently used by
Liftago with SIDMAF using two key performance indicators.
</summary>
    <author>
      <name>Jan Mrkos</name>
    </author>
    <author>
      <name>Jan Drchal</name>
    </author>
    <author>
      <name>Malcolm Egan</name>
    </author>
    <author>
      <name>Michal Jakob</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 2 figures, supplemental information for a journal paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02833v2</id>
    <updated>2016-08-10T07:44:59Z</updated>
    <published>2016-08-09T15:21:33Z</published>
    <title>Facial Expression Recognition Using a Hybrid CNN-SIFT Aggregator</title>
    <summary>  Recognizing facial expression has remained a challenging task in computer
vision. Deriving an effective facial expression recognition is an important
step for successful human-computer interaction systems. This paper describes a
novel approach towards facial expression recognition task. It is motivated by
the success of Convolutional Neural Networks (CNN) on face recognition
problems. Unlike other works, we focus on getting good accuracy results while
requiring only a small sample data to train the model by merging the CNN and
SIFT features. The proposed classification model is an aggregation of multiple
deep convolutional neural networks and a hybrid CNN-SIFT classifiers. The goal
of using SIFT features is to increase the performance on small data as SIFT
does not require large training data to generate useful features. The model has
been tested on FER-2013, CK+ and SFEW 2.0 datasets. The results showed how
CNN-SIFT feature improve the accuracy when added as a voting member in an
ensemble classifier. It generates state-of-art results on FER-2013 and CK+
datasets, where it achieved 73.58% on FER-2013 and 99.35% on CK+.
</summary>
    <author>
      <name>Mundher Al-Shabi</name>
    </author>
    <author>
      <name>Wooi Ping Cheah</name>
    </author>
    <author>
      <name>Tee Connie</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02833v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02833v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02763v1</id>
    <updated>2016-08-09T11:13:46Z</updated>
    <published>2016-08-09T11:13:46Z</published>
    <title>Resolving Spatial-Time Conflicts In A Set Of Any-angle Or
  Angle-constrained Grid Paths</title>
    <summary>  We study the multi-agent path finding problem (MAPF) for a group of agents
which are allowed to move into arbitrary directions on a 2D square grid. We
focus on centralized conflict resolution for independently computed plans. We
propose an algorithm that eliminates conflicts by using local re-planning and
introducing time offsets to the execution of paths by different agents.
Experimental results show that the algorithm can find high quality
conflict-free solutions at low computational cost.
</summary>
    <author>
      <name>Konstantin Yakovlev</name>
    </author>
    <author>
      <name>Anton Andreychuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">as submitted to the 2nd Workshop on Multi-Agent Path Finding
  (http://www.andrew.cmu.edu/user/gswagner/workshop/ijcai_2016_multirobot_path_finding.html)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02717v1</id>
    <updated>2016-08-09T08:24:02Z</updated>
    <published>2016-08-09T08:24:02Z</published>
    <title>Mean Box Pooling: A Rich Image Representation and Output Embedding for
  the Visual Madlibs Task</title>
    <summary>  We present Mean Box Pooling, a novel visual representation that pools over
CNN representations of a large number, highly overlapping object proposals. We
show that such representation together with nCCA, a successful multimodal
embedding technique, achieves state-of-the-art performance on the Visual
Madlibs task. Moreover, inspired by the nCCA's objective function, we extend
classical CNN+LSTM approach to train the network by directly maximizing the
similarity between the internal representation of the deep learning
architecture and candidate answers. Again, such approach achieves a significant
improvement over the prior work that also uses CNN+LSTM approach on Visual
Madlibs.
</summary>
    <author>
      <name>Ashkan Mokarian</name>
    </author>
    <author>
      <name>Mateusz Malinowski</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to BMVC'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02693v1</id>
    <updated>2016-08-09T05:48:51Z</updated>
    <published>2016-08-09T05:48:51Z</published>
    <title>Deeply Semantic Inductive Spatio-Temporal Learning</title>
    <summary>  We present an inductive spatio-temporal learning framework rooted in
inductive logic programming. With an emphasis on visuo-spatial language, logic,
and cognition, the framework supports learning with relational spatio-temporal
features identifiable in a range of domains involving the processing and
interpretation of dynamic visuo-spatial imagery. We present a prototypical
system, and an example application in the domain of computing for visual arts
and computational cognitive science.
</summary>
    <author>
      <name>Jakob Suchan</name>
    </author>
    <author>
      <name>Mehul Bhatt</name>
    </author>
    <author>
      <name>Carl Schultz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ILP 2016: 26th International Conference
  on Inductive Logic Programming 4th - 6th September 2016, London. Keywords:
  Spatio-Temporal Learning; Dynamic Visuo-Spatial Imagery; Declarative Spatial
  Reasoning; Inductive Logic Programming; AI and Art</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02682v1</id>
    <updated>2016-08-09T03:07:50Z</updated>
    <published>2016-08-09T03:07:50Z</published>
    <title>Exact Structure Learning of Bayesian Networks by Optimal Path Extension</title>
    <summary>  Bayesian networks are probabilistic graphical models often used in big data
analytics. The problem of exact structure learning is to find a network
structure that is optimal under certain scoring criteria. The problem is known
to be NP-hard and the existing methods are both computationally and memory
intensive. In this paper, we introduce a new approach for exact structure
learning. Our strategy is to leverage relationship between a partial network
structure and the remaining variables to constraint the number of ways in which
the partial network can be optimally extended. Via experimental results, we
show that the method provides up to three times improvement in runtime, and
orders of magnitude reduction in memory consumption over the current best
algorithms.
</summary>
    <author>
      <name>Subhadeep Karan</name>
    </author>
    <author>
      <name>Jaroslaw Zola</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02659v1</id>
    <updated>2016-08-08T23:48:19Z</updated>
    <published>2016-08-08T23:48:19Z</published>
    <title>Mouse Movement and Probabilistic Graphical Models Based E-Learning
  Activity Recognition Improvement Possibilistic Model</title>
    <summary>  Automatically recognizing the e-learning activities is an important task for
improving the online learning process. Probabilistic graphical models such as
hidden Markov models and conditional random fields have been successfully used
in order to identify a Web users activity. For such models, the sequences of
observation are crucial for training and inference processes. Despite the
efficiency of these probabilistic graphical models in segmenting and labeling
stochastic sequences, their performance is adversely affected by the imperfect
quality of data used for the construction of sequences of observation. In this
paper, a formalism of the possibilistic theory will be used in order to propose
a new approach for observation sequences preparation. The eminent contribution
of our approach is to evaluate the effect of possibilistic reasoning during the
generation of observation sequences on the effectiveness of hidden Markov
models and conditional random fields models. Using a dataset containing 51 real
manipulations related to three types of learners tasks, the preliminary
experiments demonstrate that the sequences of observation obtained based on
possibilistic reasoning significantly improve the performance of hidden Marvov
models and conditional random fields models in the automatic recognition of the
e-learning activities.
</summary>
    <author>
      <name>Anis Elbahi</name>
    </author>
    <author>
      <name>Mohamed Nazih Omri</name>
    </author>
    <author>
      <name>Mohamed Ali Mahjoub</name>
    </author>
    <author>
      <name>Kamel Garrouch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in AJSE 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02658v2</id>
    <updated>2016-08-24T21:38:17Z</updated>
    <published>2016-08-08T23:46:59Z</published>
    <title>Revisiting Causality Inference In Markov Chain</title>
    <summary>  Identifying causal relationships is a key premise of scientific research.
Given the mass of observational data in many disciplines, new machine learning
methods offer the possibility of using an empirical approach to identifying
unappreciated causal relationships and to understanding causal behavior.
Conventional methods of causality inference from observational data require a
considerable length of time series data to capture cause and effect
relationships. We believe that important causal relationships can be inferred
from the composition of one-step transition rates (Markov Chains) to and from
an event. Here we introduce 'Causality Inference using Composition of
Transitions' (CICT), a computationally efficient method that reveals causal
structure with high accuracy. We characterize the differences in causes,
effects, and random events in the composition of their inputs and outputs. To
demonstrate our method, we have used an administrative inpatient healthcare
dataset to set up a graph network of patients transition between different
diagnoses. Then we apply our method to patients transition graph, revealing
deep and complex causal structure between clinical conditions. Our method is
highly accurate in predicting whether a transition in a Markov chain is causal
or random and performs well in identifying the direction of causality in
bidirectional associations. Moreover, CICT brings in new information that
enables unsupervised clustering methods to discriminate causality from
randomness. Comprehensive performance analysis using C-statistics,
goodness-of-fit statistics and decision analysis of predictive models, as well
as comparison with the medical ground truth, validates our findings.
</summary>
    <author>
      <name>Abbas Shojaee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages; 5560 words. This edition is improved with further details
  in the discussion section and Figure 1. Draft version for public review.
  Other authors will be added in final revision; For feedback, opinions, or
  questions please contact: abbas.shojaee@gmail.com OR abbas.shojaee@yale.edu</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02658v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02658v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02644v2</id>
    <updated>2016-08-10T03:22:11Z</updated>
    <published>2016-08-08T22:33:13Z</published>
    <title>Holophrasm: a neural Automated Theorem Prover for higher-order logic</title>
    <summary>  I propose a system for Automated Theorem Proving in higher order logic using
deep learning and eschewing hand-constructed features. Holophrasm exploits the
formalism of the Metamath language and explores partial proof trees using a
neural-network-augmented bandit algorithm and a sequence-to-sequence model for
action enumeration. The system proves 14% of its test theorems from Metamath's
set.mm module.
</summary>
    <author>
      <name>Daniel Whalen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02644v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02644v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02450v1</id>
    <updated>2016-08-08T14:26:46Z</updated>
    <published>2016-08-08T14:26:46Z</published>
    <title>ASP for Minimal Entailment in a Rational Extension of SROEL</title>
    <summary>  In this paper we exploit Answer Set Programming (ASP) for reasoning in a
rational extension SROEL-R-T of the low complexity description logic SROEL,
which underlies the OWL EL ontology language. In the extended language, a
typicality operator T is allowed to define concepts T(C) (typical C's) under a
rational semantics. It has been proven that instance checking under rational
entailment has a polynomial complexity. To strengthen rational entailment, in
this paper we consider a minimal model semantics. We show that, for arbitrary
SROEL-R-T knowledge bases, instance checking under minimal entailment is
\Pi^P_2-complete. Relying on a Small Model result, where models correspond to
answer sets of a suitable ASP encoding, we exploit Answer Set Preferences (and,
in particular, the asprin framework) for reasoning under minimal entailment.
The paper is under consideration for acceptance in Theory and Practice of Logic
Programming.
</summary>
    <author>
      <name>Laura Giordano</name>
    </author>
    <author>
      <name>Daniele Theseider Dupré</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03532v1</id>
    <updated>2016-08-08T12:54:57Z</updated>
    <published>2016-08-08T12:54:57Z</published>
    <title>QPass: a Merit-based Evaluation of Soccer Passes</title>
    <summary>  Quantitative analysis of soccer players' passing ability focuses on
descriptive statistics without considering the players' real contribution to
the passing and ball possession strategy of their team. Which player is able to
help the build-up of an attack, or to maintain the possession of the ball? We
introduce a novel methodology called QPass to answer questions like these
quantitatively. Based on the analysis of an entire season, we rank the players
based on the intrinsic value of their passes using QPass. We derive an album of
pass trajectories for different gaming styles. Our methodology reveals a quite
counterintuitive paradigm: losing the ball possession could lead to better
chances to win a game.
</summary>
    <author>
      <name>Laszlo Gyarmati</name>
    </author>
    <author>
      <name>Rade Stanojevic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2016 ACM KDD Workshop on Large-Scale Sports Analytics</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02406v1</id>
    <updated>2016-08-08T12:24:05Z</updated>
    <published>2016-08-08T12:24:05Z</published>
    <title>Complexity Results for Manipulation, Bribery and Control of the Kemeny
  Procedure in Judgment Aggregation</title>
    <summary>  We study the computational complexity of several scenarios of strategic
behavior for the Kemeny procedure in the setting of judgment aggregation. In
particular, we investigate (1) manipulation, where an individual aims to
achieve a better group outcome by reporting an insincere individual opinion,
(2) bribery, where an external agent aims to achieve an outcome with certain
properties by bribing a number of individuals, and (3) control (by adding or
deleting issues), where an external agent aims to achieve an outcome with
certain properties by influencing the set of issues in the judgment aggregation
situation. We show that determining whether these types of strategic behavior
are possible (and if so, computing a policy for successful strategic behavior)
is complete for the second level of the Polynomial Hierarchy. That is, we show
that these problems are $\Sigma^p_2$-complete.
</summary>
    <author>
      <name>Ronald de Haan</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02341v1</id>
    <updated>2016-08-08T07:44:24Z</updated>
    <published>2016-08-08T07:44:24Z</published>
    <title>Towards Representation Learning with Tractable Probabilistic Models</title>
    <summary>  Probabilistic models learned as density estimators can be exploited in
representation learning beside being toolboxes used to answer inference queries
only. However, how to extract useful representations highly depends on the
particular model involved. We argue that tractable inference, i.e. inference
that can be computed in polynomial time, can enable general schemes to extract
features from black box models. We plan to investigate how Tractable
Probabilistic Models (TPMs) can be exploited to generate embeddings by random
query evaluations. We devise two experimental designs to assess and compare
different TPMs as feature extractors in an unsupervised representation learning
framework. We show some experimental results on standard image datasets by
applying such a method to Sum-Product Networks and Mixture of Trees as
tractable models generating embeddings.
</summary>
    <author>
      <name>Antonio Vergari</name>
    </author>
    <author>
      <name>Nicola Di Mauro</name>
    </author>
    <author>
      <name>Floriana Esposito</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, submitted to ECML-PKDD 2016 Doctoral Consortium</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02315v1</id>
    <updated>2016-08-08T04:59:40Z</updated>
    <published>2016-08-08T04:59:40Z</published>
    <title>Blankets Joint Posterior score for learning irregular Markov network
  structures</title>
    <summary>  Markov networks are extensively used to model complex sequential, spatial,
and relational interactions in a wide range of fields. By learning the
structure of independences of a domain, more accurate joint probability
distributions can be obtained for inference tasks or, more directly, for
interpreting the most significant relations among the variables. However, the
performance of current available methods for learning the structure is heavily
dependent on the choice of two factors: the structure representation, and the
approach for learning such representation. This work follows the probabilistic
maximum-a-posteriori approach for learning undirected graph structures, which
has gained interest recently. Thus, the Blankets Joint Posterior score is
designed for computing the posterior probability of structures given data. In
particular, the score proposed can improve the learning process when the
solution structure is irregular (that is, when there exists an imbalance in the
number of edges over the nodes), which is a property present in many real-world
networks. The approximation proposed computes the joint posterior distribution
from the collection of Markov blankets of the structure. Essentially, a series
of conditional distributions are calculated by using, information about other
Markov blankets in the network as evidence. Our experimental results
demonstrate that the proposed score has better sample complexity for learning
irregular structures, when compared to state-of-the-art scores. By considering
optimization with greedy hill-climbing search, we prove for several study cases
that our score identifies structures with fewer errors than competitors.
</summary>
    <author>
      <name>Federico Schlüter</name>
    </author>
    <author>
      <name>Yanela Strappa</name>
    </author>
    <author>
      <name>Facundo Bromberg</name>
    </author>
    <author>
      <name>Diego H. Milone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02287v1</id>
    <updated>2016-08-08T00:14:50Z</updated>
    <published>2016-08-08T00:14:50Z</published>
    <title>Delta Epsilon Alpha Star: A PAC-Admissible Search Algorithm</title>
    <summary>  Delta Epsilon Alpha Star is a minimal coverage, real-time robotic search
algorithm that yields a moderately aggressive search path with minimal
backtracking. Search performance is bounded by a placing a combinatorial bound,
epsilon and delta, on the maximum deviation from the theoretical shortest path
and the probability at which further deviations can occur. Additionally, we
formally define the notion of PAC-admissibility -- a relaxed admissibility
criteria for algorithms, and show that PAC-admissible algorithms are better
suited to robotic search situations than epsilon-admissible or strict
algorithms.
</summary>
    <author>
      <name>David Cox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02229v1</id>
    <updated>2016-08-07T15:52:28Z</updated>
    <published>2016-08-07T15:52:28Z</published>
    <title>Towards the Self-constructive Brain: emergence of adaptive behavior</title>
    <summary>  Adaptive behavior is mainly the result of adaptive brains. We go a step
beyond and claim that the brain does not only adapt to its surrounding reality
but rather, it builds itself up to constructs its own reality. That is, rather
than just trying to passively understand its environment, the brain is the
architect of its own reality in an active process where its internal models of
the external world frame how its new interactions with the environment are
assimilated. These internal models represent relevant predictive patterns of
interaction all over the different brain structures: perceptual, sensorimotor,
motor, etc. The emergence of adaptive behavior arises from this
self-constructive nature of the brain, based on the following principles of
organization: self-experimental, self- growing, and self-repairing.
Self-experimental, since to ensure survival, the self-constructive brain (SCB)
is an active machine capable of performing experiments of its own interactions
with the environment by mental simulation. Self-growing, since it dynamically
and incrementally constructs internal structures in order to build a model of
the world as it gathers statistics from its interactions with the environment.
Self-repairing, since to survive the SCB must also be robust and capable of
finding ways to repair parts of previously working structures and hence
re-construct a previous relevant pattern of activity.
</summary>
    <author>
      <name>Fernando Corbacho</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02193v2</id>
    <updated>2016-09-14T10:39:58Z</updated>
    <published>2016-08-07T08:35:03Z</published>
    <title>Spacetimes with Semantics (III) - The Structure of Functional Knowledge
  Representation and Artificial Reasoning</title>
    <summary>  Using the previously developed concepts of semantic spacetime, I explore the
interpretation of knowledge representations, and their structure, as a semantic
system, within the framework of promise theory. By assigning interpretations to
phenomena, from observers to observed, we may approach a simple description of
knowledge-based functional systems, with direct practical utility. The focus is
especially on the interpretation of concepts, associative knowledge, and
context awareness. The inference seems to be that most if not all of these
concepts emerge from purely semantic spacetime properties, which opens the
possibility for a more generalized understanding of what constitutes a
learning, or even `intelligent' system.
  Some key principles emerge for effective knowledge representation: 1)
separation of spacetime scales, 2) the recurrence of four irreducible types of
association, by which intent propagates: aggregation, causation, cooperation,
and similarity, 3) the need for discrimination of identities (discrete), which
is assisted by distinguishing timeline simultaneity from sequential events, and
4) the ability to learn (memory). It is at least plausible that emergent
knowledge abstraction capabilities have their origin in basic spacetime
structures.
  These notes present a unified view of mostly well-known results; they allow
us to see information models, knowledge representations, machine learning, and
semantic networking (transport and information base) in a common framework. The
notion of `smart spaces' thus encompasses artificial systems as well as living
systems, across many different scales, e.g. smart cities and organizations.
</summary>
    <author>
      <name>Mark Burgess</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">122 pages, builiding on parts I and II</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02193v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02193v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11; F.4.1; I.2.4; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02165v1</id>
    <updated>2016-08-07T00:29:53Z</updated>
    <published>2016-08-07T00:29:53Z</published>
    <title>ShapeFit and ShapeKick for Robust, Scalable Structure from Motion</title>
    <summary>  We introduce a new method for location recovery from pair-wise directions
that leverages an efficient convex program that comes with exact recovery
guarantees, even in the presence of adversarial outliers. When pairwise
directions represent scaled relative positions between pairs of views
(estimated for instance with epipolar geometry) our method can be used for
location recovery, that is the determination of relative pose up to a single
unknown scale. For this task, our method yields performance comparable to the
state-of-the-art with an order of magnitude speed-up. Our proposed numerical
framework is flexible in that it accommodates other approaches to location
recovery and can be used to speed up other methods. These properties are
demonstrated by extensively testing against state-of-the-art methods for
location recovery on 13 large, irregular collections of images of real scenes
in addition to simulated data with ground truth.
</summary>
    <author>
      <name>Thomas Goldstein</name>
    </author>
    <author>
      <name>Paul Hand</name>
    </author>
    <author>
      <name>Choongbum Lee</name>
    </author>
    <author>
      <name>Vladislav Voroninski</name>
    </author>
    <author>
      <name>Stefano Soatto</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; I.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02164v1</id>
    <updated>2016-08-06T23:49:48Z</updated>
    <published>2016-08-06T23:49:48Z</published>
    <title>Adapting Deep Network Features to Capture Psychological Representations</title>
    <summary>  Deep neural networks have become increasingly successful at solving classic
perception problems such as object recognition, semantic segmentation, and
scene understanding, often reaching or surpassing human-level accuracy. This
success is due in part to the ability of DNNs to learn useful representations
of high-dimensional inputs, a problem that humans must also solve. We examine
the relationship between the representations learned by these networks and
human psychological representations recovered from similarity judgments. We
find that deep features learned in service of object classification account for
a significant amount of the variance in human similarity judgments for a set of
animal images. However, these features do not capture some qualitative
distinctions that are a key part of human representations. To remedy this, we
develop a method for adapting deep features to align with human similarity
judgments, resulting in image representations that can potentially be used to
extend the scope of psychological experiments.
</summary>
    <author>
      <name>Joshua C. Peterson</name>
    </author>
    <author>
      <name>Joshua T. Abbott</name>
    </author>
    <author>
      <name>Thomas L. Griffiths</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, To appear in the Proceedings of the 38th Annual
  Conference of the Cognitive Science Society, Winner of the Computational
  Modeling Prize in Perception/Action</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02158v1</id>
    <updated>2016-08-06T22:18:18Z</updated>
    <published>2016-08-06T22:18:18Z</published>
    <title>Deep Survival Analysis</title>
    <summary>  The electronic health record (EHR) provides an unprecedented opportunity to
build actionable tools to support physicians at the point of care. In this
paper, we investigate survival analysis in the context of EHR data. We
introduce deep survival analysis, a hierarchical generative approach to
survival analysis. It departs from previous approaches in two primary ways: (1)
all observations, including covariates, are modeled jointly conditioned on a
rich latent structure; and (2) the observations are aligned by their failure
time, rather than by an arbitrary time zero as in traditional survival
analysis. Further, it (3) scalably handles heterogeneous (continuous and
discrete) data types that occur in the EHR. We validate deep survival analysis
model by stratifying patients according to risk of developing coronary heart
disease (CHD). Specifically, we study a dataset of 313,000 patients
corresponding to 5.5 million months of observations. When compared to the
clinically validated Framingham CHD risk score, deep survival analysis is
significantly superior in stratifying patients according to their risk.
</summary>
    <author>
      <name>Rajesh Ranganath</name>
    </author>
    <author>
      <name>Adler Perotte</name>
    </author>
    <author>
      <name>Noémie Elhadad</name>
    </author>
    <author>
      <name>David Blei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 2016 Machine Learning and Healthcare Conference (MLHC
  2016), Los Angeles, CA</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03544v1</id>
    <updated>2016-08-06T14:13:28Z</updated>
    <published>2016-08-06T14:13:28Z</published>
    <title>Online Context-Dependent Clustering in Recommendations based on
  Exploration-Exploitation Algorithms</title>
    <summary>  We investigate two context-dependent clustering techniques for content
recommendation based on exploration-exploitation strategies in contextual
multi-armed bandit settings. Our algorithms dynamically group users based on
the items under consideration and, possibly, group items based on the
similarity of the clusterings induced over the users. The resulting algorithm
thus takes advantage of preference patterns in the data in a way akin to
collaborative filtering methods. We provide an empirical analysis on extensive
real-world datasets, showing scalability and increased prediction performance
over state-of-the-art methods for clustering bandits. For one of the two
algorithms we also give a regret analysis within a standard linear stochastic
noise setting.
</summary>
    <author>
      <name>Shuai Li</name>
    </author>
    <author>
      <name>Claudio Gentile</name>
    </author>
    <author>
      <name>Alexandros Karatzoglou</name>
    </author>
    <author>
      <name>Giovanni Zappella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a copy of arXiv:1502.03473v3. It is necessary because the
  original file was overwritten with another paper. arXiv admin note:
  substantial text overlap with arXiv:1510.03164</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02082v2</id>
    <updated>2016-08-12T16:56:18Z</updated>
    <published>2016-08-06T08:59:21Z</published>
    <title>COREALMLIB: An ALM Library Translated from the Component Library</title>
    <summary>  This paper presents COREALMLIB, an ALM library of commonsense knowledge about
dynamic domains. The library was obtained by translating part of the COMPONENT
LIBRARY (CLIB) into the modular action language ALM. CLIB consists of general
reusable and composable commonsense concepts, selected based on a thorough
study of ontological and lexical resources. Our translation targets CLIB states
(i.e., fluents) and actions. The resulting ALM library contains the
descriptions of 123 action classes grouped into 43 reusable modules that are
organized into a hierarchy. It is made available online and of interest to
researchers in the action language, answer-set programming, and natural
language understanding communities. We believe that our translation has two
main advantages over its CLIB counterpart: (i) it specifies axioms about
actions in a more elaboration tolerant and readable way, and (ii) it can be
seamlessly integrated with ASP reasoning algorithms (e.g., for planning and
postdiction). In contrast, axioms are described in CLIB using STRIPS-like
operators, and CLIB's inference engine cannot handle planning nor postdiction.
Under consideration for publication in TPLP.
</summary>
    <author>
      <name>Daniela Inclezan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,
  LaTeX, 3 figures (2 of which in PDF format)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02082v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02082v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02076v1</id>
    <updated>2016-08-06T07:16:31Z</updated>
    <published>2016-08-06T07:16:31Z</published>
    <title>Bi-directional Attention with Agreement for Dependency Parsing</title>
    <summary>  We develop a novel bi-directional attention model for dependency parsing,
which learns to agree on headword predictions from the forward and backward
parsing directions. The parsing procedure for each direction is formulated as
sequentially querying the memory component that stores continuous headword
embeddings. The proposed parser makes use of soft headword embeddings, allowing
the model to implicitly capture high-order parsing history without dramatically
increasing the computational complexity. We conduct experiments on English,
Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that
the proposed model achieves state-of-the-art unlabeled attachment scores on 7
languages.
</summary>
    <author>
      <name>Hao Cheng</name>
    </author>
    <author>
      <name>Hao Fang</name>
    </author>
    <author>
      <name>Xiaodong He</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Li Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.02076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01987v1</id>
    <updated>2016-08-05T19:55:57Z</updated>
    <published>2016-08-05T19:55:57Z</published>
    <title>Human collective intelligence as distributed Bayesian inference</title>
    <summary>  Collective intelligence is believed to underly the remarkable success of
human society. The formation of accurate shared beliefs is one of the key
components of human collective intelligence. How are accurate shared beliefs
formed in groups of fallible individuals? Answering this question requires a
multiscale analysis. We must understand both the individual decision mechanisms
people use, and the properties and dynamics of those mechanisms in the
aggregate. As of yet, mathematical tools for such an approach have been
lacking. To address this gap, we introduce a new analytical framework: We
propose that groups arrive at accurate shared beliefs via distributed Bayesian
inference. Distributed inference occurs through information processing at the
individual level, and yields rational belief formation at the group level. We
instantiate this framework in a new model of human social decision-making,
which we validate using a dataset we collected of over 50,000 users of an
online social trading platform where investors mimic each others' trades using
real money in foreign exchange and other asset markets. We find that in this
setting people use a decision mechanism in which popularity is treated as a
prior distribution for which decisions are best to make. This mechanism is
boundedly rational at the individual level, but we prove that in the aggregate
implements a type of approximate "Thompson sampling"---a well-known and highly
effective single-agent Bayesian machine learning algorithm for sequential
decision-making. The perspective of distributed Bayesian inference therefore
reveals how collective rationality emerges from the boundedly rational decision
mechanisms people use.
</summary>
    <author>
      <name>Peter M. Krafft</name>
    </author>
    <author>
      <name>Julia Zheng</name>
    </author>
    <author>
      <name>Wei Pan</name>
    </author>
    <author>
      <name>Nicolás Della Penna</name>
    </author>
    <author>
      <name>Yaniv Altshuler</name>
    </author>
    <author>
      <name>Erez Shmueli</name>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
    </author>
    <author>
      <name>Alex Pentland</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01961v1</id>
    <updated>2016-08-05T18:14:19Z</updated>
    <published>2016-08-05T18:14:19Z</published>
    <title>De-Conflated Semantic Representations</title>
    <summary>  One major deficiency of most semantic representation techniques is that they
usually model a word type as a single point in the semantic space, hence
conflating all the meanings that the word can have. Addressing this issue by
learning distinct representations for individual meanings of words has been the
subject of several research studies in the past few years. However, the
generated sense representations are either not linked to any sense inventory or
are unreliable for infrequent word senses. We propose a technique that tackles
these problems by de-conflating the representations of words based on the deep
knowledge it derives from a semantic network. Our approach provides multiple
advantages in comparison to the past work, including its high coverage and the
ability to generate accurate representations even for infrequent word senses.
We carry out evaluations on six datasets across two semantic similarity tasks
and report state-of-the-art results on most of them.
</summary>
    <author>
      <name>Mohammad Taher Pilehvar</name>
    </author>
    <author>
      <name>Nigel Collier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01946v1</id>
    <updated>2016-08-05T17:33:23Z</updated>
    <published>2016-08-05T17:33:23Z</published>
    <title>Iterative Learning of Answer Set Programs from Context Dependent
  Examples</title>
    <summary>  In recent years, several frameworks and systems have been proposed that
extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP)
paradigm. In ILP, examples must all be explained by a hypothesis together with
a given background knowledge. In existing systems, the background knowledge is
the same for all examples; however, examples may be context-dependent. This
means that some examples should be explained in the context of some
information, whereas others should be explained in different contexts. In this
paper, we capture this notion and present a context-dependent extension of the
Learning from Ordered Answer Sets framework. In this extension, contexts can be
used to further structure the background knowledge. We then propose a new
iterative algorithm, ILASP2i, which exploits this feature to scale up the
existing ILASP2 system to learning tasks with large numbers of examples. We
demonstrate the gain in scalability by applying both algorithms to various
learning tasks. Our results show that, compared to ILASP2, the newly proposed
ILASP2i system can be two orders of magnitude faster and use two orders of
magnitude less memory, whilst preserving the same average accuracy. This paper
is under consideration for acceptance in TPLP.
</summary>
    <author>
      <name>Mark Law</name>
    </author>
    <author>
      <name>Alessandra Russo</name>
    </author>
    <author>
      <name>Krysia Broda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 22 pages,
  LaTeX, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01884v1</id>
    <updated>2016-08-05T13:39:08Z</updated>
    <published>2016-08-05T13:39:08Z</published>
    <title>Winograd Schemas and Machine Translation</title>
    <summary>  A Winograd schema is a pair of sentences that differ in a single word and
that contain an ambiguous pronoun whose referent is different in the two
sentences and requires the use of commonsense knowledge or world knowledge to
disambiguate. This paper discusses how Winograd schemas and other sentence
pairs could be used as challenges for machine translation using distinctions
between pronouns, such as gender, that appear in the target language but not in
the source.
</summary>
    <author>
      <name>Ernest Davis</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01856v1</id>
    <updated>2016-08-05T12:26:22Z</updated>
    <published>2016-08-05T12:26:22Z</published>
    <title>The Power of Non-Ground Rules in Answer Set Programming</title>
    <summary>  Answer set programming (ASP) is a well-established logic programming language
that offers an intuitive, declarative syntax for problem solving. In its
traditional application, a fixed ASP program for a given problem is designed
and the actual instance of the problem is fed into the program as a set of
facts. This approach typically results in programs with comparably short and
simple rules. However, as is known from complexity analysis, such an approach
limits the expressive power of ASP; in fact, an entire NP-check can be encoded
into a single large rule body of bounded arity that performs both a guess and a
check within the same rule.
  Here, we propose a novel paradigm for encoding hard problems in ASP by making
explicit use of large rules which depend on the actual instance of the problem.
We illustrate how this new encoding paradigm can be used, providing examples of
problems from the first, second, and even third level of the polynomial
hierarchy. As state-of-the-art solvers are tuned towards short rules, rule
decomposition is a key technique in the practical realization of our approach.
We also provide some preliminary benchmarks which indicate that giving up the
convenient way of specifying a fixed program can lead to a significant
speed-up.
  This paper is under consideration for acceptance into TPLP.
</summary>
    <author>
      <name>Manuel Bichler</name>
    </author>
    <author>
      <name>Michael Morak</name>
    </author>
    <author>
      <name>Stefan Woltran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,
  LaTeX, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01835v3</id>
    <updated>2016-08-15T05:25:55Z</updated>
    <published>2016-08-05T11:18:12Z</published>
    <title>Stable-Unstable Semantics: Beyond NP with Normal Logic Programs</title>
    <summary>  Standard answer set programming (ASP) targets at solving search problems from
the first level of the polynomial time hierarchy (PH). Tackling search problems
beyond NP using ASP is less straightforward. The class of disjunctive logic
programs offers the most prominent way of reaching the second level of the PH,
but encoding respective hard problems as disjunctive programs typically
requires sophisticated techniques such as saturation or meta-interpretation.
The application of such techniques easily leads to encodings that are
inaccessible to non-experts. Furthermore, while disjunctive ASP solvers often
rely on calls to a (co-)NP oracle, it may be difficult to detect from the input
program where the oracle is being accessed. In other formalisms, such as
Quantified Boolean Formulas (QBFs), the interface to the underlying oracle is
more transparent as it is explicitly recorded in the quantifier prefix of a
formula. On the other hand, ASP has advantages over QBFs from the modeling
perspective. The rich high-level languages such as ASP-Core-2 offer a wide
variety of primitives that enable concise and natural encodings of search
problems. In this paper, we present a novel logic programming--based modeling
paradigm that combines the best features of ASP and QBFs. We develop so-called
combined logic programs in which oracles are directly cast as (normal) logic
programs themselves. Recursive incarnations of this construction enable logic
programming on arbitrarily high levels of the PH. We develop a proof-of-concept
implementation for our new paradigm.
  This paper is under consideration for acceptance in TPLP.
</summary>
    <author>
      <name>Bart Bogaerts</name>
    </author>
    <author>
      <name>Tomi Janhunen</name>
    </author>
    <author>
      <name>Shahab Tasharrofi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 16 pages,
  LaTeX, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01835v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01835v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.6; F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01668v1</id>
    <updated>2016-08-05T05:51:24Z</updated>
    <published>2016-08-05T05:51:24Z</published>
    <title>Self-Organising Maps in Computer Security</title>
    <summary>  Some argue that biologically inspired algorithms are the future of solving
difficult problems in computer science. Others strongly believe that the future
lies in the exploration of mathematical foundations of problems at hand. The
field of computer security tends to accept the latter view as a more
appropriate approach due to its more workable validation and verification
possibilities. The lack of rigorous scientific practices prevalent in
biologically inspired security research does not aid in presenting bio-inspired
security approaches as a viable way of dealing with complex security problems.
This chapter introduces a biologically inspired algorithm, called the Self
Organising Map (SOM), that was developed by Teuvo Kohonen in 1981. Since the
algorithm's inception it has been scrutinised by the scientific community and
analysed in more than 4000 research papers, many of which dealt with various
computer security issues, from anomaly detection, analysis of executables all
the way to wireless network monitoring. In this chapter a review of security
related SOM research undertaken in the past is presented and analysed. The
algorithm's biological analogies are detailed and the author's view on the
future possibilities of this successful bio-inspired approach are given. The
SOM algorithm's close relation to a number of vital functions of the human
brain and the emergence of multi-core computer architectures are the two main
reasons behind our assumption that the future of the SOM algorithm and its
variations is promising, notably in the field of computer security.
</summary>
    <author>
      <name>Jan Feyereisl</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">pp. 1-30, Computer Security: Intrusion, Detection and Prevention,
  2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01716v1</id>
    <updated>2016-08-04T23:18:47Z</updated>
    <published>2016-08-04T23:18:47Z</published>
    <title>A Polynomial-Time Deterministic Approach to the Traveling Salesperson
  Problem</title>
    <summary>  We propose a new polynomial-time deterministic algorithm that produces an
approximated solution for the traveling salesperson problem. The proposed
algorithm ranks cities based on their priorities calculated using a power
function of means and standard deviations of their distances from other cities
and then connects the cities to their neighbors in the order of their
priorities. When connecting a city, a neighbor is selected based on their
neighbors' priorities calculated as another power function that additionally
includes their distance from the focal city to be connected. This repeats until
all the cities are connected into a single loop. The time complexity of the
proposed algorithm is $O(n^2)$, where $n$ is the number of cities. Numerical
evaluation shows that the proposed algorithm produces shorter tours with less
time complexity than other conventional tour construction heuristics.
</summary>
    <author>
      <name>Ali Jazayeri</name>
    </author>
    <author>
      <name>Hiroki Sayama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01611v1</id>
    <updated>2016-08-04T16:56:31Z</updated>
    <published>2016-08-04T16:56:31Z</published>
    <title>Deploying learning materials to game content for serious education game
  development: A case study</title>
    <summary>  The ultimate goals of serious education games (SEG) are to facilitate
learning and maximizing enjoyment during playing SEGs. In SEG development,
there are normally two spaces to be taken into account: knowledge space
regarding learning materials and content space regarding games to be used to
convey learning materials. How to deploy the learning materials seamlessly and
effectively into game content becomes one of the most challenging problems in
SEG development. Unlike previous work where experts in education have to be
used heavily, we proposed a novel approach that works toward minimizing the
efforts of education experts in mapping learning materials to content space.
For a proof-of-concept, we apply the proposed approach in developing an SEG
game, named \emph{Chem Dungeon}, as a case study in order to demonstrate the
effectiveness of our proposed approach. This SEG game has been tested with a
number of users, and the user survey suggests our method works reasonably well.
</summary>
    <author>
      <name>Harits Ar Rosyid</name>
    </author>
    <author>
      <name>Matt Palmerlee</name>
    </author>
    <author>
      <name>Ke Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01604v1</id>
    <updated>2016-08-04T16:38:52Z</updated>
    <published>2016-08-04T16:38:52Z</published>
    <title>Query Answering in Resource-Based Answer Set Semantics</title>
    <summary>  In recent work we defined resource-based answer set semantics, which is an
extension to answer set semantics stemming from the study of its relationship
with linear logic. In fact, the name of the new semantics comes from the fact
that in the linear-logic formulation every literal (including negative ones)
were considered as a resource. In this paper, we propose a query-answering
procedure reminiscent of Prolog for answer set programs under this extended
semantics as an extension of XSB-resolution for logic programs with negation.
We prove formal properties of the proposed procedure.
  Under consideration for acceptance in TPLP.
</summary>
    <author>
      <name>Stefania Costantini</name>
    </author>
    <author>
      <name>Andrea Formisano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,
  LaTeX, 3 PDF figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01603v1</id>
    <updated>2016-08-04T16:32:57Z</updated>
    <published>2016-08-04T16:32:57Z</published>
    <title>Stable Models for Infinitary Formulas with Extensional Atoms</title>
    <summary>  The definition of stable models for propositional formulas with infinite
conjunctions and disjunctions can be used to describe the semantics of answer
set programming languages. In this note, we enhance that definition by
introducing a distinction between intensional and extensional atoms. The
symmetric splitting theorem for first-order formulas is then extended to
infinitary formulas and used to reason about infinitary definitions. This note
is under consideration for publication in Theory and Practice of Logic
Programming.
</summary>
    <author>
      <name>Amelia Harrison</name>
    </author>
    <author>
      <name>Vladimir Lifschitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,
  LaTeX, 3 PDF figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01404v1</id>
    <updated>2016-08-04T00:36:57Z</updated>
    <published>2016-08-04T00:36:57Z</published>
    <title>Quantifier Scope in Categorical Compositional Distributional Semantics</title>
    <summary>  In previous work with J. Hedges, we formalised a generalised quantifiers
theory of natural language in categorical compositional distributional
semantics with the help of bialgebras. In this paper, we show how quantifier
scope ambiguity can be represented in that setting and how this representation
can be generalised to branching quantifiers.
</summary>
    <author>
      <name>Mehrnoosh Sadrzadeh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Queen Mary University of London</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.221.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.221.6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SLPCS 2016, arXiv:1608.01018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 221, 2016, pp. 49-57</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.01404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01402v1</id>
    <updated>2016-08-04T00:36:21Z</updated>
    <published>2016-08-04T00:36:21Z</published>
    <title>Interacting Conceptual Spaces</title>
    <summary>  We propose applying the categorical compositional scheme of [6] to conceptual
space models of cognition. In order to do this we introduce the category of
convex relations as a new setting for categorical compositional semantics,
emphasizing the convex structure important to conceptual space applications. We
show how conceptual spaces for composite types such as adjectives and verbs can
be constructed. We illustrate this new model on detailed examples.
</summary>
    <author>
      <name>Josef Bolt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univesity of Oxford</arxiv:affiliation>
    </author>
    <author>
      <name>Bob Coecke</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univesity of Oxford</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrizio Genovese</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univesity of Oxford</arxiv:affiliation>
    </author>
    <author>
      <name>Martha Lewis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univesity of Oxford</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Marsden</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univesity of Oxford</arxiv:affiliation>
    </author>
    <author>
      <name>Robin Piedeleu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univesity of Oxford</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.221.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.221.2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SLPCS 2016, arXiv:1608.01018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 221, 2016, pp. 11-19</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.01402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01338v2</id>
    <updated>2016-08-05T19:21:19Z</updated>
    <published>2016-08-03T20:26:20Z</published>
    <title>Paraconsistency and Word Puzzles</title>
    <summary>  Word puzzles and the problem of their representations in logic languages have
received considerable attention in the last decade (Ponnuru et al. 2004;
Shapiro 2011; Baral and Dzifcak 2012; Schwitter 2013). Of special interest is
the problem of generating such representations directly from natural language
(NL) or controlled natural language (CNL). An interesting variation of this
problem, and to the best of our knowledge, scarcely explored variation in this
context, is when the input information is inconsistent. In such situations, the
existing encodings of word puzzles produce inconsistent representations and
break down. In this paper, we bring the well-known type of paraconsistent
logics, called Annotated Predicate Calculus (APC) (Kifer and Lozinskii 1992),
to bear on the problem. We introduce a new kind of non-monotonic semantics for
APC, called consistency preferred stable models and argue that it makes APC
into a suitable platform for dealing with inconsistency in word puzzles and,
more generally, in NL sentences. We also devise a number of general principles
to help the user choose among the different representations of NL sentences,
which might seem equivalent but, in fact, behave differently when inconsistent
information is taken into account. These principles can be incorporated into
existing CNL translators, such as Attempto Controlled English (ACE) (Fuchs et
al. 2008) and PENG Light (White and Schwitter 2009). Finally, we show that APC
with the consistency preferred stable model semantics can be equivalently
embedded in ASP with preferences over stable models, and we use this embedding
to implement this version of APC in Clingo (Gebser et al. 2011) and its Asprin
add-on (Brewka et al. 2015).
</summary>
    <author>
      <name>Tiantian Gao</name>
    </author>
    <author>
      <name>Paul Fodor</name>
    </author>
    <author>
      <name>Michael Kifer</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01338v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01338v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01302v1</id>
    <updated>2016-08-03T19:50:39Z</updated>
    <published>2016-08-03T19:50:39Z</published>
    <title>Learning to Rank for Synthesizing Planning Heuristics</title>
    <summary>  We investigate learning heuristics for domain-specific planning. Prior work
framed learning a heuristic as an ordinary regression problem. However, in a
greedy best-first search, the ordering of states induced by a heuristic is more
indicative of the resulting planner's performance than mean squared error.
Thus, we instead frame learning a heuristic as a learning to rank problem which
we solve using a RankSVM formulation. Additionally, we introduce new methods
for computing features that capture temporal interactions in an approximate
plan. Our experiments on recent International Planning Competition problems
show that the RankSVM learned heuristics outperform both the original
heuristics and heuristics learned through ordinary regression.
</summary>
    <author>
      <name>Caelan Reed Garrett</name>
    </author>
    <author>
      <name>Leslie Pack Kaelbling</name>
    </author>
    <author>
      <name>Tomas Lozano-Perez</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Joint Conference on Artificial Intelligence (IJCAI)
  2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.01302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01212v1</id>
    <updated>2016-08-03T14:58:53Z</updated>
    <published>2016-08-03T14:58:53Z</published>
    <title>A Novel Approach for Data-Driven Automatic Site Recommendation and
  Selection</title>
    <summary>  This paper presents a novel, generic, and automatic method for data-driven
site selection. Site selection is one of the most crucial and important
decisions made by any company. Such a decision depends on various factors of
sites, including socio-economic, geographical, ecological, as well as specific
requirements of companies. The existing approaches for site selection (commonly
used by economists) are manual, subjective, and not scalable, especially to Big
Data. The presented method for site selection is robust, efficient, scalable,
and is capable of handling challenges emerging in Big Data. To assess the
effectiveness of the presented method, it is evaluated on real data (collected
from Federal Statistical Office of Germany) of around 200 influencing factors
which are considered by economists for site selection of Supermarkets in
Germany (Lidl, EDEKA, and NP). Evaluation results show that there is a big
overlap (86.4 \%) between the sites of existing supermarkets and the sites
recommended by the presented method. In addition, the method also recommends
many sites (328) for supermarket where a store should be opened.
</summary>
    <author>
      <name>Sebastian Baumbach</name>
    </author>
    <author>
      <name>Frank Wittich</name>
    </author>
    <author>
      <name>Florian Sachs</name>
    </author>
    <author>
      <name>Sheraz Ahmed</name>
    </author>
    <author>
      <name>Andreas Dengel</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01127v1</id>
    <updated>2016-08-03T09:25:35Z</updated>
    <published>2016-08-03T09:25:35Z</published>
    <title>Autonomous Grounding of Visual Field Experience through Sensorimotor
  Prediction</title>
    <summary>  In a developmental framework, autonomous robots need to explore the world and
learn how to interact with it. Without an a priori model of the system, this
opens the challenging problem of having robots master their interface with the
world: how to perceive their environment using their sensors, and how to act in
it using their motors. The sensorimotor approach of perception claims that a
naive agent can learn to master this interface by capturing regularities in the
way its actions transform its sensory inputs. In this paper, we apply such an
approach to the discovery and mastery of the visual field associated with a
visual sensor. A computational model is formalized and applied to a simulated
system to illustrate the approach.
</summary>
    <author>
      <name>Alban Laflaquière</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, ICDL-Epirob 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02441v1</id>
    <updated>2016-08-03T09:05:32Z</updated>
    <published>2016-08-03T09:05:32Z</published>
    <title>Proceedings of the Second Summer School on Argumentation: Computational
  and Linguistic Perspectives (SSA'16)</title>
    <summary>  This volume contains the thesis abstracts presented at the Second Summer
School on Argumentation: Computational and Linguistic Perspectives (SSA'2016)
held on September 8-12 in Potsdam, Germany.
</summary>
    <author>
      <name>Sarah A. Gaggl</name>
    </author>
    <author>
      <name>Matthias Thimm</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01093v1</id>
    <updated>2016-08-03T07:23:48Z</updated>
    <published>2016-08-03T07:23:48Z</published>
    <title>Generation of Near-Optimal Solutions Using ILP-Guided Sampling</title>
    <summary>  Our interest in this paper is in optimisation problems that are intractable
to solve by direct numerical optimisation, but nevertheless have significant
amounts of relevant domain-specific knowledge. The category of heuristic search
techniques known as estimation of distribution algorithms (EDAs) seek to
incrementally sample from probability distributions in which optimal (or
near-optimal) solutions have increasingly higher probabilities. Can we use
domain knowledge to assist the estimation of these distributions? To answer
this in the affirmative, we need: (a) a general-purpose technique for the
incorporation of domain knowledge when constructing models for optimal values;
and (b) a way of using these models to generate new data samples. Here we
investigate a combination of the use of Inductive Logic Programming (ILP) for
(a), and standard logic-programming machinery to generate new samples for (b).
Specifically, on each iteration of distribution estimation, an ILP engine is
used to construct a model for good solutions. The resulting theory is then used
to guide the generation of new data instances, which are now restricted to
those derivable using the ILP model in conjunction with the background
knowledge). We demonstrate the approach on two optimisation problems
(predicting optimal depth-of-win for the KRK endgame, and job-shop scheduling).
Our results are promising: (a) On each iteration of distribution estimation,
samples obtained with an ILP theory have a substantially greater proportion of
good solutions than samples without a theory; and (b) On termination of
distribution estimation, samples obtained with an ILP theory contain more
near-optimal samples than samples without a theory. Taken together, these
results suggest that the use of ILP-constructed theories could be a useful
technique for incorporating complex domain-knowledge into estimation
distribution procedures.
</summary>
    <author>
      <name>Ashwin Srinivasan</name>
    </author>
    <author>
      <name>Gautam Shroff</name>
    </author>
    <author>
      <name>Lovekesh Vig</name>
    </author>
    <author>
      <name>Sarmimala Saikia</name>
    </author>
    <author>
      <name>Puneet Agarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01039v1</id>
    <updated>2016-08-03T01:13:54Z</updated>
    <published>2016-08-03T01:13:54Z</published>
    <title>Empirical Evaluation of Real World Tournaments</title>
    <summary>  Computational Social Choice (ComSoc) is a rapidly developing field at the
intersection of computer science, economics, social choice, and political
science. The study of tournaments is fundamental to ComSoc and many results
have been published about tournament solution sets and reasoning in
tournaments. Theoretical results in ComSoc tend to be worst case and tell us
little about performance in practice. To this end we detail some experiments on
tournaments using real wold data from soccer and tennis. We make three main
contributions to the understanding of tournaments using real world data from
English Premier League, the German Bundesliga, and the ATP World Tour: (1) we
find that the NP-hard question of finding a seeding for which a given team can
win a tournament is easily solvable in real world instances, (2) using detailed
and principled methodology from statistical physics we show that our real world
data obeys a log-normal distribution; and (3) leveraging our log-normal
distribution result and using robust statistical methods, we show that the
popular Condorcet Random (CR) tournament model does not generate realistic
tournament data.
</summary>
    <author>
      <name>Nicholas Mattei</name>
    </author>
    <author>
      <name>Toby Walsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A80, 91B74" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; I.2; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01018v1</id>
    <updated>2016-08-02T22:28:45Z</updated>
    <published>2016-08-02T22:28:45Z</published>
    <title>Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection
  of NLP, Physics and Cognitive Science</title>
    <summary>  This volume contains the Proceedings of the 2016 Workshop on Semantic Spaces
at the Intersection of NLP, Physics and Cognitive Science (SLPCS 2016), which
was held on the 11th of June at the University of Strathclyde, Glasgow, and was
co-located with Quantum Physics and Logic (QPL 2016). Exploiting the common
ground provided by the concept of a vector space, the workshop brought together
researchers working at the intersection of Natural Language Processing (NLP),
cognitive science, and physics, offering them an appropriate forum for
presenting their uniquely motivated work and ideas. The interplay between these
three disciplines inspired theoretically motivated approaches to the
understanding of how word meanings interact with each other in sentences and
discourse, how diagrammatic reasoning depicts and simplifies this interaction,
how language models are determined by input from the world, and how word and
sentence meanings interact logically. This first edition of the workshop
consisted of three invited talks from distinguished speakers (Hans Briegel,
Peter G\"ardenfors, Dominic Widdows) and eight presentations of selected
contributed papers. Each submission was refereed by at least three members of
the Programme Committee, who delivered detailed and insightful comments and
suggestions.
</summary>
    <author>
      <name>Dimitrios Kartsaklis</name>
    </author>
    <author>
      <name>Martha Lewis</name>
    </author>
    <author>
      <name>Laura Rimell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.221</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.221" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 221, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.01018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00876v1</id>
    <updated>2016-08-02T15:48:58Z</updated>
    <published>2016-08-02T15:48:58Z</published>
    <title>Relational Similarity Machines</title>
    <summary>  This paper proposes Relational Similarity Machines (RSM): a fast, accurate,
and flexible relational learning framework for supervised and semi-supervised
learning tasks. Despite the importance of relational learning, most existing
methods are hard to adapt to different settings, due to issues with efficiency,
scalability, accuracy, and flexibility for handling a wide variety of
classification problems, data, constraints, and tasks. For instance, many
existing methods perform poorly for multi-class classification problems, graphs
that are sparsely labeled or network data with low relational autocorrelation.
In contrast, the proposed relational learning framework is designed to be (i)
fast for learning and inference at real-time interactive rates, and (ii)
flexible for a variety of learning settings (multi-class problems), constraints
(few labeled instances), and application domains. The experiments demonstrate
the effectiveness of RSM for a variety of tasks and data.
</summary>
    <author>
      <name>Ryan A. Rossi</name>
    </author>
    <author>
      <name>Rong Zhou</name>
    </author>
    <author>
      <name>Nesreen K. Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MLG16</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00810v1</id>
    <updated>2016-08-02T13:22:49Z</updated>
    <published>2016-08-02T13:22:49Z</published>
    <title>Directed expected utility networks</title>
    <summary>  A variety of statistical graphical models have been defined to represent the
conditional independences underlying a random vector of interest. Similarly,
many different graphs embedding various types of preferential independences, as
for example conditional utility independence and generalized additive
independence, have more recently started to appear. In this paper we define a
new graphical model, called a directed expected utility network, whose edges
depict both probabilistic and utility conditional independences. These embed a
very flexible and general class of utility models, much larger than those
usually conceived in standard influence diagrams. Our graphical representation,
and various transformations of the original graph into a tree structure, are
then used to guide fast routines for the computation of a decision problem's
expected utilities. We show that our routines generalize those usually utilized
in standard influence diagrams' evaluations under much more restrictive
conditions. Our algorithms are illustrated using a class of linear regression
models and a family of linear polynomial utility functions.
</summary>
    <author>
      <name>Manuele Leonelli</name>
    </author>
    <author>
      <name>Jim Q. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00737v1</id>
    <updated>2016-08-02T08:57:14Z</updated>
    <published>2016-08-02T08:57:14Z</published>
    <title>Context Discovery for Model Learning in Partially Observable
  Environments</title>
    <summary>  The ability to learn a model is essential for the success of autonomous
agents. Unfortunately, learning a model is difficult in partially observable
environments, where latent environmental factors influence what the agent
observes. In the absence of a supervisory training signal, autonomous agents
therefore require a mechanism to autonomously discover these environmental
factors, or sensorimotor contexts.
  This paper presents a method to discover sensorimotor contexts in partially
observable environments, by constructing a hierarchical transition model. The
method is evaluated in a simulation experiment, in which a robot learns that
different rooms are characterized by different objects that are found in them.
</summary>
    <author>
      <name>Nikolas J. Hemion</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6th Joint IEEE International Conference on Development and Learning
  and on Epigenetic Robotics (IEEE ICDL-EPIROB 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00730v1</id>
    <updated>2016-08-02T08:36:08Z</updated>
    <published>2016-08-02T08:36:08Z</published>
    <title>Combining Answer Set Programming and Domain Heuristics for Solving Hard
  Industrial Problems (Application Paper)</title>
    <summary>  Answer Set Programming (ASP) is a popular logic programming paradigm that has
been applied for solving a variety of complex problems. Among the most
challenging real-world applications of ASP are two industrial problems defined
by Siemens: the Partner Units Problem (PUP) and the Combined Configuration
Problem (CCP). The hardest instances of PUP and CCP are out of reach for
state-of-the-art ASP solvers. Experiments show that the performance of ASP
solvers could be significantly improved by embedding domain-specific
heuristics, but a proper effective integration of such criteria in
off-the-shelf ASP implementations is not obvious. In this paper the combination
of ASP and domain-specific heuristics is studied with the goal of effectively
solving real-world problem instances of PUP and CCP. As a byproduct of this
activity, the ASP solver WASP was extended with an interface that eases
embedding new external heuristics in the solver. The evaluation shows that our
domain-heuristic-driven ASP solver finds solutions for all the real-world
instances of PUP and CCP ever provided by Siemens. This paper is under
consideration for acceptance in TPLP.
</summary>
    <author>
      <name>Carmine Dodaro</name>
    </author>
    <author>
      <name>Philip Gasteiger</name>
    </author>
    <author>
      <name>Nicola Leone</name>
    </author>
    <author>
      <name>Benjamin Musitsch</name>
    </author>
    <author>
      <name>Francesco Ricca</name>
    </author>
    <author>
      <name>Kostyantyn Shchekotykhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,
  LaTeX, 3 PDF figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00700v2</id>
    <updated>2016-08-23T05:15:03Z</updated>
    <published>2016-08-02T05:50:11Z</published>
    <title>A Survey of Visual Analysis of Human Motion and Its Applications</title>
    <summary>  This paper summarizes the recent progress in human motion analysis and its
applications. In the beginning, we reviewed the motion capture systems and the
representation model of human's motion data. Next, we sketched the advanced
human motion data processing technologies, including motion data filtering,
temporal alignment, and segmentation. The following parts overview the
state-of-the-art approaches of action recognition and dynamics measuring since
these two are the most active research areas in human motion analysis. The last
part discusses some emerging applications of the human motion analysis in
healthcare, human robot interaction, security surveillance, virtual reality and
animation. The promising research topics of human motion analysis in the future
is also summarized in the last part.
</summary>
    <author>
      <name>Qifei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, conference paper in VCIP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00700v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00700v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00667v1</id>
    <updated>2016-08-02T01:30:25Z</updated>
    <published>2016-08-02T01:30:25Z</published>
    <title>Can Active Learning Experience Be Transferred?</title>
    <summary>  Active learning is an important machine learning problem in reducing the
human labeling effort. Current active learning strategies are designed from
human knowledge, and are applied on each dataset in an immutable manner. In
other words, experience about the usefulness of strategies cannot be updated
and transferred to improve active learning on other datasets. This paper
initiates a pioneering study on whether active learning experience can be
transferred. We first propose a novel active learning model that linearly
aggregates existing strategies. The linear weights can then be used to
represent the active learning experience. We equip the model with the popular
linear upper- confidence-bound (LinUCB) algorithm for contextual bandit to
update the weights. Finally, we extend our model to transfer the experience
across datasets with the technique of biased regularization. Empirical studies
demonstrate that the learned experience not only is competitive with existing
strategies on most single datasets, but also can be transferred across datasets
to improve the performance on future learning tasks.
</summary>
    <author>
      <name>Hong-Min Chu</name>
    </author>
    <author>
      <name>Hsuan-Tien Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figs, 4 tables, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00655v1</id>
    <updated>2016-08-02T00:36:49Z</updated>
    <published>2016-08-02T00:36:49Z</published>
    <title>A Web-based Tool for Identifying Strategic Intervention Points in
  Complex Systems</title>
    <summary>  Steering a complex system towards a desired outcome is a challenging task.
The lack of clarity on the system's exact architecture and the often scarce
scientific data upon which to base the operationalisation of the dynamic rules
that underpin the interactions between participant entities are two
contributing factors. We describe an analytical approach that builds on Fuzzy
Cognitive Mapping (FCM) to address the latter and represent the system as a
complex network. We apply results from network controllability to address the
former and determine minimal control configurations - subsets of factors, or
system levers, which comprise points for strategic intervention in steering the
system. We have implemented the combination of these techniques in an
analytical tool that runs in the browser, and generates all minimal control
configurations of a complex network. We demonstrate our approach by reporting
on our experience of working alongside industrial, local-government, and NGO
stakeholders in the Humber region, UK. Our results are applied to the
decision-making process involved in the transition of the region to a bio-based
economy.
</summary>
    <author>
      <name>Sotiris Moschoyiannis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, University of Surrey, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Nicholas Elia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Extended-Content Solutions, London, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandra S. Penn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Sociology, University of Surrey, UK</arxiv:affiliation>
    </author>
    <author>
      <name>David J. B. Lloyd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Mathematics, University of Surrey, UK</arxiv:affiliation>
    </author>
    <author>
      <name>Chris Knight</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Mathematics, University of Surrey, UK</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.220.4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.220.4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings Cassting'16/SynCoP'16, arXiv:1608.00177</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 220, 2016, pp. 39-52</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1608.00655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00627v1</id>
    <updated>2016-08-01T21:53:04Z</updated>
    <published>2016-08-01T21:53:04Z</published>
    <title>Learning Transferable Policies for Monocular Reactive MAV Control</title>
    <summary>  The ability to transfer knowledge gained in previous tasks into new contexts
is one of the most important mechanisms of human learning. Despite this,
adapting autonomous behavior to be reused in partially similar settings is
still an open problem in current robotics research. In this paper, we take a
small step in this direction and propose a generic framework for learning
transferable motion policies. Our goal is to solve a learning problem in a
target domain by utilizing the training data in a different but related source
domain. We present this in the context of an autonomous MAV flight using
monocular reactive control, and demonstrate the efficacy of our proposed
approach through extensive real-world flight experiments in outdoor cluttered
environments.
</summary>
    <author>
      <name>Shreyansh Daftry</name>
    </author>
    <author>
      <name>J. Andrew Bagnell</name>
    </author>
    <author>
      <name>Martial Hebert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Experimental Robotics (ISER 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00359v1</id>
    <updated>2016-08-01T09:09:04Z</updated>
    <published>2016-08-01T09:09:04Z</published>
    <title>Discovering Latent States for Model Learning: Applying Sensorimotor
  Contingencies Theory and Predictive Processing to Model Context</title>
    <summary>  Autonomous robots need to be able to adapt to unforeseen situations and to
acquire new skills through trial and error. Reinforcement learning in principle
offers a suitable methodological framework for this kind of autonomous
learning. However current computational reinforcement learning agents mostly
learn each individual skill entirely from scratch. How can we enable artificial
agents, such as robots, to acquire some form of generic knowledge, which they
could leverage for the learning of new skills? This paper argues that, like the
brain, the cognitive system of artificial agents has to develop a world model
to support adaptive behavior and learning. Inspiration is taken from two recent
developments in the cognitive science literature: predictive processing
theories of cognition, and the sensorimotor contingencies theory of perception.
Based on these, a hypothesis is formulated about what the content of
information might be that is encoded in an internal world model, and how an
agent could autonomously acquire it. A computational model is described to
formalize this hypothesis, and is evaluated in a series of simulation
experiments.
</summary>
    <author>
      <name>Nikolas J. Hemion</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00329v2</id>
    <updated>2016-08-03T03:07:46Z</updated>
    <published>2016-08-01T06:00:22Z</published>
    <title>Keyphrase Extraction using Sequential Labeling</title>
    <summary>  Keyphrases efficiently summarize a document's content and are used in various
document processing and retrieval tasks. Several unsupervised techniques and
classifiers exist for extracting keyphrases from text documents. Most of these
methods operate at a phrase-level and rely on part-of-speech (POS) filters for
candidate phrase generation. In addition, they do not directly handle
keyphrases of varying lengths. We overcome these modeling shortcomings by
addressing keyphrase extraction as a sequential labeling task in this paper. We
explore a basic set of features commonly used in NLP tasks as well as
predictions from various unsupervised methods to train our taggers. In addition
to a more natural modeling for the keyphrase extraction problem, we show that
tagging models yield significant performance benefits over existing
state-of-the-art extraction methods.
</summary>
    <author>
      <name>Sujatha Das Gollapalli</name>
    </author>
    <author>
      <name>Xiao-li Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages including 2 pages of references, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00302v1</id>
    <updated>2016-08-01T02:34:07Z</updated>
    <published>2016-08-01T02:34:07Z</published>
    <title>Formulating Semantics of Probabilistic Argumentation by Characterizing
  Subgraphs: Theory and Empirical Results</title>
    <summary>  In existing literature, while approximate approaches based on Monte-Carlo
simulation technique have been proposed to compute the semantics of
probabilistic argumentation, how to improve the efficiency of computation
without using simulation technique is still an open problem. In this paper, we
address this problem from the following two perspectives. First, conceptually,
we define specific properties to characterize the subgraphs of a PrAG with
respect to a given extension, such that the probability of a set of arguments E
being an extension can be defined in terms of these properties, without (or
with less) construction of subgraphs. Second, computationally, we take
preferred semantics as an example, and develop algorithms to evaluate the
efficiency of our approach. The results show that our approach not only
dramatically decreases the time for computing p(E^\sigma), but also has an
attractive property, which is contrary to that of existing approaches: the
denser the edges of a PrAG are or the bigger the size of a given extension E
is, the more efficient our approach computes p(E^\sigma). Meanwhile, it is
shown that under complete and preferred semantics, the problems of determining
p(E^\sigma) are fixed-parameter tractable.
</summary>
    <author>
      <name>Beishui Liao</name>
    </author>
    <author>
      <name>Kang Xu</name>
    </author>
    <author>
      <name>Huaxin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First version submitted to JLC on Feb 12, 2016. This is a revised
  version, submitted to JLC on Jul 28, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00139v1</id>
    <updated>2016-07-30T16:14:16Z</updated>
    <published>2016-07-30T16:14:16Z</published>
    <title>A Linear Algebraic Approach to Datalog Evaluation</title>
    <summary>  In this paper, we propose a fundamentally new approach to Datalog evaluation.
Given a linear Datalog program DB written using N constants and binary
predicates, we first translate if-and-only-if completions of clauses in DB into
a set Eq(DB) of matrix equations with a non-linear operation where relations in
M_DB, the least Herbrand model of DB, are encoded as adjacency matrices. We
then translate Eq(DB) into another, but purely linear matrix equations
tilde_Eq(DB). It is proved that the least solution of tilde_Eq(DB) in the sense
of matrix ordering is converted to the least solution of Eq(DB) and the latter
gives M_DB as a set of adjacency matrices. Hence computing the least solution
of tilde_Eq(DB) is equivalent to computing M_DB specified by DB. For a class of
tail recursive programs and for some other types of programs, our approach
achieves O(N^3) time complexity irrespective of the number of variables in a
clause since only matrix operations costing O(N^3) or less are used.
  We conducted two experiments that compute the least Herbrand models of linear
Datalog programs. The first experiment computes transitive closure of
artificial data and real network data taken from the Koblenz Network
Collection. The second one compared the proposed approach with the
state-of-the-art symbolic systems including two Prolog systems and two ASP
systems, in terms of computation time for a transitive closure program and the
same generation program. In the experiment, it is observed that our linear
algebraic approach runs 10^1 ~ 10^4 times faster than the symbolic systems when
data is not sparse.
</summary>
    <author>
      <name>Taisuke Sato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00100v1</id>
    <updated>2016-07-30T10:44:58Z</updated>
    <published>2016-07-30T10:44:58Z</published>
    <title>Online Learning of Event Definitions</title>
    <summary>  Systems for symbolic event recognition infer occurrences of events in time
using a set of event definitions in the form of first-order rules. The Event
Calculus is a temporal logic that has been used as a basis in event recognition
applications, providing among others, direct connections to machine learning,
via Inductive Logic Programming (ILP). We present an ILP system for online
learning of Event Calculus theories. To allow for a single-pass learning
strategy, we use the Hoeffding bound for evaluating clauses on a subset of the
input stream. We employ a decoupling scheme of the Event Calculus axioms during
the learning process, that allows to learn each clause in isolation. Moreover,
we use abductive-inductive logic programming techniques to handle unobserved
target predicates. We evaluate our approach on an activity recognition
application and compare it to a number of batch learning techniques. We obtain
results of comparable predicative accuracy with significant speed-ups in
training time. We also outperform hand-crafted rules and match the performance
of a sound incremental learner that can only operate on noise-free datasets.
This paper is under consideration for acceptance in TPLP.
</summary>
    <author>
      <name>Nikos Katzouris</name>
    </author>
    <author>
      <name>Alexander Artikis</name>
    </author>
    <author>
      <name>Georgios Paliouras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,
  LaTeX, 1 PDF figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.00100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08898v1</id>
    <updated>2016-07-29T19:16:08Z</updated>
    <published>2016-07-29T19:16:08Z</published>
    <title>Personalized Emphasis Framing for Persuasive Message Generation</title>
    <summary>  In this paper, we present a study on personalized emphasis framing which can
be used to tailor the content of a message to enhance its appeal to different
individuals. With this framework, we directly model content selection decisions
based on a set of psychologically-motivated domain-independent personal traits
including personality (e.g., extraversion and conscientiousness) and basic
human values (e.g., self-transcendence and hedonism). We also demonstrate how
the analysis results can be used in automated personalized content selection
for persuasive message generation.
</summary>
    <author>
      <name>Tao Ding</name>
    </author>
    <author>
      <name>Shimei Pan</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08878v1</id>
    <updated>2016-07-29T18:06:39Z</updated>
    <published>2016-07-29T18:06:39Z</published>
    <title>Identifying and Harnessing the Building Blocks of Machine Learning
  Pipelines for Sensible Initialization of a Data Science Automation Tool</title>
    <summary>  As data science continues to grow in popularity, there will be an increasing
need to make data science tools more scalable, flexible, and accessible. In
particular, automated machine learning (AutoML) systems seek to automate the
process of designing and optimizing machine learning pipelines. In this
chapter, we present a genetic programming-based AutoML system called TPOT that
optimizes a series of feature preprocessors and machine learning models with
the goal of maximizing classification accuracy on a supervised classification
problem. Further, we analyze a large database of pipelines that were previously
used to solve various supervised classification problems and identify 100 short
series of machine learning operations that appear the most frequently, which we
call the building blocks of machine learning pipelines. We harness these
building blocks to initialize TPOT with promising solutions, and find that this
sensible initialization method significantly improves TPOT's performance on one
benchmark at no cost of significantly degrading performance on the others.
Thus, sensible initialization with machine learning pipeline building blocks
shows promise for GP-based AutoML systems, and should be further refined in
future work.
</summary>
    <author>
      <name>Randal S. Olson</name>
    </author>
    <author>
      <name>Jason H. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, preprint of chapter to appear in GPTP 2016 book</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08864v2</id>
    <updated>2016-08-02T13:23:12Z</updated>
    <published>2016-07-29T16:26:54Z</published>
    <title>The DLVHEX System for Knowledge Representation: Recent Advances (System
  Description)</title>
    <summary>  The DLVHEX system implements the HEX-semantics, which integrates answer set
programming (ASP) with arbitrary external sources. Since its first release ten
years ago, significant advancements were achieved. Most importantly, the
exploitation of properties of external sources led to efficiency improvements
and flexibility enhancements of the language, and technical improvements on the
system side increased user's convenience. In this paper, we present the current
status of the system and point out the most important recent enhancements over
early versions. While existing literature focuses on theoretical aspects and
specific components, a bird's eye view of the overall system is missing. In
order to promote the system for real-world applications, we further present
applications which were already successfully realized on top of DLVHEX. This
paper is under consideration for acceptance in Theory and Practice of Logic
Programming.
</summary>
    <author>
      <name>Christoph Redl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 32nd International Conference on Logic
  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,
  LaTeX, 3 PDF figures (arXiv:1607.08864)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08864v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08864v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08723v2</id>
    <updated>2016-08-31T08:07:08Z</updated>
    <published>2016-07-29T08:33:10Z</published>
    <title>Cognitive Science in the era of Artificial Intelligence: A roadmap for
  reverse-engineering the infant language-learner</title>
    <summary>  During their first years of life, infants learn the language(s) of their
environment at an amazing speed despite large cross cultural variations in
amount and complexity of the available language input. Understanding this
simple fact still escapes current cognitive and linguistic theories. Recently,
spectacular progress in the engineering science, notably, machine learning and
wearable technology, offer the promise of revolutionizing the study of
cognitive development. Machine learning offers powerful learning algorithms
that can achieve human-like performance on many linguistic tasks. Wearable
sensors can capture vast amounts of data, which enable the reconstruction of
the sensory experience of infants in their natural environment. The project of
'reverse engineering' language development, i.e., of building an effective
system that mimics infant's achievements appears therefore to be within reach.
  Here, we analyze the conditions under which such a project can contribute to
our scientific understanding of early language development. We argue that
instead of defining a sub-problem or simplifying the data, computational models
should address the full complexity of the learning situation, and take as input
the raw sensory signals available to infants. This implies that (1) accessible
but privacy-preserving repositories of home data be setup and widely shared,
and (2) models be evaluated at different linguistic levels through a benchmark
of psycholinguist tests that can be passed by machines and humans alike, (3)
linguistically and psychologically plausible learning architectures be scaled
up to real data using probabilistic/optimization principles from machine
learning. We discuss the feasibility of this approach and present preliminary
results.
</summary>
    <author>
      <name>Emmanuel Dupoux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures, 3</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08723v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08723v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08695v1</id>
    <updated>2016-07-29T06:35:14Z</updated>
    <published>2016-07-29T06:35:14Z</published>
    <title>Semi-supervised evidential label propagation algorithm for graph data</title>
    <summary>  In the task of community detection, there often exists some useful prior
information. In this paper, a Semi-supervised clustering approach using a new
Evidential Label Propagation strategy (SELP) is proposed to incorporate the
domain knowledge into the community detection model. The main advantage of SELP
is that it can take limited supervised knowledge to guide the detection
process. The prior information of community labels is expressed in the form of
mass functions initially. Then a new evidential label propagation rule is
adopted to propagate the labels from labeled data to unlabeled ones. The
outliers can be identified to be in a special class. The experimental results
demonstrate the effectiveness of SELP.
</summary>
    <author>
      <name>Kuang Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NPU, DRUID</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DRUID</arxiv:affiliation>
    </author>
    <author>
      <name>Quan Pan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NPU</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in The 4th International Conference on Belief Functions, Sep 2016,
  Prague, Czech Republic</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08665v1</id>
    <updated>2016-07-28T23:27:13Z</updated>
    <published>2016-07-28T23:27:13Z</published>
    <title>Introspective Perception: Learning to Predict Failures in Vision Systems</title>
    <summary>  As robots aspire for long-term autonomous operations in complex dynamic
environments, the ability to reliably take mission-critical decisions in
ambiguous situations becomes critical. This motivates the need to build systems
that have situational awareness to assess how qualified they are at that moment
to make a decision. We call this self-evaluating capability as introspection.
In this paper, we take a small step in this direction and propose a generic
framework for introspective behavior in perception systems. Our goal is to
learn a model to reliably predict failures in a given system, with respect to a
task, directly from input sensor data. We present this in the context of
vision-based autonomous MAV flight in outdoor natural environments, and show
that it effectively handles uncertain situations.
</summary>
    <author>
      <name>Shreyansh Daftry</name>
    </author>
    <author>
      <name>Sam Zeng</name>
    </author>
    <author>
      <name>J. Andrew Bagnell</name>
    </author>
    <author>
      <name>Martial Hebert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08592v1</id>
    <updated>2016-07-28T19:47:25Z</updated>
    <published>2016-07-28T19:47:25Z</published>
    <title>Modeling selectional restrictions in a relational type system</title>
    <summary>  Selectional restrictions are semantic constraints on forming certain complex
types in natural language. The paper gives an overview of modeling selectional
restrictions in a relational type system with morphological and syntactic
types. We discuss some foundations of the system and ways of formalizing
selectional restrictions.
  Keywords: type theory, selectional restrictions, syntax, morphology
</summary>
    <author>
      <name>Erkki Luuk</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08583v1</id>
    <updated>2016-07-28T19:30:04Z</updated>
    <published>2016-07-28T19:30:04Z</published>
    <title>Darknet and Deepnet Mining for Proactive Cybersecurity Threat
  Intelligence</title>
    <summary>  In this paper, we present an operational system for cyber threat intelligence
gathering from various social platforms on the Internet particularly sites on
the darknet and deepnet. We focus our attention to collecting information from
hacker forum discussions and marketplaces offering products and services
focusing on malicious hacking. We have developed an operational system for
obtaining information from these sites for the purposes of identifying emerging
cyber threats. Currently, this system collects on average 305 high-quality
cyber threat warnings each week. These threat warnings include information on
newly developed malware and exploits that have not yet been deployed in a
cyber-attack. This provides a significant service to cyber-defenders. The
system is significantly augmented through the use of various data mining and
machine learning techniques. With the use of machine learning models, we are
able to recall 92% of products in marketplaces and 80% of discussions on forums
relating to malicious hacking with high precision. We perform preliminary
analysis on the data collected, demonstrating its application to aid a security
expert for better threat analysis.
</summary>
    <author>
      <name>Eric Nunes</name>
    </author>
    <author>
      <name>Ahmad Diab</name>
    </author>
    <author>
      <name>Andrew Gunn</name>
    </author>
    <author>
      <name>Ericsson Marin</name>
    </author>
    <author>
      <name>Vineet Mishra</name>
    </author>
    <author>
      <name>Vivin Paliath</name>
    </author>
    <author>
      <name>John Robertson</name>
    </author>
    <author>
      <name>Jana Shakarian</name>
    </author>
    <author>
      <name>Amanda Thart</name>
    </author>
    <author>
      <name>Paulo Shakarian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 page paper accepted to be presented at IEEE Intelligence and
  Security Informatics 2016 Tucson, Arizona USA September 27-30, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08580v2</id>
    <updated>2016-08-29T18:51:21Z</updated>
    <published>2016-07-28T19:13:20Z</published>
    <title>MIST: Missing Person Intelligence Synthesis Toolkit</title>
    <summary>  Each day, approximately 500 missing persons cases occur that go
unsolved/unresolved in the United States. The non-profit organization known as
the Find Me Group (FMG), led by former law enforcement professionals, is
dedicated to solving or resolving these cases. This paper introduces the
Missing Person Intelligence Synthesis Toolkit (MIST) which leverages a
data-driven variant of geospatial abductive inference. This system takes search
locations provided by a group of experts and rank-orders them based on the
probability assigned to areas based on the prior performance of the experts
taken as a group. We evaluate our approach compared to the current practices
employed by the Find Me Group and found it significantly reduces the search
area - leading to a reduction of 31 square miles over 24 cases we examined in
our experiments. Currently, we are using MIST to aid the Find Me Group in an
active missing person case.
</summary>
    <author>
      <name>Elham Shaabani</name>
    </author>
    <author>
      <name>Hamidreza Alvari</name>
    </author>
    <author>
      <name>Paulo Shakarian</name>
    </author>
    <author>
      <name>J. E. Kelly Snyder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures, Accepted in CIKM 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08580v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08580v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; J.4; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08485v1</id>
    <updated>2016-07-28T14:47:52Z</updated>
    <published>2016-07-28T14:47:52Z</published>
    <title>A symbolic algebra for the computation of expected utilities in
  multiplicative influence diagrams</title>
    <summary>  Influence diagrams provide a compact graphical representation of decision
problems. Several algorithms for the quick computation of their associated
expected utilities are available in the literature. However, often they rely on
a full quantification of both probabilistic uncertainties and utility values.
For problems where all random variables and decision spaces are finite and
discrete, here we develop a symbolic way to calculate the expected utilities of
influence diagrams that does not require a full numerical representation.
Within this approach expected utilities correspond to families of polynomials.
After characterizing their polynomial structure, we develop an efficient
symbolic algorithm for the propagation of expected utilities through the
diagram and provide an implementation of this algorithm using a computer
algebra system. We then characterize many of the standard manipulations of
influence diagrams as transformations of polynomials. We also generalize the
decision analytic framework of these diagrams by defining asymmetries as
operations over the expected utility polynomials.
</summary>
    <author>
      <name>Manuele Leonelli</name>
    </author>
    <author>
      <name>Eva Riccomagno</name>
    </author>
    <author>
      <name>Jim Q. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08438v1</id>
    <updated>2016-07-28T13:10:27Z</updated>
    <published>2016-07-28T13:10:27Z</published>
    <title>Faceless Person Recognition; Privacy Implications in Social Media</title>
    <summary>  As we shift more of our lives into the virtual domain, the volume of data
shared on the web keeps increasing and presents a threat to our privacy. This
works contributes to the understanding of privacy implications of such data
sharing by analysing how well people are recognisable in social media data. To
facilitate a systematic study we define a number of scenarios considering
factors such as how many heads of a person are tagged and if those heads are
obfuscated or not. We propose a robust person recognition system that can
handle large variations in pose and clothing, and can be trained with few
training samples. Our results indicate that a handful of images is enough to
threaten users' privacy, even in the presence of obfuscation. We show detailed
experimental results, and discuss their implications.
</summary>
    <author>
      <name>Seong Joon Oh</name>
    </author>
    <author>
      <name>Rodrigo Benenson</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08329v2</id>
    <updated>2016-08-22T03:47:51Z</updated>
    <published>2016-07-28T06:40:30Z</published>
    <title>Robust Contextual Outlier Detection: Where Context Meets Sparsity</title>
    <summary>  Outlier detection is a fundamental data science task with applications
ranging from data cleaning to network security. Given the fundamental nature of
the task, this has been the subject of much research. Recently, a new class of
outlier detection algorithms has emerged, called {\it contextual outlier
detection}, and has shown improved performance when studying anomalous behavior
in a specific context. However, as we point out in this article, such
approaches have limited applicability in situations where the context is sparse
(i.e. lacking a suitable frame of reference). Moreover, approaches developed to
date do not scale to large datasets. To address these problems, here we propose
a novel and robust approach alternative to the state-of-the-art called RObust
Contextual Outlier Detection (ROCOD). We utilize a local and global behavioral
model based on the relevant contexts, which is then integrated in a natural and
robust fashion. We also present several optimizations to improve the
scalability of the approach. We run ROCOD on both synthetic and real-world
datasets and demonstrate that it outperforms other competitive baselines on the
axes of efficacy and efficiency (40X speedup compared to modern contextual
outlier detection methods). We also drill down and perform a fine-grained
analysis to shed light on the rationale for the performance gains of ROCOD and
reveal its effectiveness when handling objects with sparse contexts.
</summary>
    <author>
      <name>Jiongqian Liang</name>
    </author>
    <author>
      <name>Srinivasan Parthasarathy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. Extended version of CIKM'16 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08325v1</id>
    <updated>2016-07-28T06:15:24Z</updated>
    <published>2016-07-28T06:15:24Z</published>
    <title>VHT: Vertical Hoeffding Tree</title>
    <summary>  IoT Big Data requires new machine learning methods able to scale to large
size of data arriving at high speed. Decision trees are popular machine
learning models since they are very effective, yet easy to interpret and
visualize. In the literature, we can find distributed algorithms for learning
decision trees, and also streaming algorithms, but not algorithms that combine
both features. In this paper we present the Vertical Hoeffding Tree (VHT), the
first distributed streaming algorithm for learning decision trees. It features
a novel way of distributing decision trees via vertical parallelism. The
algorithm is implemented on top of Apache SAMOA, a platform for mining
distributed data streams, and thus able to run on real-world clusters. We run
several experiments to study the accuracy and throughput performance of our new
VHT algorithm, as well as its ability to scale while keeping its superior
performance with respect to non-distributed decision trees.
</summary>
    <author>
      <name>Nicolas Kourtellis</name>
    </author>
    <author>
      <name>Gianmarco De Francisci Morales</name>
    </author>
    <author>
      <name>Albert Bifet</name>
    </author>
    <author>
      <name>Arinto Murdopo</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08316v1</id>
    <updated>2016-07-28T05:03:32Z</updated>
    <published>2016-07-28T05:03:32Z</published>
    <title>Hyperparameter Optimization of Deep Neural Networks Using
  Non-Probabilistic RBF Surrogate Model</title>
    <summary>  Recently, Bayesian optimization has been successfully applied for optimizing
hyperparameters of deep neural networks, significantly outperforming the
expert-set hyperparameter values. The methods approximate and minimize the
validation error as a function of hyperparameter values through probabilistic
models like Gaussian processes. However, probabilistic models that require a
prior distribution of the errors may be not adequate for approximating very
complex error functions of deep neural networks. In this work, we propose to
employ radial basis function as the surrogate of the error functions for
optimizing both continuous and integer hyperparameters. The proposed
non-probabilistic algorithm, called Hyperparameter Optimization using RBF and
DYCORS (HORD), searches the surrogate for the most promising hyperparameter
values while providing a good balance between exploration and exploitation.
Extensive evaluations demonstrate HORD significantly outperforms the
well-established Bayesian optimization methods such as Spearmint and TPE, both
in terms of finding a near optimal solution with fewer expensive function
evaluations, and in terms of a final validation error. Further, HORD performs
equally well in low- and high-dimensional hyperparameter spaces, and by
avoiding expensive covariance computation can also scale to a high number of
observations.
</summary>
    <author>
      <name>Ilija Ilievski</name>
    </author>
    <author>
      <name>Taimoor Akhtar</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <author>
      <name>Christine Annette Shoemaker</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08289v1</id>
    <updated>2016-07-28T01:22:26Z</updated>
    <published>2016-07-28T01:22:26Z</published>
    <title>Mammalian Value Systems</title>
    <summary>  Characterizing human values is a topic deeply interwoven with the sciences,
humanities, art, and many other human endeavors. In recent years, a number of
thinkers have argued that accelerating trends in computer science, cognitive
science, and related disciplines foreshadow the creation of intelligent
machines which meet and ultimately surpass the cognitive abilities of human
beings, thereby entangling an understanding of human values with future
technological development. Contemporary research accomplishments suggest
sophisticated AI systems becoming widespread and responsible for managing many
aspects of the modern world, from preemptively planning users' travel schedules
and logistics, to fully autonomous vehicles, to domestic robots assisting in
daily living. The extrapolation of these trends has been most forcefully
described in the context of a hypothetical "intelligence explosion," in which
the capabilities of an intelligent software agent would rapidly increase due to
the presence of feedback loops unavailable to biological organisms. The
possibility of superintelligent agents, or simply the widespread deployment of
sophisticated, autonomous AI systems, highlights an important theoretical
problem: the need to separate the cognitive and rational capacities of an agent
from the fundamental goal structure, or value system, which constrains and
guides the agent's actions. The "value alignment problem" is to specify a goal
structure for autonomous agents compatible with human values. In this brief
article, we suggest that recent ideas from affective neuroscience and related
disciplines aimed at characterizing neurological and behavioral universals in
the mammalian kingdom provide important conceptual foundations relevant to
describing human values. We argue that the notion of "mammalian value systems"
points to a potential avenue for fundamental research in AI safety and AI
ethics.
</summary>
    <author>
      <name>Gopal P. Sarma</name>
    </author>
    <author>
      <name>Nick J. Hay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08186v1</id>
    <updated>2016-07-27T17:22:00Z</updated>
    <published>2016-07-27T17:22:00Z</published>
    <title>Android Malware Detection Using Parallel Machine Learning Classifiers</title>
    <summary>  Mobile malware has continued to grow at an alarming rate despite on-going
efforts towards mitigating the problem. This has been particularly noticeable
on Android due to its being an open platform that has subsequently overtaken
other platforms in the share of the mobile smart devices market. Hence,
incentivizing a new wave of emerging Android malware sophisticated enough to
evade most common detection methods. This paper proposes and investigates a
parallel machine learning based classification approach for early detection of
Android malware. Using real malware samples and benign applications, a
composite classification model is developed from parallel combination of
heterogeneous classifiers. The empirical evaluation of the model under
different combination schemes demonstrates its efficacy and potential to
improve detection accuracy. More importantly, by utilizing several classifiers
with diverse characteristics, their strengths can be harnessed not only for
enhanced Android malware detection but also quicker white box analysis by means
of the more interpretable constituent classifiers.
</summary>
    <author>
      <name>Suleiman Y. Yerima</name>
    </author>
    <author>
      <name>Sakir Sezer</name>
    </author>
    <author>
      <name>Igor Muttik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/NGMAST.2014.23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/NGMAST.2014.23" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8th International Conference on Next Generation Mobile Applications,
  Services and Technologies, (NGMAST), 10-14 Sept., 2014, Oxford, United
  Kingdom</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08181v1</id>
    <updated>2016-07-27T17:08:05Z</updated>
    <published>2016-07-27T17:08:05Z</published>
    <title>Psychologically inspired planning method for smart relocation task</title>
    <summary>  Behavior planning is known to be one of the basic cognitive functions, which
is essential for any cognitive architecture of any control system used in
robotics. At the same time most of the widespread planning algorithms employed
in those systems are developed using only approaches and models of Artificial
Intelligence and don't take into account numerous results of cognitive
experiments. As a result, there is a strong need for novel methods of behavior
planning suitable for modern cognitive architectures aimed at robot control.
One such method is presented in this work and is studied within a special class
of navigation task called smart relocation task. The method is based on the
hierarchical two-level model of abstraction and knowledge representation, e.g.
symbolic and subsymbolic. On the symbolic level sign world model is used for
knowledge representation and hierarchical planning algorithm, PMA, is utilized
for planning. On the subsymbolic level the task of path planning is considered
and solved as a graph search problem. Interaction between both planners is
examined and inter-level interfaces and feedback loops are described.
Preliminary experimental results are presented.
</summary>
    <author>
      <name>Aleksandr I. Panov</name>
    </author>
    <author>
      <name>Konstantin Yakovlev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">As submitted to the 7th International Conference on Biologically
  Inspired Cognitive Architectures (BICA 2016), New-York, USA, July 16-19 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08149v1</id>
    <updated>2016-07-27T15:32:18Z</updated>
    <published>2016-07-27T15:32:18Z</published>
    <title>N-opcode Analysis for Android Malware Classification and Categorization</title>
    <summary>  Malware detection is a growing problem particularly on the Android mobile
platform due to its increasing popularity and accessibility to numerous third
party app markets. This has also been made worse by the increasingly
sophisticated detection avoidance techniques employed by emerging malware
families. This calls for more effective techniques for detection and
classification of Android malware. Hence, in this paper we present an n-opcode
analysis based approach that utilizes machine learning to classify and
categorize Android malware. This approach enables automated feature discovery
that eliminates the need for applying expert or domain knowledge to define the
needed features. Our experiments on 2520 samples that were performed using up
to 10-gram opcode features showed that an f-measure of 98% is achievable using
this approach.
</summary>
    <author>
      <name>BooJoong Kang</name>
    </author>
    <author>
      <name>Suleiman Y. Yerima</name>
    </author>
    <author>
      <name>Kieran McLaughlin</name>
    </author>
    <author>
      <name>Sakir Sezer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CyberSecPODS.2016.7502343</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CyberSecPODS.2016.7502343" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08131v1</id>
    <updated>2016-07-27T14:54:47Z</updated>
    <published>2016-07-27T14:54:47Z</published>
    <title>Neuromorphic Robot Dream</title>
    <summary>  In this paper we present the next step in our approach to neurobiologically
plausible implementation of emotional reactions and behaviors for real-time
autonomous robotic systems. The working metaphor we use is the "day" and the
"night" phases of mammalian life. During the "day phase" a robotic system
stores the inbound information and is controlled by a light-weight rule-based
system in real time. In contrast to that, during the "night phase" information
that has been stored is transferred to a supercomputing system to update the
realistic neural network: emotional and behavioral strategies.
</summary>
    <author>
      <name>Alexander Tchitchigin</name>
    </author>
    <author>
      <name>Max Talanov</name>
    </author>
    <author>
      <name>Larisa Safina</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <link href="http://arxiv.org/abs/1607.08131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08100v1</id>
    <updated>2016-07-27T14:10:28Z</updated>
    <published>2016-07-27T14:10:28Z</published>
    <title>Automatically Reinforcing a Game AI</title>
    <summary>  A recent research trend in Artificial Intelligence (AI) is the combination of
several programs into one single, stronger, program; this is termed portfolio
methods. We here investigate the application of such methods to Game Playing
Programs (GPPs). In addition, we consider the case in which only one GPP is
available - by decomposing this single GPP into several ones through the use of
parameters or even simply random seeds. These portfolio methods are trained in
a learning phase. We propose two different offline approaches. The simplest
one, BestArm, is a straightforward optimization of seeds or parame- ters; it
performs quite well against the original GPP, but performs poorly against an
opponent which repeats games and learns. The second one, namely Nash-portfolio,
performs similarly in a "one game" test, and is much more robust against an
opponent who learns. We also propose an online learning portfolio, which tests
several of the GPP repeatedly and progressively switches to the best one -
using a bandit algorithm.
</summary>
    <author>
      <name>David L. St-Pierre</name>
    </author>
    <author>
      <name>Jean-Baptiste Hoock</name>
    </author>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Fabien Teytaud</name>
    </author>
    <author>
      <name>Olivier Teytaud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 31 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08085v1</id>
    <updated>2016-07-27T13:35:16Z</updated>
    <published>2016-07-27T13:35:16Z</published>
    <title>Improving Semantic Embedding Consistency by Metric Learning for
  Zero-Shot Classification</title>
    <summary>  This paper addresses the task of zero-shot image classification. The key
contribution of the proposed approach is to control the semantic embedding of
images -- one of the main ingredients of zero-shot learning -- by formulating
it as a metric learning problem. The optimized empirical criterion associates
two types of sub-task constraints: metric discriminating capacity and accurate
attribute prediction. This results in a novel expression of zero-shot learning
not requiring the notion of class in the training phase: only pairs of
image/attributes, augmented with a consistency indicator, are given as ground
truth. At test time, the learned model can predict the consistency of a test
image with a given set of attributes , allowing flexible ways to produce
recognition inferences. Despite its simplicity, the proposed approach gives
state-of-the-art results on four challenging datasets used for zero-shot
recognition evaluation.
</summary>
    <author>
      <name>Maxime Bucher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Palaiseau</arxiv:affiliation>
    </author>
    <author>
      <name>Stéphane Herbin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Palaiseau</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Jurie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in ECCV 2016, Oct 2016, amsterdam, Netherlands. 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08075v1</id>
    <updated>2016-07-27T13:13:41Z</updated>
    <published>2016-07-27T13:13:41Z</published>
    <title>Harmonization of conflicting medical opinions using argumentation
  protocols and textual entailment - a case study on Parkinson disease</title>
    <summary>  Parkinson's disease is the second most common neurodegenerative disease,
affecting more than 1.2 million people in Europe. Medications are available for
the management of its symptoms, but the exact cause of the disease is unknown
and there is currently no cure on the market. To better understand the
relations between new findings and current medical knowledge, we need tools
able to analyse published medical papers based on natural language processing
and tools capable to identify various relationships of new findings with the
current medical knowledge. Our work aims to fill the above technological gap.
  To identify conflicting information in medical documents, we enact textual
entailment technology. To encapsulate existing medical knowledge, we rely on
ontologies. To connect the formal axioms in ontologies with natural text in
medical articles, we exploit ontology verbalisation techniques. To assess the
level of disagreement between human agents with respect to a medical issue, we
rely on fuzzy aggregation. To harmonize this disagreement, we design mediation
protocols within a multi-agent framework.
</summary>
    <author>
      <name>Adrian Groza</name>
    </author>
    <author>
      <name>Madalina Mand Nagy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08074v1</id>
    <updated>2016-07-27T13:08:41Z</updated>
    <published>2016-07-27T13:08:41Z</published>
    <title>Mining Arguments from Cancer Documents Using Natural Language Processing
  and Ontologies</title>
    <summary>  In the medical domain, the continuous stream of scientific research contains
contradictory results supported by arguments and counter-arguments. As medical
expertise occurs at different levels, part of the human agents have
difficulties to face the huge amount of studies, but also to understand the
reasons and pieces of evidences claimed by the proponents and the opponents of
the debated topic. To better understand the supporting arguments for new
findings related to current state of the art in the medical domain we need
tools able to identify arguments in scientific papers. Our work here aims to
fill the above technological gap.
  Quite aware of the difficulty of this task, we embark to this road by relying
on the well-known interleaving of domain knowledge with natural language
processing. To formalise the existing medical knowledge, we rely on ontologies.
To structure the argumentation model we use also the expressivity and reasoning
capabilities of Description Logics. To perform argumentation mining we
formalise various linguistic patterns in a rule-based language. We tested our
solution against a corpus of scientific papers related to breast cancer. The
run experiments show a F-measure between 0.71 and 0.86 for identifying
conclusions of an argument and between 0.65 and 0.86 for identifying premises
of an argument.
</summary>
    <author>
      <name>Adrian Groza</name>
    </author>
    <author>
      <name>Oana Popa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08073v1</id>
    <updated>2016-07-27T13:08:13Z</updated>
    <published>2016-07-27T13:08:13Z</published>
    <title>Assisting Drivers During Overtaking Using Car-2-Car Communication and
  Multi-Agent Systems</title>
    <summary>  A warning system for assisting drivers during overtaking maneuvers is
proposed. The system relies on Car-2-Car communication technologies and
multi-agent systems. A protocol for safety overtaking is proposed based on ACL
communicative acts. The mathematical model for safety overtaking used Kalman
filter to minimize localization error.
</summary>
    <author>
      <name>Adrian Groza</name>
    </author>
    <author>
      <name>Calin Cara</name>
    </author>
    <author>
      <name>Sergiu Zaporojan</name>
    </author>
    <author>
      <name>Igor Calmicov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint ICCP 2016, Cluj-Napoca</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08038v1</id>
    <updated>2016-07-27T11:12:02Z</updated>
    <published>2016-07-27T11:12:02Z</published>
    <title>Behavior and path planning for the coalition of cognitive robots in
  smart relocation tasks</title>
    <summary>  In this paper we outline the approach of solving special type of navigation
tasks for robotic systems, when a coalition of robots (agents) acts in the 2D
environment, which can be modified by the actions, and share the same goal
location. The latter is originally unreachable for some members of the
coalition, but the common task still can be accomplished as the agents can
assist each other (e.g. by modifying the environment). We call such tasks smart
relocation tasks (as the can not be solved by pure path planning methods) and
study spatial and behavior interaction of robots while solving them. We use
cognitive approach and introduce semiotic knowledge representation - sign world
model which underlines behavioral planning methodology. Planning is viewed as a
recursive search process in the hierarchical state-space induced by sings with
path planning signs reside on the lowest level. Reaching this level triggers
path planning which is accomplished by state of the art grid-based planners
focused on producing smooth paths (e.g. LIAN) and thus indirectly guarantying
feasibility of that paths against agent's dynamic constraints.
</summary>
    <author>
      <name>Aleksandr I. Panov</name>
    </author>
    <author>
      <name>Konstantin Yakovlev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-31293-4_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-31293-4_1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">As submitted to the 4th International Conference on Robot
  Intelligence Technology and Applications (RiTA-2015), Bucheon, Korea,
  December 14-16, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07956v1</id>
    <updated>2016-07-27T04:51:17Z</updated>
    <published>2016-07-27T04:51:17Z</published>
    <title>Joint Embedding of Hierarchical Categories and Entities for Concept
  Categorization and Dataless Classification</title>
    <summary>  Due to the lack of structured knowledge applied in learning distributed
representation of cate- gories, existing work cannot incorporate category
hierarchies into entity information. We propose a framework that embeds
entities and categories into a semantic space by integrating structured
knowledge and taxonomy hierarchy from large knowledge bases. The framework
allows to com- pute meaningful semantic relatedness between entities and
categories. Our framework can han- dle both single-word concepts and
multiple-word concepts with superior performance on concept categorization and
yield state of the art results on dataless hierarchical classification.
</summary>
    <author>
      <name>Yuezhang Li</name>
    </author>
    <author>
      <name>Ronghuo Zheng</name>
    </author>
    <author>
      <name>Tian Tian</name>
    </author>
    <author>
      <name>Zhiting Hu</name>
    </author>
    <author>
      <name>Rahul Iyer</name>
    </author>
    <author>
      <name>Katia Sycara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, submitted to Coling 2016. arXiv admin note: substantial
  text overlap with arXiv:1605.03924</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07942v1</id>
    <updated>2016-07-27T02:53:24Z</updated>
    <published>2016-07-27T02:53:24Z</published>
    <title>Multiple scan data association by convex variational inference</title>
    <summary>  Data association, the reasoning over correspondence between targets and
measurements, is a problem of fundamental importance in target tracking.
Recently, belief propagation (BP) has emerged as a promising method for
estimating the marginal probabilities of target/measurement association,
providing fast, accurate estimates. The excellent performance of BP in the
particular formulation used may be attributed to the convexity of the
underlying free energy which it implicitly optimises. This paper studies
multiple scan data association problems, i.e., problems that reason over
correspondence between targets and several sets of measurements, which may
correspond to different sensors or different time steps. We find that the
multiple scan extension of the single scan BP formulation is non-convex and
demonstrate the undesirable behaviour that can result. A convex free energy is
constructed using the recently proposed fractional free energy, and optimised
using a primal-dual coordinate ascent. Finally, based on a variational
interpretation of joint probabilistic data association (JPDA), we develop a
sequential variant of the algorithm that is similar to JPDA, but retains
consistency constraints from prior scans. The performance of the proposed
methods is demonstrated on a bearings only target localisation problem.
</summary>
    <author>
      <name>Jason L. Williams</name>
    </author>
    <author>
      <name>Roslyn A. Lau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07906v1</id>
    <updated>2016-07-26T22:06:51Z</updated>
    <published>2016-07-26T22:06:51Z</published>
    <title>Approximation and Parameterized Complexity of Minimax Approval Voting</title>
    <summary>  We present three results on the complexity of Minimax Approval Voting. First,
we study Minimax Approval Voting parameterized by the Hamming distance $d$ from
the solution to the votes. We show Minimax Approval Voting admits no algorithm
running in time $\mathcal{O}^\star(2^{o(d\log d)})$, unless the Exponential
Time Hypothesis (ETH) fails. This means that the $\mathcal{O}^\star(d^{2d})$
algorithm of Misra et al. [AAMAS 2015] is essentially optimal. Motivated by
this, we then show a parameterized approximation scheme, running in time
$\mathcal{O}^\star(\left({3}/{\epsilon}\right)^{2d})$, which is essentially
tight assuming ETH. Finally, we get a new polynomial-time randomized
approximation scheme for Minimax Approval Voting, which runs in time
$n^{\mathcal{O}(1/\epsilon^2 \cdot \log(1/\epsilon))} \cdot \mathrm{poly}(m)$,
almost matching the running time of the fastest known PTAS for Closest String
due to Ma and Sun [SIAM J. Comp. 2009].
</summary>
    <author>
      <name>Marek Cygan</name>
    </author>
    <author>
      <name>Łukasz Kowalik</name>
    </author>
    <author>
      <name>Arkadiusz Socała</name>
    </author>
    <author>
      <name>Krzysztof Sornat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures, 2 pseudocodes</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W20, 68W25, 68Q17, 68Q25, 68T42" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.11; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07896v1</id>
    <updated>2016-07-26T20:43:02Z</updated>
    <published>2016-07-26T20:43:02Z</published>
    <title>Polling-systems-based Autonomous Vehicle Coordination in Traffic
  Intersections with No Traffic Signals</title>
    <summary>  The rapid development of autonomous vehicles spurred a careful investigation
of the potential benefits of all-autonomous transportation networks. Most
studies conclude that autonomous systems can enable drastic improvements in
performance. A widely studied concept is all-autonomous, collision-free
intersections, where vehicles arriving in a traffic intersection with no
traffic light adjust their speeds to cross safely through the intersection as
quickly as possible. In this paper, we propose a coordination control algorithm
for this problem, assuming stochastic models for the arrival times of the
vehicles. The proposed algorithm provides provable guarantees on safety and
performance. More precisely, it is shown that no collisions occur surely, and
moreover a rigorous upper bound is provided for the expected wait time. The
algorithm is also demonstrated in simulations. The proposed algorithms are
inspired by polling systems. In fact, the problem studied in this paper leads
to a new polling system where customers are subject to differential
constraints, which may be interesting in its own right.
</summary>
    <author>
      <name>David Miculescu</name>
    </author>
    <author>
      <name>Sertac Karaman</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07847v1</id>
    <updated>2016-07-26T19:17:11Z</updated>
    <published>2016-07-26T19:17:11Z</published>
    <title>Technical Report: Giving Hints for Logic Programming Examples without
  Revealing Solutions</title>
    <summary>  We introduce a framework for supporting learning to program in the paradigm
of Answer Set Programming (ASP), which is a declarative logic programming
formalism. Based on the idea of teaching by asking the student to complete
small example ASP programs, we introduce a three-stage method for giving hints
to the student without revealing the correct solution of an example. We
categorize mistakes into (i) syntactic mistakes, (ii) unexpected but
syntactically correct input, and (iii) semantic mistakes, describe mathematical
definitions of these mistakes, and show how to compute hints from these
definitions.
</summary>
    <author>
      <name>Gokhan Avci</name>
    </author>
    <author>
      <name>Mustafa Mehuljic</name>
    </author>
    <author>
      <name>Peter Schüller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. This is an extended English version of "Gokhan Avci, Mustafa
  Mehuljic, and Peter Schuller. Cozumu Aciga Cikarmadan Mantiksal Programlama
  Orneklerine Ipucu Verme, Sinyal Isleme ve Iletisim Uygulamalari Kurultayi
  (SIU), pages 513-516, 2016, DOI: 10.1109/SIU.2016.7495790"</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07762v1</id>
    <updated>2016-07-26T15:48:03Z</updated>
    <published>2016-07-26T15:48:03Z</published>
    <title>Focused Model-Learning and Planning for Non-Gaussian Continuous
  State-Action Systems</title>
    <summary>  We introduce a joint framework for model leaning and planning in stochastic
domains with continuous state and action spaces with non-Gaussian transition
noise. It is efficient in large spaces with large amounts of data because (1)
local models are estimated only when the planner requires them; (2) the planner
focuses on the most relevant states to the current planning problem; and (3)
the planner focuses on the most informative and/or high-value actions. Our
theoretical analysis shows that the expected difference between the optimal
value function of the original problem and the value of the policy we compute
vanishes sub-linearly in the number of actions we test, under mild assumptions.
We show empirically that multi-modal transition models are necessary if the
underlying dynamics is not single-mode, and our algorithm is able to complete
both learning and planning within minutes for a stochastic pushing problem in
simulation given more than a million data points, as a result of focused
planning.
</summary>
    <author>
      <name>Zi Wang</name>
    </author>
    <author>
      <name>Stefanie Jegelka</name>
    </author>
    <author>
      <name>Leslie Pack Kaelbling</name>
    </author>
    <author>
      <name>Tomás Lozano-Pérez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07437v1</id>
    <updated>2016-07-26T15:27:33Z</updated>
    <published>2016-07-26T15:27:33Z</published>
    <title>Symbols of a cosmic order</title>
    <summary>  The world runs on communicated sequences of symbols, e.g. numerals. Examining
both engineered and natural communications networks reveals an unsuspected
order that depends on contact with an unpredictable entity. This order has
three roots. The first is a proof within quantum theory that no evidence can
ever determine its explanation, so that an agent choosing an explanation must
do so unpredictably. The second root is the showing that clocks that step
computers do not "tell time" but serve as self-adjusting symbol-handling agents
that regulate "logically synchronized" motion in response to unpredictable
disturbances. Such a clock-agent has a certain independence as well as the
capacity to communicate via unpredictable symbols with other clock-agents and
to adjust its own tick rate in response to that communication. The third root
is the noticing of unpredictable symbol exchange in natural systems, including
the transmission of symbols found in molecular biology. We introduce a
symbol-handling agent as a role played in some cases by a person, for example a
physicist who chooses an explanation of given experimental outcomes, and in
other cases by some other biological entity, and in still other cases by an
inanimate device, such as a computer-based detector used in physical
measurements. While we forbear to try to explain the propensity of agents at
all levels from cells to civilizations to form and operate networks of
logically synchronized symbol-handling agents, we point to this propensity as
an overlooked cosmic order, an order structured by the unpredictability ensuing
from the proof. Appreciating the cosmic order leads to a conception of agency
that replaces volition by unpredictability and re-conceives the notion of
objectivity in a way that makes a place for agency in the world as described by
physics. Some specific implications for physics are outlined.
</summary>
    <author>
      <name>F. Hadi Madjid</name>
    </author>
    <author>
      <name>John M. Myers</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.aop.2016.07.022</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.aop.2016.07.022" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages; accepted for publication in Annals of Physics</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07745v1</id>
    <updated>2016-07-26T15:19:12Z</updated>
    <published>2016-07-26T15:19:12Z</published>
    <title>Leveraging Unstructured Data to Detect Emerging Reliability Issues</title>
    <summary>  Unstructured data refers to information that does not have a predefined data
model or is not organized in a pre-defined manner. Loosely speaking,
unstructured data refers to text data that is generated by humans. In
after-sales service businesses, there are two main sources of unstructured
data: customer complaints, which generally describe symptoms, and technician
comments, which outline diagnostics and treatment information. A legitimate
customer complaint can eventually be tracked to a failure or a claim. However,
there is a delay between the time of a customer complaint and the time of a
failure or a claim. A proactive strategy aimed at analyzing customer complaints
for symptoms can help service providers detect reliability problems in advance
and initiate corrective actions such as recalls. This paper introduces
essential text mining concepts in the context of reliability analysis and a
method to detect emerging reliability issues. The application of the method is
illustrated using a case study.
</summary>
    <author>
      <name>Deovrat Kakde</name>
    </author>
    <author>
      <name>Arin Chaudhuri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RAMS.2015.7105093</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RAMS.2015.7105093" rel="related"/>
    <link href="http://arxiv.org/abs/1607.07745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07684v1</id>
    <updated>2016-07-26T13:23:20Z</updated>
    <published>2016-07-26T13:23:20Z</published>
    <title>The Price of Anarchy in Auctions</title>
    <summary>  This survey outlines a general and modular theory for proving approximation
guarantees for equilibria of auctions in complex settings. This theory
complements traditional economic techniques, which generally focus on exact and
optimal solutions and are accordingly limited to relatively stylized settings.
  We highlight three user-friendly analytical tools: smoothness-type
inequalities, which immediately yield approximation guarantees for many auction
formats of interest in the special case of complete information and
deterministic strategies; extension theorems, which extend such guarantees to
randomized strategies, no-regret learning outcomes, and incomplete-information
settings; and composition theorems, which extend such guarantees from simpler
to more complex auctions. Combining these tools yields tight worst-case
approximation guarantees for the equilibria of many widely-used auction
formats.
</summary>
    <author>
      <name>Tim Roughgarden</name>
    </author>
    <author>
      <name>Vasilis Syrgkanis</name>
    </author>
    <author>
      <name>Eva Tardos</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07602v1</id>
    <updated>2016-07-26T09:19:46Z</updated>
    <published>2016-07-26T09:19:46Z</published>
    <title>OntoCat: Automatically categorizing knowledge in API Documentation</title>
    <summary>  Most application development happens in the context of complex APIs;
reference documentation for APIs has grown tremendously in variety, complexity,
and volume, and can be difficult to navigate. There is a growing need to
develop well-organized ways to access the knowledge latent in the
documentation; several research efforts deal with the organization (ontology)
of API-related knowledge. Extensive knowledge-engineering work, supported by a
rigorous qualitative analysis, by Maalej &amp; Robillard [3] has identified a
useful taxonomy of API knowledge. Based on this taxonomy, we introduce a domain
independent technique to extract the knowledge types from the given API
reference documentation. Our system, OntoCat, introduces total nine different
features and their semantic and statistical combinations to classify the
different knowledge types. We tested OntoCat on python API reference
documentation. Our experimental results show the effectiveness of the system
and opens the scope of probably related research areas (i.e., user behavior,
documentation quality, etc.).
</summary>
    <author>
      <name>Niraj Kumar</name>
    </author>
    <author>
      <name>Premkumar Devanbu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be submitted for journal publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07514v1</id>
    <updated>2016-07-26T00:58:14Z</updated>
    <published>2016-07-26T00:58:14Z</published>
    <title>Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM
  Encoder-Decoder</title>
    <summary>  We present Tweet2Vec, a novel method for generating general-purpose vector
representation of tweets. The model learns tweet embeddings using
character-level CNN-LSTM encoder-decoder. We trained our model on 3 million,
randomly selected English-language tweets. The model was evaluated using two
methods: tweet semantic similarity and tweet sentiment categorization,
outperforming the previous state-of-the-art in both tasks. The evaluations
demonstrate the power of the tweet embeddings generated by our model for
various tweet categorization tasks. The vector representations generated by our
model are generic, and hence can be applied to a variety of tasks. Though the
model presented in this paper is trained on English-language tweets, the method
presented can be used to learn tweet embeddings for different languages.
</summary>
    <author>
      <name>Soroush Vosoughi</name>
    </author>
    <author>
      <name>Prashanth Vijayaraghavan</name>
    </author>
    <author>
      <name>Deb Roy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2911451.2914762</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2911451.2914762" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGIR 2016, July 17-21, 2016, Pisa. Proceedings of SIGIR 2016. Pisa,
  Italy (2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07326v1</id>
    <updated>2016-07-25T15:54:07Z</updated>
    <published>2016-07-25T15:54:07Z</published>
    <title>Meta-Prod2Vec - Product Embeddings Using Side-Information for
  Recommendation</title>
    <summary>  We propose Meta-Prod2vec, a novel method to compute item similarities for
recommendation that leverages existing item metadata. Such scenarios are
frequently encountered in applications such as content recommendation, ad
targeting and web search. Our method leverages past user interactions with
items and their attributes to compute low-dimensional embeddings of items.
Specifically, the item metadata is in- jected into the model as side
information to regularize the item embeddings. We show that the new item
representa- tions lead to better performance on recommendation tasks on an open
music dataset.
</summary>
    <author>
      <name>Flavian Vasile</name>
    </author>
    <author>
      <name>Elena Smirnova</name>
    </author>
    <author>
      <name>Alexis Conneau</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2959100.2959160</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2959100.2959160" rel="related"/>
    <link href="http://arxiv.org/abs/1607.07326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07311v1</id>
    <updated>2016-07-25T15:17:06Z</updated>
    <published>2016-07-25T15:17:06Z</published>
    <title>Estimating Activity at Multiple Scales using Spatial Abstractions</title>
    <summary>  Autonomous robots operating in dynamic environments must maintain beliefs
over a hypothesis space that is rich enough to represent the activities of
interest at different scales. This is important both in order to accommodate
the availability of evidence at varying degrees of coarseness, such as when
interpreting and assimilating natural instructions, but also in order to make
subsequent reactive planning more efficient. We present an algorithm that
combines a topology-based trajectory clustering procedure that generates
hierarchically-structured spatial abstractions with a bank of particle filters
at each of these abstraction levels so as to produce probability estimates over
an agent's navigation activity that is kept consistent across the hierarchy. We
study the performance of the proposed method using a synthetic trajectory
dataset in 2D, as well as a dataset taken from AIS-based tracking of ships in
an extended harbour area. We show that, in comparison to a baseline which is a
particle filter that estimates activity without exploiting such structure, our
method achieves a better normalised error in predicting the trajectory as well
as better time to convergence to a true class when compared against ground
truth.
</summary>
    <author>
      <name>Majd Hawasly</name>
    </author>
    <author>
      <name>Florian T. Pokorny</name>
    </author>
    <author>
      <name>Subramanian Ramamoorthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07730v1</id>
    <updated>2016-07-25T13:04:22Z</updated>
    <published>2016-07-25T13:04:22Z</published>
    <title>A Model of Pathways to Artificial Superintelligence Catastrophe for Risk
  and Decision Analysis</title>
    <summary>  An artificial superintelligence (ASI) is artificial intelligence that is
significantly more intelligent than humans in all respects. While ASI does not
currently exist, some scholars propose that it could be created sometime in the
future, and furthermore that its creation could cause a severe global
catastrophe, possibly even resulting in human extinction. Given the high
stakes, it is important to analyze ASI risk and factor the risk into decisions
related to ASI research and development. This paper presents a graphical model
of major pathways to ASI catastrophe, focusing on ASI created via recursive
self-improvement. The model uses the established risk and decision analysis
modeling paradigms of fault trees and influence diagrams in order to depict
combinations of events and conditions that could lead to AI catastrophe, as
well as intervention options that could decrease risks. The events and
conditions include select aspects of the ASI itself as well as the human
process of ASI research, development, and management. Model structure is
derived from published literature on ASI risk. The model offers a foundation
for rigorous quantitative evaluation and decision making on the long-term risk
of ASI catastrophe.
</summary>
    <author>
      <name>Anthony M. Barrett</name>
    </author>
    <author>
      <name>Seth D. Baum</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/0952813X.2016.1186228</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/0952813X.2016.1186228" rel="related"/>
    <link href="http://arxiv.org/abs/1607.07730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07249v3</id>
    <updated>2016-09-13T10:27:06Z</updated>
    <published>2016-07-25T12:47:38Z</published>
    <title>An Evolutionary Algorithm to Learn SPARQL Queries for
  Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia</title>
    <summary>  Efficient usage of the knowledge provided by the Linked Data community is
often hindered by the need for domain experts to formulate the right SPARQL
queries to answer questions. For new questions they have to decide which
datasets are suitable and in which terminology and modelling style to phrase
the SPARQL query.
  In this work we present an evolutionary algorithm to help with this
challenging task. Given a training list of source-target node-pair examples our
algorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The
learned patterns can be visualised to form the basis for further investigation,
or they can be used to predict target nodes for new source nodes.
  Amongst others, we apply our algorithm to a dataset of several hundred human
associations (such as "circle - square") to find patterns for them in DBpedia.
We show the scalability of the algorithm by running it against a SPARQL
endpoint loaded with &gt; 7.9 billion triples. Further, we use the resulting
SPARQL queries to mimic human associations with a Mean Average Precision (MAP)
of 39.9 % and a Recall@10 of 63.9 %.
</summary>
    <author>
      <name>Jörn Hees</name>
    </author>
    <author>
      <name>Rouven Bauer</name>
    </author>
    <author>
      <name>Joachim Folz</name>
    </author>
    <author>
      <name>Damian Borth</name>
    </author>
    <author>
      <name>Andreas Dengel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, as of 2016-09-13
  6a19d5d7020770dc0711081ce2c1e52f71bf4b86</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07249v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07249v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx, 68T05, 68T10, 68T30, 05C85" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; I.2.4; I.2.6; I.5; I.5.2; I.5.3; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07043v1</id>
    <updated>2016-07-24T13:39:11Z</updated>
    <published>2016-07-24T13:39:11Z</published>
    <title>Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</title>
    <summary>  3D action recognition - analysis of human actions based on 3D skeleton data -
becomes popular recently due to its succinctness, robustness, and
view-invariant representation. Recent attempts on this problem suggested to
develop RNN-based learning methods to model the contextual dependency in the
temporal domain. In this paper, we extend this idea to spatio-temporal domains
to analyze the hidden sources of action-related information within the input
data over both domains concurrently. Inspired by the graphical structure of the
human skeleton, we further propose a more powerful tree-structure based
traversal method. To handle the noise and occlusion in 3D skeleton data, we
introduce new gating mechanism within LSTM to learn the reliability of the
sequential input data and accordingly adjust its effect on updating the
long-term context information stored in the memory cell. Our method achieves
state-of-the-art performance on 4 challenging benchmark datasets for 3D human
action analysis.
</summary>
    <author>
      <name>Jun Liu</name>
    </author>
    <author>
      <name>Amir Shahroudy</name>
    </author>
    <author>
      <name>Dong Xu</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07027v1</id>
    <updated>2016-07-24T11:22:00Z</updated>
    <published>2016-07-24T11:22:00Z</published>
    <title>Redundancy-free Verbalization of Individuals for Ontology Validation</title>
    <summary>  We investigate the problem of verbalizing Web Ontology Language (OWL) axioms
of domain ontologies in this paper. The existing approaches address the problem
of fidelity of verbalized OWL texts to OWL semantics by exploring different
ways of expressing the same OWL axiom in various linguistic forms. They also
perform grouping and aggregating of the natural language (NL) sentences that
are generated corresponding to each OWL statement into a comprehensible
structure. However, no efforts have been taken to try out a semantic reduction
at logical level to remove redundancies and repetitions, so that the reduced
set of axioms can be used for generating a more meaningful and
human-understandable (what we call redundancy-free) text. Our experiments show
that, formal semantic reduction at logical level is very helpful to generate
redundancy-free descriptions of ontology entities. In this paper, we
particularly focus on generating descriptions of individuals of SHIQ based
ontologies. The details of a case study are provided to support the usefulness
of the redundancy-free NL descriptions of individuals, in knowledge validation
application.
</summary>
    <author>
      <name>E. V. Vinu</name>
    </author>
    <author>
      <name>P Sreenivasa Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08116v1</id>
    <updated>2016-07-23T08:43:12Z</updated>
    <published>2016-07-23T08:43:12Z</published>
    <title>A DEMATEL-Based Completion Method for Incomplete Pairwise Comparison
  Matrix in AHP</title>
    <summary>  Pairwise comparison matrix as a crucial component of AHP, presents the
prefer- ence relations among alternatives. However, in many cases, the pairwise
comparison matrix is difficult to complete, which obstructs the subsequent
operations of the clas- sical AHP. In this paper, based on DEMATEL which has
ability to derive the total relation matrix from direct relation matrix, a new
completion method for incomplete pairwise comparison matrix is proposed. The
proposed method provides a new per- spective to estimate the missing values
with explicit physical meaning. Besides, the proposed method has low
computational cost. This promising method has a wide application in
multi-criteria decision-making.
</summary>
    <author>
      <name>Xinyi Zhou</name>
    </author>
    <author>
      <name>Yong Hu</name>
    </author>
    <author>
      <name>Yong Deng</name>
    </author>
    <author>
      <name>Felix T. S. Chan</name>
    </author>
    <author>
      <name>Alessio Ishizak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06875v2</id>
    <updated>2016-07-30T13:32:01Z</updated>
    <published>2016-07-23T01:46:09Z</published>
    <title>Processing Natural Language About Ongoing Actions</title>
    <summary>  Actions may not proceed as planned; they may be interrupted, resumed or
overridden. This is a challenge to handle in a natural language understanding
system. We describe extensions to an existing implementation for the control of
autonomous systems by natural language, to enable such systems to handle
incoming language requests regarding actions. Language Communication with
Autonomous Systems (LCAS) has been extended with support for X-nets,
parameterized executable schemas representing actions. X-nets enable the system
to control actions at a desired level of granularity, while providing a
mechanism for language requests to be processed asynchronously. Standard
semantics supported include requests to stop, continue, or override the
existing action. The specific domain demonstrated is the control of motion of a
simulated robot, but the approach is general, and could be applied to other
domains.
</summary>
    <author>
      <name>Steve Doubleday</name>
    </author>
    <author>
      <name>Sean Trott</name>
    </author>
    <author>
      <name>Jerome Feldman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures. Updated with PIPE citations</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06875v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06875v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06759v1</id>
    <updated>2016-07-22T17:37:24Z</updated>
    <published>2016-07-22T17:37:24Z</published>
    <title>Predicting Enemy's Actions Improves Commander Decision-Making</title>
    <summary>  The Defense Advanced Research Projects Agency (DARPA) Real-time Adversarial
Intelligence and Decision-making (RAID) program is investigating the
feasibility of "reading the mind of the enemy" - to estimate and anticipate, in
real-time, the enemy's likely goals, deceptions, actions, movements and
positions. This program focuses specifically on urban battles at echelons of
battalion and below. The RAID program leverages approximate game-theoretic and
deception-sensitive algorithms to provide real-time enemy estimates to a
tactical commander. A key hypothesis of the program is that these predictions
and recommendations will make the commander more effective, i.e. he should be
able to achieve his operational goals safer, faster, and more efficiently.
Realistic experimentation and evaluation drive the development process using
human-in-the-loop wargames to compare humans and the RAID system. Two
experiments were conducted in 2005 as part of Phase I to determine if the RAID
software could make predictions and recommendations as effectively and
accurately as a 4-person experienced staff. This report discusses the
intriguing and encouraging results of these first two experiments conducted by
the RAID program. It also provides details about the experiment environment and
methodology that were used to demonstrate and prove the research goals.
</summary>
    <author>
      <name>Michael Ownby</name>
    </author>
    <author>
      <name>Alexander Kott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A version of this paper was presented at CCRTS'06</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07288v1</id>
    <updated>2016-07-22T17:18:05Z</updated>
    <published>2016-07-22T17:18:05Z</published>
    <title>Validation of Information Fusion</title>
    <summary>  We motivate and offer a formal definition of validation as it applies to
information fusion systems. Common definitions of validation compare the actual
state of the world with that derived by the fusion process. This definition
conflates properties of the fusion system with properties of systems that
intervene between the world and the fusion system. We propose an alternative
definition where validation of an information fusion system references a
standard fusion device, such as recognized human experts. We illustrate the
approach by describing the validation process implemented in RAID, a program
conducted by DARPA and focused on information fusion in adversarial, deceptive
environments.
</summary>
    <author>
      <name>Alexander Kott</name>
    </author>
    <author>
      <name>Wes Milks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a version of the paper presented at FUSION'09</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06667v1</id>
    <updated>2016-07-22T13:12:33Z</updated>
    <published>2016-07-22T13:12:33Z</published>
    <title>Audio inpainting with similarity graphs</title>
    <summary>  In this contribution, we present a method to compensate for long duration
data gaps in audio signals, in particular music. To achieve this task, a
similarity graph is constructed, based on a short-time Fourier analysis of
reliable signal segments, e.g. the uncorrupted remainder of the music piece,
and the temporal regions adjacent to the unreliable section of the signal. A
suitable candidate segment is then selected through an optimization scheme and
smoothly inserted into the gap.
</summary>
    <author>
      <name>Nathanael Perraudin</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <author>
      <name>Piotr Majdak</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06641v1</id>
    <updated>2016-07-22T11:51:49Z</updated>
    <published>2016-07-22T11:51:49Z</published>
    <title>Optimal resampling for the noisy OneMax problem</title>
    <summary>  The OneMax problem is a standard benchmark optimisation problem for a binary
search space. Recent work on applying a Bandit-Based Random Mutation
Hill-Climbing algorithm to the noisy OneMax Problem showed that it is important
to choose a good value for the resampling number to make a careful trade off
between taking more samples in order to reduce noise, and taking fewer samples
to reduce the total computational cost. This paper extends that observation, by
deriving an analytical expression for the running time of the RMHC algorithm
with resampling applied to the noisy OneMax problem, and showing both
theoretically and empirically that the optimal resampling number increases with
the number of dimensions in the search space.
</summary>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Michael Fairbank</name>
    </author>
    <author>
      <name>Diego Pérez-Liébana</name>
    </author>
    <author>
      <name>Simon M. Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 table, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06617v1</id>
    <updated>2016-07-22T09:48:25Z</updated>
    <published>2016-07-22T09:48:25Z</published>
    <title>Latent Variable Discovery Using Dependency Patterns</title>
    <summary>  The causal discovery of Bayesian networks is an active and important research
area, and it is based upon searching the space of causal models for those which
can best explain a pattern of probabilistic dependencies shown in the data.
However, some of those dependencies are generated by causal structures
involving variables which have not been measured, i.e., latent variables. Some
such patterns of dependency "reveal" themselves, in that no model based solely
upon the observed variables can explain them as well as a model using a latent
variable. That is what latent variable discovery is based upon. Here we did a
search for finding them systematically, so that they may be applied in latent
variable discovery in a more rigorous fashion.
</summary>
    <author>
      <name>Xuhui Zhang</name>
    </author>
    <author>
      <name>Kevin B. Korb</name>
    </author>
    <author>
      <name>Ann E. Nicholson</name>
    </author>
    <author>
      <name>Steven Mascaro</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06583v1</id>
    <updated>2016-07-22T07:48:18Z</updated>
    <published>2016-07-22T07:48:18Z</published>
    <title>Classification of Alzheimer's Disease Structural MRI Data by Deep
  Learning Convolutional Neural Networks</title>
    <summary>  Recently, machine learning techniques especially predictive modeling and
pattern recognition in biomedical sciences from drug delivery system to medical
imaging has become one of the important methods which are assisting researchers
to have deeper understanding of entire issue and to solve complex medical
problems. Deep learning is a powerful machine learning algorithm in
classification while extracting low to high-level features. In this paper, we
used convolutional neural network to classify Alzheimer's brain from normal
healthy brain. The importance of classifying this kind of medical data is to
potentially develop a predict model or system in order to recognize the type
disease from normal subjects or to estimate the stage of the disease.
Classification of clinical data such as Alzheimer's disease has been always
challenging and most problematic part has been always selecting the most
discriminative features. Using Convolutional Neural Network (CNN) and the
famous architecture LeNet-5, we successfully classified structural MRI data of
Alzheimer's subjects from normal controls where the accuracy of test data on
trained data reached 98.84%. This experiment suggests us the shift and scale
invariant features extracted by CNN followed by deep learning classification is
most powerful method to distinguish clinical data from healthy data in fMRI.
This approach also enables us to expand our methodology to predict more
complicated systems.
</summary>
    <author>
      <name>Saman Sarraf</name>
    </author>
    <author>
      <name>Ghassem Tofighi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1603.08631</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06560v1</id>
    <updated>2016-07-22T05:38:37Z</updated>
    <published>2016-07-22T05:38:37Z</published>
    <title>Automated Prediction of Temporal Relations</title>
    <summary>  Background: There has been growing research interest in automated answering
of questions or generation of summary of free form text such as news article.
In order to implement this task, the computer should be able to identify the
sequence of events, duration of events, time at which event occurred and the
relationship type between event pairs, time pairs or event-time pairs. Specific
Problem: It is important to accurately identify the relationship type between
combinations of event and time before the temporal ordering of events can be
defined. The machine learning approach taken in Mani et. al (2006) provides an
accuracy of only 62.5 on the baseline data from TimeBank. The researchers used
maximum entropy classifier in their methodology. TimeML uses the TLINK
annotation to tag a relationship type between events and time. The time
complexity is quadratic when it comes to tagging documents with TLINK using
human annotation. This research proposes using decision tree and parsing to
improve the relationship type tagging. This research attempts to solve the gaps
in human annotation by automating the task of relationship type tagging in an
attempt to improve the accuracy of event and time relationship in annotated
documents. Scope information: The documents from the domain of news will be
used. The tagging will be performed within the same document and not across
documents. The relationship types will be identified only for a pair of event
and time and not a chain of events. The research focuses on documents tagged
using the TimeML specification which contains tags such as EVENT, TLINK, and
TIMEX. Each tag has attributes such as identifier, relation, POS, time etc.
</summary>
    <author>
      <name>Amol S Patwardhan</name>
    </author>
    <author>
      <name>Jacob Badeaux</name>
    </author>
    <author>
      <name> Siavash</name>
    </author>
    <author>
      <name>Gerald M Knapp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, Technical report, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06532v1</id>
    <updated>2016-07-22T00:20:09Z</updated>
    <published>2016-07-22T00:20:09Z</published>
    <title>Novel Word Embedding and Translation-based Language Modeling for
  Extractive Speech Summarization</title>
    <summary>  Word embedding methods revolve around learning continuous distributed vector
representations of words with neural networks, which can capture semantic
and/or syntactic cues, and in turn be used to induce similarity measures among
words, sentences and documents in context. Celebrated methods can be
categorized as prediction-based and count-based methods according to the
training objectives and model architectures. Their pros and cons have been
extensively analyzed and evaluated in recent studies, but there is relatively
less work continuing the line of research to develop an enhanced learning
method that brings together the advantages of the two model families. In
addition, the interpretation of the learned word representations still remains
somewhat opaque. Motivated by the observations and considering the pressing
need, this paper presents a novel method for learning the word representations,
which not only inherits the advantages of classic word embedding methods but
also offers a clearer and more rigorous interpretation of the learned word
representations. Built upon the proposed word embedding method, we further
formulate a translation-based language modeling framework for the extractive
speech summarization task. A series of empirical evaluations demonstrate the
effectiveness of the proposed word representation learning and language
modeling techniques in extractive speech summarization.
</summary>
    <author>
      <name>Kuan-Yu Chen</name>
    </author>
    <author>
      <name>Shih-Hung Liu</name>
    </author>
    <author>
      <name>Berlin Chen</name>
    </author>
    <author>
      <name>Hsin-Min Wang</name>
    </author>
    <author>
      <name>Hsin-Hsi Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06520v1</id>
    <updated>2016-07-21T22:26:20Z</updated>
    <published>2016-07-21T22:26:20Z</published>
    <title>Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
  Embeddings</title>
    <summary>  The blind application of machine learning runs the risk of amplifying biases
present in data. Such a danger is facing us with word embedding, a popular
framework to represent text data as vectors which has been used in many machine
learning and natural language processing tasks. We show that even word
embeddings trained on Google News articles exhibit female/male gender
stereotypes to a disturbing extent. This raises concerns because their
widespread use, as we describe, often tends to amplify these biases.
Geometrically, gender bias is first shown to be captured by a direction in the
word embedding. Second, gender neutral words are shown to be linearly separable
from gender definition words in the word embedding. Using these properties, we
provide a methodology for modifying an embedding to remove gender stereotypes,
such as the association between between the words receptionist and female,
while maintaining desired associations such as between the words queen and
female. We define metrics to quantify both direct and indirect gender biases in
embeddings, and develop algorithms to "debias" the embedding. Using
crowd-worker evaluation as well as standard benchmarks, we empirically
demonstrate that our algorithms significantly reduce gender bias in embeddings
while preserving the its useful properties such as the ability to cluster
related concepts and to solve analogy tasks. The resulting embeddings can be
used in applications without amplifying gender bias.
</summary>
    <author>
      <name>Tolga Bolukbasi</name>
    </author>
    <author>
      <name>Kai-Wei Chang</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <author>
      <name>Venkatesh Saligrama</name>
    </author>
    <author>
      <name>Adam Kalai</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06275v2</id>
    <updated>2016-09-01T10:56:45Z</updated>
    <published>2016-07-21T11:40:50Z</published>
    <title>Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain
  Factoid Question Answering</title>
    <summary>  While question answering (QA) with neural network, i.e. neural QA, has
achieved promising results in recent years, lacking of large scale real-word QA
dataset is still a challenge for developing and evaluating neural QA system. To
alleviate this problem, we propose a large scale human annotated real-world QA
dataset WebQA with more than 42k questions and 556k evidences. As existing
neural QA methods resolve QA either as sequence generation or
classification/ranking problem, they face challenges of expensive softmax
computation, unseen answers handling or separate candidate answer generation
component. In this work, we cast neural QA as a sequence labeling problem and
propose an end-to-end sequence labeling model, which overcomes all the above
challenges. Experimental results on WebQA show that our model outperforms the
baselines significantly with an F1 score of 74.69% with word-based input, and
the performance drops only 3.72 F1 points with more challenging character-based
input.
</summary>
    <author>
      <name>Peng Li</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Zhengyan He</name>
    </author>
    <author>
      <name>Xuguang Wang</name>
    </author>
    <author>
      <name>Ying Cao</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, withdraw experimental results on CNN/Daily Mail
  datasets</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06275v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06275v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06264v1</id>
    <updated>2016-07-21T11:06:05Z</updated>
    <published>2016-07-21T11:06:05Z</published>
    <title>Left/Right Hand Segmentation in Egocentric Videos</title>
    <summary>  Wearable cameras allow people to record their daily activities from a
user-centered (First Person Vision) perspective. Due to their favorable
location, wearable cameras frequently capture the hands of the user, and may
thus represent a promising user-machine interaction tool for different
applications. Existent First Person Vision methods handle hand segmentation as
a background-foreground problem, ignoring two important facts: i) hands are not
a single "skin-like" moving element, but a pair of interacting cooperative
entities, ii) close hand interactions may lead to hand-to-hand occlusions and,
as a consequence, create a single hand-like segment. These facts complicate a
proper understanding of hand movements and interactions. Our approach extends
traditional background-foreground strategies, by including a
hand-identification step (left-right) based on a Maxwell distribution of angle
and position. Hand-to-hand occlusions are addressed by exploiting temporal
superpixels. The experimental results show that, in addition to a reliable
left/right hand-segmentation, our approach considerably improves the
traditional background-foreground hand-segmentation.
</summary>
    <author>
      <name>Alejandro Betancourt</name>
    </author>
    <author>
      <name>Pietro Morerio</name>
    </author>
    <author>
      <name>Emilia Barakova</name>
    </author>
    <author>
      <name>Lucio Marcenaro</name>
    </author>
    <author>
      <name>Matthias Rauterberg</name>
    </author>
    <author>
      <name>Carlo Regazzoni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cviu.2016.09.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cviu.2016.09.005" rel="related"/>
    <link href="http://arxiv.org/abs/1607.06264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06198v1</id>
    <updated>2016-07-21T05:31:04Z</updated>
    <published>2016-07-21T05:31:04Z</published>
    <title>Supervised Adverse Drug Reaction Signalling Framework Imitating Bradford
  Hill's Causality Considerations</title>
    <summary>  Big longitudinal observational medical data potentially hold a wealth of
information and have been recognised as potential sources for gaining new drug
safety knowledge. Unfortunately there are many complexities and underlying
issues when analysing longitudinal observational data. Due to these
complexities, existing methods for large-scale detection of negative side
effects using observational data all tend to have issues distinguishing between
association and causality. New methods that can better discriminate causal and
non-causal relationships need to be developed to fully utilise the data. In
this paper we propose using a set of causality considerations developed by the
epidemiologist Bradford Hill as a basis for engineering features that enable
the application of supervised learning for the problem of detecting negative
side effects. The Bradford Hill considerations look at various perspectives of
a drug and outcome relationship to determine whether it shows causal traits. We
taught a classifier to find patterns within these perspectives and it learned
to discriminate between association and causality. The novelty of this research
is the combination of supervised learning and Bradford Hill's causality
considerations to automate the Bradford Hill's causality assessment. We
evaluated the framework on a drug safety gold standard know as the
observational medical outcomes partnership's nonspecified association reference
set. The methodology obtained excellent discriminate ability with area under
the curves ranging between 0.792-0.940 (existing method optimal: 0.73) and a
mean average precision of 0.640 (existing method optimal: 0.141). The proposed
features can be calculated efficiently and be readily updated, making the
framework suitable for big observational data.
</summary>
    <author>
      <name>Jenna Marie Reps</name>
    </author>
    <author>
      <name>Jonathan M. Garibaldi</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Jack E. Gibson</name>
    </author>
    <author>
      <name>Richard B. Hubbard</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Biomedical Informatics, 56 , pp. 356-368, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.06198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06187v1</id>
    <updated>2016-07-21T04:40:14Z</updated>
    <published>2016-07-21T04:40:14Z</published>
    <title>Exploring Differences in Interpretation of Words Essential in Medical
  Expert-Patient Communication</title>
    <summary>  In the context of cancer treatment and surgery, quality of life assessment is
a crucial part of determining treatment success and viability. In order to
assess it, patients completed questionnaires which employ words to capture
aspects of patients well-being are the norm. As the results of these
questionnaires are often used to assess patient progress and to determine
future treatment options, it is important to establish that the words used are
interpreted in the same way by both patients and medical professionals. In this
paper, we capture and model patients perceptions and associated uncertainty
about the words used to describe the level of their physical function used in
the highly common (in Sarcoma Services) Toronto Extremity Salvage Score (TESS)
questionnaire. The paper provides detail about the interval-valued data capture
as well as the subsequent modelling of the data using fuzzy sets. Based on an
initial sample of participants, we use Jaccard similarity on the resulting
words models to show that there may be considerable differences in the
interpretation of commonly used questionnaire terms, thus presenting a very
real risk of miscommunication between patients and medical professionals as
well as within the group of medical professionals.
</summary>
    <author>
      <name>Javier Navarro</name>
    </author>
    <author>
      <name>Christian Wagner</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Lynsey Green</name>
    </author>
    <author>
      <name>Robert Ashford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2016),
  24-29 July 2016, Vancouver, Canada, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06186v1</id>
    <updated>2016-07-21T04:36:23Z</updated>
    <published>2016-07-21T04:36:23Z</published>
    <title>Applying Interval Type-2 Fuzzy Rule Based Classifiers Through a
  Cluster-Based Class Representation</title>
    <summary>  Fuzzy Rule-Based Classification Systems (FRBCSs) have the potential to
provide so-called interpretable classifiers, i.e. classifiers which can be
introspective, understood, validated and augmented by human experts by relying
on fuzzy-set based rules. This paper builds on prior work for interval type-2
fuzzy set based FRBCs where the fuzzy sets and rules of the classifier are
generated using an initial clustering stage. By introducing Subtractive
Clustering in order to identify multiple cluster prototypes, the proposed
approach has the potential to deliver improved classification performance while
maintaining good interpretability, i.e. without resulting in an excessive
number of rules. The paper provides a detailed overview of the proposed FRBC
framework, followed by a series of exploratory experiments on both linearly and
non-linearly separable datasets, comparing results to existing rule-based and
SVM approaches. Overall, initial results indicate that the approach enables
comparable classification performance to non rule-based classifiers such as
SVM, while often achieving this with a very small number of rules.
</summary>
    <author>
      <name>Javier Navarro</name>
    </author>
    <author>
      <name>Christian Wagner</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2015 IEEE Symposium Series on Computational Intelligence, pp.
  1816-1823, IEEE, 2015, ISBN: 978-1-4799-7560-0</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06025v1</id>
    <updated>2016-07-20T16:59:21Z</updated>
    <published>2016-07-20T16:59:21Z</published>
    <title>Constructing a Natural Language Inference Dataset using Generative
  Neural Networks</title>
    <summary>  Natural Language Inference is an important task for Natural Language
Understanding. It is concerned with classifying the logical relation between
two sentences. In this paper, we propose several text generative neural
networks for constructing Natural Language Inference datasets suitable for
training classifiers. To evaluate the models, we propose a new metric - the
accuracy of the classifier trained on the generated dataset. The accuracy
obtained with our best generative model is only 2.7% lower than the accuracy of
the classifier trained on the original, manually constructed dataset. The model
learns a mapping embedding for each training example. By comparing various
metrics we show that datasets that obtain higher ROUGE or METEOR scores do not
necessarily yield higher classification accuracies. We also provide analysis of
what are the characteristics of a good dataset including the distinguishability
of the generated datasets from the original one.
</summary>
    <author>
      <name>Janez Starc</name>
    </author>
    <author>
      <name>Dunja Mladenić</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05968v1</id>
    <updated>2016-07-20T14:15:24Z</updated>
    <published>2016-07-20T14:15:24Z</published>
    <title>Robust Natural Language Processing - Combining Reasoning, Cognitive
  Semantics and Construction Grammar for Spatial Language</title>
    <summary>  We present a system for generating and understanding of dynamic and static
spatial relations in robotic interaction setups. Robots describe an environment
of moving blocks using English phrases that include spatial relations such as
"across" and "in front of". We evaluate the system in robot-robot interactions
and show that the system can robustly deal with visual perception errors,
language omissions and ungrammatical utterances.
</summary>
    <author>
      <name>Michael Spranger</name>
    </author>
    <author>
      <name>Jakob Suchan</name>
    </author>
    <author>
      <name>Mehul Bhatt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in IJCAI'16: Proceedings of the 25th international joint conference
  on Artificial intelligence, Palo Alto, 2016. AAAI Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05913v1</id>
    <updated>2016-07-20T11:02:16Z</updated>
    <published>2016-07-20T11:02:16Z</published>
    <title>Optimising Rule-Based Classification in Temporal Data</title>
    <summary>  This study optimises manually derived rule-based expert system classification
of objects according to changes in their properties over time. One of the key
challenges that this study tries to address is how to classify objects that
exhibit changes in their behaviour over time, for example how to classify
companies' share price stability over a period of time or how to classify
students' preferences for subjects while they are progressing through school. A
specific case the paper considers is the strategy of players in public goods
games (as common in economics) across multiple consecutive games. Initial
classification starts from expert definitions specifying class allocation for
players based on aggregated attributes of the temporal data. Based on these
initial classifications, the optimisation process tries to find an improved
classifier which produces the best possible compact classes of objects
(players) for every time point in the temporal data. The compactness of the
classes is measured by a cost function based on internal cluster indices like
the Dunn Index, distance measures like Euclidean distance or statistically
derived measures like standard deviation. The paper discusses the approach in
the context of incorporating changing player strategies in the aforementioned
public good games, where common classification approaches so far do not
consider such changes in behaviour resulting from learning or in-game
experience. By using the proposed process for classifying temporal data and the
actual players' contribution during the games, we aim to produce a more refined
classification which in turn may inform the interpretation of public goods game
data.
</summary>
    <author>
      <name>Polla Fattah</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Christian Wagner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ZANCO Journal of Pure and Applied Sciences, 28 (2), pp. 135-146,
  2016, ISSN: 2412-3986</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05912v1</id>
    <updated>2016-07-20T10:57:14Z</updated>
    <published>2016-07-20T10:57:14Z</published>
    <title>Simulating user learning in authoritative technology adoption: An agent
  based model for council-led smart meter deployment planning in the UK</title>
    <summary>  How do technology users effectively transit from having zero knowledge about
a technology to making the best use of it after an authoritative technology
adoption? This post-adoption user learning has received little research
attention in technology management literature. In this paper we investigate
user learning in authoritative technology adoption by developing an agent-based
model using the case of council-led smart meter deployment in the UK City of
Leeds. Energy consumers gain experience of using smart meters based on the
learning curve in behavioural learning. With the agent-based model we carry out
experiments to validate the model and test different energy interventions that
local authorities can use to facilitate energy consumers' learning and maintain
their continuous use of the technology. Our results show that the easier energy
consumers become experienced, the more energy-efficient they are and the more
energy saving they can achieve; encouraging energy consumers' contacts via
various informational means can facilitate their learning; and developing and
maintaining their positive attitude toward smart metering can enable them to
use the technology continuously. Contributions and energy policy/intervention
implications are discussed in this paper.
</summary>
    <author>
      <name>Tao Zhang</name>
    </author>
    <author>
      <name>Peer-Olaf Siebers</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technological Forecasting and Social Change, 106 , pp. 74-84, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05909v1</id>
    <updated>2016-07-20T10:52:17Z</updated>
    <published>2016-07-20T10:52:17Z</published>
    <title>Supervised Anomaly Detection in Uncertain Pseudoperiodic Data Streams</title>
    <summary>  Uncertain data streams have been widely generated in many Web applications.
The uncertainty in data streams makes anomaly detection from sensor data
streams far more challenging. In this paper, we present a novel framework that
supports anomaly detection in uncertain data streams. The proposed framework
adopts an efficient uncertainty pre-processing procedure to identify and
eliminate uncertainties in data streams. Based on the corrected data streams,
we develop effective period pattern recognition and feature extraction
techniques to improve the computational efficiency. We use classification
methods for anomaly detection in the corrected data stream. We also empirically
show that the proposed approach shows a high accuracy of anomaly detection on a
number of real datasets.
</summary>
    <author>
      <name>Jiangang Ma</name>
    </author>
    <author>
      <name>Le Sun</name>
    </author>
    <author>
      <name>Hua Wang</name>
    </author>
    <author>
      <name>Yanchun Zhang</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Internet Technology (TOIT), 16 (1 (4)), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05906v1</id>
    <updated>2016-07-20T10:45:57Z</updated>
    <published>2016-07-20T10:45:57Z</published>
    <title>Refining adverse drug reaction signals by incorporating interaction
  variables identified using emergent pattern mining</title>
    <summary>  Purpose: To develop a framework for identifying and incorporating candidate
confounding interaction terms into a regularised cox regression analysis to
refine adverse drug reaction signals obtained via longitudinal observational
data. Methods: We considered six drug families that are commonly associated
with myocardial infarction in observational healthcare data, but where the
causal relationship ground truth is known (adverse drug reaction or not). We
applied emergent pattern mining to find itemsets of drugs and medical events
that are associated with the development of myocardial infarction. These are
the candidate confounding interaction terms. We then implemented a cohort study
design using regularised cox regression that incorporated and accounted for the
candidate confounding interaction terms. Results The methodology was able to
account for signals generated due to confounding and a cox regression with
elastic net regularisation correctly ranked the drug families known to be true
adverse drug reactions above those.
</summary>
    <author>
      <name>Jenna M. Reps</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Richard B. Hubbard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computers in Biology and Medicine, 69 , pp. 61-70, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05888v1</id>
    <updated>2016-07-20T09:47:31Z</updated>
    <published>2016-07-20T09:47:31Z</published>
    <title>Juxtaposition of System Dynamics and Agent-based Simulation for a Case
  Study in Immunosenescence</title>
    <summary>  Advances in healthcare and in the quality of life significantly increase
human life expectancy. With the ageing of populations, new un-faced challenges
are brought to science. The human body is naturally selected to be
well-functioning until the age of reproduction to keep the species alive.
However, as the lifespan extends, unseen problems due to the body deterioration
emerge. There are several age-related diseases with no appropriate treatment;
therefore, the complex ageing phenomena needs further understanding.
Immunosenescence, the ageing of the immune system, is highly correlated to the
negative effects of ageing, such as the increase of auto-inflammatory diseases
and decrease in responsiveness to new diseases. Besides clinical and
mathematical tools, we believe there is opportunity to further exploit
simulation tools to understand immunosenescence. Compared to real-world
experimentation, benefits include time and cost effectiveness due to the
laborious, resource-intensiveness of the biological environment and the
possibility of conducting experiments without ethic restrictions. Contrasted
with mathematical models, simulation modelling is more suitable for
representing complex systems and emergence. In addition, there is the belief
that simulation models are easier to communicate in interdisciplinary contexts.
Our work investigates the usefulness of simulations to understand
immunosenescence by employing two different simulation methods, agent-based and
system dynamics simulation, to a case study of immune cells depletion with age.
</summary>
    <author>
      <name>Grazziela P. Figueredo</name>
    </author>
    <author>
      <name>Peer-Olaf Siebers</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Amanda Whitbrook</name>
    </author>
    <author>
      <name>Jonathan M. Garibaldi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PLOS One, 10 (3), 2015, ISBN: e0118359</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05869v1</id>
    <updated>2016-07-20T09:03:17Z</updated>
    <published>2016-07-20T09:03:17Z</published>
    <title>Indebted households profiling: a knowledge discovery from database
  approach</title>
    <summary>  A major challenge in consumer credit risk portfolio management is to classify
households according to their risk profile. In order to build such risk
profiles it is necessary to employ an approach that analyses data
systematically in order to detect important relationships, interactions,
dependencies and associations amongst the available continuous and categorical
variables altogether and accurately generate profiles of most interesting
household segments according to their credit risk. The objective of this work
is to employ a knowledge discovery from database process to identify groups of
indebted households and describe their profiles using a database collected by
the Consumer Credit Counselling Service (CCCS) in the UK. Employing a framework
that allows the usage of both categorical and continuous data altogether to
find hidden structures in unlabelled data it was established the ideal number
of clusters and such clusters were described in order to identify the
households who exhibit a high propensity of excessive debt levels.
</summary>
    <author>
      <name>Rodrigo Scarpel</name>
    </author>
    <author>
      <name>Alexandros Ladas</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Annals of Data Science, 2 (1), pp. 43-59, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05845v1</id>
    <updated>2016-07-20T07:42:52Z</updated>
    <published>2016-07-20T07:42:52Z</published>
    <title>Identifying Candidate Risk Factors for Prescription Drug Side Effects
  using Causal Contrast Set Mining</title>
    <summary>  Big longitudinal observational databases present the opportunity to extract
new knowledge in a cost effective manner. Unfortunately, the ability of these
databases to be used for causal inference is limited due to the passive way in
which the data are collected resulting in various forms of bias. In this paper
we investigate a method that can overcome these limitations and determine
causal contrast set rules efficiently from big data. In particular, we present
a new methodology for the purpose of identifying risk factors that increase a
patients likelihood of experiencing the known rare side effect of renal failure
after ingesting aminosalicylates. The results show that the methodology was
able to identify previously researched risk factors such as being prescribed
diuretics and highlighted that patients with a higher than average risk of
renal failure may be even more susceptible to experiencing it as a side effect
after ingesting aminosalicylates.
</summary>
    <author>
      <name>Jenna Reps</name>
    </author>
    <author>
      <name>Zhaoyang Guo</name>
    </author>
    <author>
      <name>Haoyue Zhu</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Health Information Science (4th International Conference, HIS 2015,
  Melbourne, Australia, May 28-30), pp. 45-55, Lecture Notes in Computer
  Science, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06332v1</id>
    <updated>2016-07-20T07:30:43Z</updated>
    <published>2016-07-20T07:30:43Z</published>
    <title>Modelling Office Energy Consumption: An Agent Based Approach</title>
    <summary>  In this paper, we develop an agent-based model which integrates four
important elements, i.e. organisational energy management policies/regulations,
energy management technologies, electric appliances and equipment, and human
behaviour, based on a case study, to simulate the energy consumption in office
buildings. With the model, we test the effectiveness of different energy
management strategies, and solve practical office energy consumption problems.
This paper theoretically contributes to an integration of four elements
involved in the complex organisational issue of office energy consumption, and
practically contributes to an application of agent-based approach for office
building energy consumption study.
</summary>
    <author>
      <name>Tao Zhang</name>
    </author>
    <author>
      <name>Peer-Olaf Siebers</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 3rd World Congress on Social Simulation
  (WCSS2010), 5-9 Sep, Kassel, Germany, 2010. arXiv admin note: substantial
  text overlap with arXiv:1305.7437</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05810v1</id>
    <updated>2016-07-20T03:47:19Z</updated>
    <published>2016-07-20T03:47:19Z</published>
    <title>You want to survive the data deluge: Be careful, Computational
  Intelligence will not serve you as a rescue boat</title>
    <summary>  We are at the dawn of a new era, where advances in computer power, broadband
communication and digital sensor technologies have led to an unprecedented
flood of data inundating our surrounding. It is generally believed that means
such as Computational Intelligence will help to outlive these tough times.
However, these hopes are improperly high. Computational Intelligence is a
surprising composition of two mutually exclusive and contradicting constituents
that could be coupled only if you disregard and neglect their controversies:
"Computational" implies reliance on data processing and "Intelligence" implies
reliance on information processing. Only those who are indifferent to
data-information discrepancy can believe that such a combination can be viable.
We do not believe in miracles, so we will try to share with you our
reservations.
</summary>
    <author>
      <name>Emanuel Diamant</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Oral presentation at the ICNSC 2016 Conference, Mexico City, April
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05809v1</id>
    <updated>2016-07-20T03:25:31Z</updated>
    <published>2016-07-20T03:25:31Z</published>
    <title>Neural Contextual Conversation Learning with Labeled Question-Answering
  Pairs</title>
    <summary>  Neural conversational models tend to produce generic or safe responses in
different contexts, e.g., reply \textit{"Of course"} to narrative statements or
\textit{"I don't know"} to questions. In this paper, we propose an end-to-end
approach to avoid such problem in neural generative models. Additional memory
mechanisms have been introduced to standard sequence-to-sequence (seq2seq)
models, so that context can be considered while generating sentences. Three
seq2seq models, which memorize a fix-sized contextual vector from hidden input,
hidden input/output and a gated contextual attention structure respectively,
have been trained and tested on a dataset of labeled question-answering pairs
in Chinese. The model with contextual attention outperforms others including
the state-of-the-art seq2seq models on perplexity test. The novel contextual
model generates diverse and robust responses, and is able to carry out
conversations on a wide range of topics appropriately.
</summary>
    <author>
      <name>Kun Xiong</name>
    </author>
    <author>
      <name>Anqi Cui</name>
    </author>
    <author>
      <name>Zefeng Zhang</name>
    </author>
    <author>
      <name>Ming Li</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05954v1</id>
    <updated>2016-07-19T15:16:56Z</updated>
    <published>2016-07-19T15:16:56Z</published>
    <title>On the estimation of stellar parameters with uncertainty prediction from
  Generative Artificial Neural Networks: application to Gaia RVS simulated
  spectra</title>
    <summary>  Aims. We present an innovative artificial neural network (ANN) architecture,
called Generative ANN (GANN), that computes the forward model, that is it
learns the function that relates the unknown outputs (stellar atmospheric
parameters, in this case) to the given inputs (spectra). Such a model can be
integrated in a Bayesian framework to estimate the posterior distribution of
the outputs. Methods. The architecture of the GANN follows the same scheme as a
normal ANN, but with the inputs and outputs inverted. We train the network with
the set of atmospheric parameters (Teff, logg, [Fe/H] and [alpha/Fe]),
obtaining the stellar spectra for such inputs. The residuals between the
spectra in the grid and the estimated spectra are minimized using a validation
dataset to keep solutions as general as possible. Results. The performance of
both conventional ANNs and GANNs to estimate the stellar parameters as a
function of the star brightness is presented and compared for different
Galactic populations. GANNs provide significantly improved parameterizations
for early and intermediate spectral types with rich and intermediate
metallicities. The behaviour of both algorithms is very similar for our sample
of late-type stars, obtaining residuals in the derivation of [Fe/H] and
[alpha/Fe] below 0.1dex for stars with Gaia magnitude Grvs&lt;12, which accounts
for a number in the order of four million stars to be observed by the Radial
Velocity Spectrograph of the Gaia satellite. Conclusions. Uncertainty
estimation of computed astrophysical parameters is crucial for the validation
of the parameterization itself and for the subsequent exploitation by the
astronomical community. GANNs produce not only the parameters for a given
spectrum, but a goodness-of-fit between the observed spectrum and the predicted
one for a given set of parameters. Moreover, they allow us to obtain the full
posterior distribution...
</summary>
    <author>
      <name>C. Dafonte</name>
    </author>
    <author>
      <name>D. Fustes</name>
    </author>
    <author>
      <name>M. Manteiga</name>
    </author>
    <author>
      <name>D. Garabato</name>
    </author>
    <author>
      <name>M. A. Alvarez</name>
    </author>
    <author>
      <name>A. Ulla</name>
    </author>
    <author>
      <name>C. Allende Prieto</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05540v1</id>
    <updated>2016-07-19T12:19:35Z</updated>
    <published>2016-07-19T12:19:35Z</published>
    <title>Exploiting Vagueness for Multi-Agent Consensus</title>
    <summary>  A framework for consensus modelling is introduced using Kleene's three valued
logic as a means to express vagueness in agents' beliefs. Explicitly borderline
cases are inherent to propositions involving vague concepts where sentences of
a propositional language may be absolutely true, absolutely false or
borderline. By exploiting these intermediate truth values, we can allow agents
to adopt a more vague interpretation of underlying concepts in order to weaken
their beliefs and reduce the levels of inconsistency, so as to achieve
consensus. We consider a consensus combination operation which results in
agents adopting the borderline truth value as a shared viewpoint if they are in
direct conflict. Simulation experiments are presented which show that applying
this operator to agents chosen at random (subject to a consistency threshold)
from a population, with initially diverse opinions, results in convergence to a
smaller set of more precise shared beliefs. Furthermore, if the choice of
agents for combination is dependent on the payoff of their beliefs, this acting
as a proxy for performance or usefulness, then the system converges to beliefs
which, on average, have higher payoff.
</summary>
    <author>
      <name>Michael Crosscombe</name>
    </author>
    <author>
      <name>Jonathan Lawry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the second international workshop on Smart Simulation
  and Modelling for Complex Systems (SSMCS) at IJCAI 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05387v1</id>
    <updated>2016-07-19T03:09:31Z</updated>
    <published>2016-07-19T03:09:31Z</published>
    <title>Generating Images Part by Part with Composite Generative Adversarial
  Networks</title>
    <summary>  Image generation remains a fundamental problem in artificial intelligence in
general and deep learning in specific. The generative adversarial network (GAN)
was successful in generating high quality samples of natural images. We propose
a model called composite generative adversarial network, that reveals the
complex structure of images with multiple generators in which each generator
generates some part of the image. Those parts are combined by alpha blending
process to create a new single image. It can generate, for example, background
and face sequentially with two generators, after training on face dataset.
Training was done in an unsupervised way without any labels about what each
generator should generate. We found possibilities of learning the structure by
using this generative model empirically.
</summary>
    <author>
      <name>Hanock Kwak</name>
    </author>
    <author>
      <name>Byoung-Tak Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2016 Workshop on Deep Learning for Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05351v2</id>
    <updated>2016-08-15T12:06:28Z</updated>
    <published>2016-07-18T23:23:21Z</published>
    <title>Towards Analytics Aware Ontology Based Access to Static and Streaming
  Data (Extended Version)</title>
    <summary>  Real-time analytics that requires integration and aggregation of
heterogeneous and distributed streaming and static data is a typical task in
many industrial scenarios such as diagnostics of turbines in Siemens. OBDA
approach has a great potential to facilitate such tasks; however, it has a
number of limitations in dealing with analytics that restrict its use in
important industrial applications. Based on our experience with Siemens, we
argue that in order to overcome those limitations OBDA should be extended and
become analytics, source, and cost aware. In this work we propose such an
extension. In particular, we propose an ontology, mapping, and query language
for OBDA, where aggregate and other analytical functions are first class
citizens. Moreover, we develop query optimisation techniques that allow to
efficiently process analytical tasks over static and streaming data. We
implement our approach in a system and evaluate our system with Siemens turbine
data.
</summary>
    <author>
      <name>Evgeny Kharlamov</name>
    </author>
    <author>
      <name>Yannis Kotidis</name>
    </author>
    <author>
      <name>Theofilos Mailis</name>
    </author>
    <author>
      <name>Christian Neuenstadt</name>
    </author>
    <author>
      <name>Charalampos Nikolaou</name>
    </author>
    <author>
      <name>Özgür Özcep</name>
    </author>
    <author>
      <name>Christoforos Svingos</name>
    </author>
    <author>
      <name>Dmitriy Zheleznyakov</name>
    </author>
    <author>
      <name>Sebastian Brandt</name>
    </author>
    <author>
      <name>Ian Horrocks</name>
    </author>
    <author>
      <name>Yannis Ioannidis</name>
    </author>
    <author>
      <name>Steffen Lamparter</name>
    </author>
    <author>
      <name>Ralf Möller</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05601v1</id>
    <updated>2016-07-18T16:53:14Z</updated>
    <published>2016-07-18T16:53:14Z</published>
    <title>An Event Grouping Based Algorithm for University Course Timetabling
  Problem</title>
    <summary>  This paper presents the study of an event grouping based algorithm for a
university course timetabling problem. Several publications which discuss the
problem and some approaches for its solution are analyzed. The grouping of
events in groups with an equal number of events in each group is not applicable
to all input data sets. For this reason, a universal approach to all possible
groupings of events in commensurate in size groups is proposed here. Also, an
implementation of an algorithm based on this approach is presented. The
methodology, conditions and the objectives of the experiment are described. The
experimental results are analyzed and the ensuing conclusions are stated. The
future guidelines for further research are formulated.
</summary>
    <author>
      <name>Velin Kralev</name>
    </author>
    <author>
      <name>Radoslava Kraleva</name>
    </author>
    <author>
      <name>Borislav Yurukov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures, 9 tables, International Journal of Computer
  Science and Information Security (IJCSIS), PaperID 31051699, Vol. 14, No. 06,
  June 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05174v1</id>
    <updated>2016-07-18T16:44:34Z</updated>
    <published>2016-07-18T16:44:34Z</published>
    <title>Is spoken language all-or-nothing? Implications for future speech-based
  human-machine interaction</title>
    <summary>  Recent years have seen significant market penetration for voice-based
personal assistants such as Apple's Siri. However, despite this success, user
take-up is frustratingly low. This position paper argues that there is a
habitability gap caused by the inevitable mismatch between the capabilities and
expectations of human users and the features and benefits provided by
contemporary technology. Suggestions are made as to how such problems might be
mitigated, but a more worrisome question emerges: "is spoken language
all-or-nothing"? The answer, based on contemporary views on the special nature
of (spoken) language, is that there may indeed be a fundamental limit to the
interaction that can take place between mismatched interlocutors (such as
humans and machines). However, it is concluded that interactions between native
and non-native speakers, or between adults and children, or even between humans
and dogs, might provide critical inspiration for the design of future
speech-based human-machine interaction.
</summary>
    <author>
      <name>Roger K. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in K. Jokinen &amp; G. Wilcock (Eds.), Dialogues with Social
  Robots - Enablements, Analyses, and Evaluation. Springer Lecture Notes in
  Electrical Engineering (LNEE)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05077v1</id>
    <updated>2016-07-18T13:55:54Z</updated>
    <published>2016-07-18T13:55:54Z</published>
    <title>Playing Atari Games with Deep Reinforcement Learning and Human
  Checkpoint Replay</title>
    <summary>  This paper introduces a novel method for learning how to play the most
difficult Atari 2600 games from the Arcade Learning Environment using deep
reinforcement learning. The proposed method, human checkpoint replay, consists
in using checkpoints sampled from human gameplay as starting points for the
learning process. This is meant to compensate for the difficulties of current
exploration strategies, such as epsilon-greedy, to find successful control
policies in games with sparse rewards. Like other deep reinforcement learning
architectures, our model uses a convolutional neural network that receives only
raw pixel inputs to estimate the state value function. We tested our method on
Montezuma's Revenge and Private Eye, two of the most challenging games from the
Atari platform. The results we obtained show a substantial improvement compared
to previous learning approaches, as well as over a random player. We also
propose a method for training deep reinforcement learning agents using human
gameplay experience, which we call human experience replay.
</summary>
    <author>
      <name>Ionel-Alexandru Hosu</name>
    </author>
    <author>
      <name>Traian Rebedea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, EGPAI 2016 - Evaluating General Purpose AI,
  workshop held in conjunction with ECAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05023v1</id>
    <updated>2016-07-18T11:28:11Z</updated>
    <published>2016-07-18T11:28:11Z</published>
    <title>Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?</title>
    <summary>  In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?
</summary>
    <author>
      <name>Gabriella Panuccio</name>
    </author>
    <author>
      <name>Marianna Semprini</name>
    </author>
    <author>
      <name>Lorenzo Natale</name>
    </author>
    <author>
      <name>Michela Chiappalone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Number of pages: 15 Words in abstract: 49 Words in main text: 3265
  Number of figures: 5 Number of references: 25 Keywords: artificial
  intelligence, biohybrid system, closed-loop control, functional brain repair</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04917v1</id>
    <updated>2016-07-17T21:49:00Z</updated>
    <published>2016-07-17T21:49:00Z</published>
    <title>Piecewise convexity of artificial neural networks</title>
    <summary>  Although artificial neural networks have shown great promise in applications
ranging from computer vision to speech recognition, there remains considerable
practical and theoretical difficulty in optimizing their parameters. The
seemingly unreasonable success of gradient descent methods in minimizing these
non-convex functions remains poorly understood. In this work we offer some
theoretical guarantees concerning networks with continuous piecewise affine
activation functions, which have in recent years become the norm. We prove
three main results. Firstly, that the network is piecewise convex as a function
of the input data. Secondly, that the network, considered as a function of the
parameters in a single layer, all others held constant, is again piecewise
convex. Finally, that the network as a function of all its parameters is
piecewise multi-convex, a generalization of biconvexity. Accordingly, we show
that any point to which gradient descent converges is a local minimum of some
piece. Thus gradient descent converges to non-minima only at the boundaries of
pieces. These results might offer some insights into the effectiveness of
gradient descent methods in optimizing this class of networks.
</summary>
    <author>
      <name>Blaine Rister</name>
    </author>
    <link href="http://arxiv.org/abs/1607.04917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04817v1</id>
    <updated>2016-07-17T01:41:16Z</updated>
    <published>2016-07-17T01:41:16Z</published>
    <title>Global Continuous Optimization with Error Bound and Fast Convergence</title>
    <summary>  This paper considers global optimization with a black-box unknown objective
function that can be non-convex and non-differentiable. Such a difficult
optimization problem arises in many real-world applications, such as parameter
tuning in machine learning, engineering design problem, and planning with a
complex physics simulator. This paper proposes a new global optimization
algorithm, called Locally Oriented Global Optimization (LOGO), to aim for both
fast convergence in practice and finite-time error bound in theory. The
advantage and usage of the new algorithm are illustrated via theoretical
analysis and an experiment conducted with 11 benchmark test functions. Further,
we modify the LOGO algorithm to specifically solve a planning problem via
policy search with continuous state/action space and long time horizon while
maintaining its finite-time error bound. We apply the proposed planning method
to accident management of a nuclear power plant. The result of the application
study demonstrates the practical utility of our method.
</summary>
    <author>
      <name>Kenji Kawaguchi</name>
    </author>
    <author>
      <name>Yu Maruyama</name>
    </author>
    <author>
      <name>Xiaoyu Zheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1613/jair.4742</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1613/jair.4742" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Artificial Intelligence Research, volume 56, pages
  153-195 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.04817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04809v1</id>
    <updated>2016-07-16T23:42:44Z</updated>
    <published>2016-07-16T23:42:44Z</published>
    <title>Knowledge Representation on the Web revisited: Tools for Prototype Based
  Ontologies</title>
    <summary>  In recent years RDF and OWL have become the most common knowledge
representation languages in use on the Web, propelled by the recommendation of
the W3C. In this paper we present a practical implementation of a different
kind of knowledge representation based on Prototypes. In detail, we present a
concrete syntax easily and effectively parsable by applications. We also
present extensible implementations of a prototype knowledge base, specifically
designed for storage of Prototypes. These implementations are written in Java
and can be extended by using the implementation as a library. Alternatively,
the software can be deployed as such. Further, results of benchmarks for both
local and web deployment are presented. This paper augments a research paper,
in which we describe the more theoretical aspects of our Prototype system.
</summary>
    <author>
      <name>Michael Cochez</name>
    </author>
    <author>
      <name>Stefan Decker</name>
    </author>
    <author>
      <name>Eric Prud'hommeaux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Related software available from
  https://github.com/miselico/knowledgebase/</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04379v1</id>
    <updated>2016-07-15T04:28:55Z</updated>
    <published>2016-07-15T04:28:55Z</published>
    <title>DeepQA: Improving the estimation of single protein model quality with
  deep belief networks</title>
    <summary>  Protein quality assessment (QA) by ranking and selecting protein models has
long been viewed as one of the major challenges for protein tertiary structure
prediction. Especially, estimating the quality of a single protein model, which
is important for selecting a few good models out of a large model pool
consisting of mostly low-quality models, is still a largely unsolved problem.
We introduce a novel single-model quality assessment method DeepQA based on
deep belief network that utilizes a number of selected features describing the
quality of a model from different perspectives, such as energy, physio-chemical
characteristics, and structural information. The deep belief network is trained
on several large datasets consisting of models from the Critical Assessment of
Protein Structure Prediction (CASP) experiments, several publicly available
datasets, and models generated by our in-house ab initio method. Our experiment
demonstrate that deep belief network has better performance compared to Support
Vector Machines and Neural Networks on the protein model quality assessment
problem, and our method DeepQA achieves the state-of-the-art performance on
CASP11 dataset. It also outperformed two well-established methods in selecting
good outlier models from a large set of models of mostly low quality generated
by ab initio modeling methods. DeepQA is a useful tool for protein single model
quality assessment and protein structure prediction. The source code,
executable, document and training/test datasets of DeepQA for Linux is freely
available to non-commercial users at http://cactus.rnet.missouri.edu/DeepQA/.
</summary>
    <author>
      <name>Renzhi Cao</name>
    </author>
    <author>
      <name>Debswapna Bhattacharya</name>
    </author>
    <author>
      <name>Jie Hou</name>
    </author>
    <author>
      <name>Jianlin Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 figure, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04376v1</id>
    <updated>2016-07-15T04:19:31Z</updated>
    <published>2016-07-15T04:19:31Z</published>
    <title>Intrinsically Motivated Multimodal Structure Learning</title>
    <summary>  We present a long-term intrinsically motivated structure learning method for
modeling transition dynamics during controlled interactions between a robot and
semi-permanent structures in the world. In particular, we discuss how
partially-observable state is represented using distributions over a Markovian
state and build models of objects that predict how state distributions change
in response to interactions with such objects. These structures serve as the
basis for a number of possible future tasks defined as Markov Decision
Processes (MDPs). The approach is an example of a structure learning technique
applied to a multimodal affordance representation that yields a population of
forward models for use in planning. We evaluate the approach using experiments
on a bimanual mobile manipulator (uBot-6) that show the performance of model
acquisition as the number of transition actions increases.
</summary>
    <author>
      <name>Jay Ming Wong</name>
    </author>
    <author>
      <name>Roderic A. Grupen</name>
    </author>
    <link href="http://arxiv.org/abs/1607.04376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04373v1</id>
    <updated>2016-07-15T03:35:56Z</updated>
    <published>2016-07-15T03:35:56Z</published>
    <title>Vista: A Visually, Socially, and Temporally-aware Model for Artistic
  Recommendation</title>
    <summary>  Understanding users' interactions with highly subjective content---like
artistic images---is challenging due to the complex semantics that guide our
preferences. On the one hand one has to overcome `standard' recommender systems
challenges, such as dealing with large, sparse, and long-tailed datasets. On
the other, several new challenges present themselves, such as the need to model
content in terms of its visual appearance, or even social dynamics, such as a
preference toward a particular artist that is independent of the art they
create.
  In this paper we build large-scale recommender systems to model the dynamics
of a vibrant digital art community, Behance, consisting of tens of millions of
interactions (clicks and `appreciates') of users toward digital art.
Methodologically, our main contributions are to model (a) rich content,
especially in terms of its visual appearance; (b) temporal dynamics, in terms
of how users prefer `visually consistent' content within and across sessions;
and (c) social dynamics, in terms of how users exhibit preferences both towards
certain art styles, as well as the artists themselves.
</summary>
    <author>
      <name>Ruining He</name>
    </author>
    <author>
      <name>Chen Fang</name>
    </author>
    <author>
      <name>Zhaowen Wang</name>
    </author>
    <author>
      <name>Julian McAuley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2959100.2959152</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2959100.2959152" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04110v1</id>
    <updated>2016-07-14T12:45:07Z</updated>
    <published>2016-07-14T12:45:07Z</published>
    <title>Using Recurrent Neural Network for Learning Expressive Ontologies</title>
    <summary>  Recently, Neural Networks have been proven extremely effective in many
natural language processing tasks such as sentiment analysis, question
answering, or machine translation. Aiming to exploit such advantages in the
Ontology Learning process, in this technical report we present a detailed
description of a Recurrent Neural Network based system to be used to pursue
such goal.
</summary>
    <author>
      <name>Giulio Petrucci</name>
    </author>
    <author>
      <name>Chiara Ghidini</name>
    </author>
    <author>
      <name>Marco Rospocher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03979v1</id>
    <updated>2016-07-14T02:21:14Z</updated>
    <published>2016-07-14T02:21:14Z</published>
    <title>Resource Planning For Rescue Operations</title>
    <summary>  After an earthquake, disaster sites pose a multitude of health and safety
concerns. A rescue operation of people trapped in the ruins after an earthquake
disaster requires a series of intelligent behavior, including planning. For a
successful rescue operation, given a limited number of available actions and
regulations, the role of planning in rescue operations is crucial. Fortunately,
recent developments in automated planning by artificial intelligence community
can help different organization in this crucial task. Due to the number of
rules and regulations, we believe that a rule based system for planning can be
helpful for this specific planning problem. In this research work, we use logic
rules to represent rescue and related regular regulations, together with a
logic based planner to solve this complicated problem. Although this research
is still in the prototyping and modeling stage, it clearly shows that rule
based languages can be a good infrastructure for this computational task. The
results of this research can be used by different organizations, such as
Iranian Red Crescent Society and International Institute of Seismology and
Earthquake Engineering (IISEE).
</summary>
    <author>
      <name>Mona Khaffaf</name>
    </author>
    <author>
      <name>Arshia Khaffaf</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03705v1</id>
    <updated>2016-07-13T12:45:53Z</updated>
    <published>2016-07-13T12:45:53Z</published>
    <title>Possibilistic Networks: Parameters Learning from Imprecise Data and
  Evaluation strategy</title>
    <summary>  There has been an ever-increasing interest in multidisciplinary research on
representing and reasoning with imperfect data. Possibilistic networks present
one of the powerful frameworks of interest for representing uncertain and
imprecise information. This paper covers the problem of their parameters
learning from imprecise datasets, i.e., containing multi-valued data. We
propose in the rst part of this paper a possibilistic networks sampling
process. In the second part, we propose a likelihood function which explores
the link between random sets theory and possibility theory. This function is
then deployed to parametrize possibilistic networks.
</summary>
    <author>
      <name>Maroua Haddad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA, LARODEC</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Leray</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LINA</arxiv:affiliation>
    </author>
    <author>
      <name>Nahla Ben Amor</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LARODEC</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1607.03705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03611v1</id>
    <updated>2016-07-13T07:15:30Z</updated>
    <published>2016-07-13T07:15:30Z</published>
    <title>Characterizing Driving Styles with Deep Learning</title>
    <summary>  Characterizing driving styles of human drivers using vehicle sensor data,
e.g., GPS, is an interesting research problem and an important real-world
requirement from automotive industries. A good representation of driving
features can be highly valuable for autonomous driving, auto insurance, and
many other application scenarios. However, traditional methods mainly rely on
handcrafted features, which limit machine learning algorithms to achieve a
better performance. In this paper, we propose a novel deep learning solution to
this problem, which could be the first attempt of studying deep learning for
driving behavior analysis. The proposed approach can effectively extract high
level and interpretable features describing complex driving patterns from GPS
data. It also requires significantly less human experience and work. The power
of the learned driving style representations are validated through the driver
identification problem using a large real dataset.
</summary>
    <author>
      <name>Weishan Dong</name>
    </author>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Renjie Yao</name>
    </author>
    <author>
      <name>Changsheng Li</name>
    </author>
    <author>
      <name>Ting Yuan</name>
    </author>
    <author>
      <name>Lanjun Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03516v2</id>
    <updated>2016-08-01T09:58:13Z</updated>
    <published>2016-07-12T20:48:58Z</published>
    <title>Deep Reconstruction-Classification Networks for Unsupervised Domain
  Adaptation</title>
    <summary>  In this paper, we propose a novel unsupervised domain adaptation algorithm
based on deep learning for visual object recognition. Specifically, we design a
new model called Deep Reconstruction-Classification Network (DRCN), which
jointly learns a shared encoding representation for two tasks: i) supervised
classification of labeled source data, and ii) unsupervised reconstruction of
unlabeled target data.In this way, the learnt representation not only preserves
discriminability, but also encodes useful information from the target domain.
Our new DRCN model can be optimized by using backpropagation similarly as the
standard neural networks.
  We evaluate the performance of DRCN on a series of cross-domain object
recognition tasks, where DRCN provides a considerable improvement (up to ~8% in
accuracy) over the prior state-of-the-art algorithms. Interestingly, we also
observe that the reconstruction pipeline of DRCN transforms images from the
source domain into images whose appearance resembles the target dataset. This
suggests that DRCN's performance is due to constructing a single composite
representation that encodes information about both the structure of target
images and the classification of source images. Finally, we provide a formal
analysis to justify the algorithm's objective in domain adaptation context.
</summary>
    <author>
      <name>Muhammad Ghifary</name>
    </author>
    <author>
      <name>W. Bastiaan Kleijn</name>
    </author>
    <author>
      <name>Mengjie Zhang</name>
    </author>
    <author>
      <name>David Balduzzi</name>
    </author>
    <author>
      <name>Wen Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in European Conference on Computer Vision (ECCV) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.03516v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03516v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03354v1</id>
    <updated>2016-07-12T13:46:52Z</updated>
    <published>2016-07-12T13:46:52Z</published>
    <title>Extended Graded Modalities in Strategy Logic</title>
    <summary>  Strategy Logic (SL) is a logical formalism for strategic reasoning in
multi-agent systems. Its main feature is that it has variables for strategies
that are associated to specific agents with a binding operator. We introduce
Graded Strategy Logic (GradedSL), an extension of SL by graded quantifiers over
tuples of strategy variables, i.e., "there exist at least g different tuples
(x_1,...,x_n) of strategies" where g is a cardinal from the set N union
{aleph_0, aleph_1, 2^aleph_0}. We prove that the model-checking problem of
GradedSL is decidable. We then turn to the complexity of fragments of GradedSL.
When the g's are restricted to finite cardinals, written GradedNSL, the
complexity of model-checking is no harder than for SL, i.e., it is
non-elementary in the quantifier rank. We illustrate our formalism by showing
how to count the number of different strategy profiles that are Nash equilibria
(NE), or subgame-perfect equilibria (SPE). By analyzing the structure of the
specific formulas involved, we conclude that the important problems of checking
for the existence of a unique NE or SPE can both be solved in 2ExpTime, which
is not harder than merely checking for the existence of such equilibria.
</summary>
    <author>
      <name>Benjamin Aminof</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technische Universitat Wien, Austria</arxiv:affiliation>
    </author>
    <author>
      <name>Vadim Malvone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Università degli Studi di Napoli Federico II, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Aniello Murano</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Università degli Studi di Napoli Federico II, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Sasha Rubin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Università degli Studi di Napoli Federico II, Italy</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.218.1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.218.1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SR 2016, arXiv:1607.02694</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 218, 2016, pp. 1-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.03354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03317v1</id>
    <updated>2016-07-12T11:52:48Z</updated>
    <published>2016-07-12T11:52:48Z</published>
    <title>Populations can be essential in tracking dynamic optima</title>
    <summary>  Real-world optimisation problems are often dynamic. Previously good solutions
must be updated or replaced due to changes in objectives and constraints. It is
often claimed that evolutionary algorithms are particularly suitable for
dynamic optimisation because a large population can contain different solutions
that may be useful in the future. However, rigorous theoretical demonstrations
for how populations in dynamic optimisation can be essential are sparse and
restricted to special cases.
  This paper provides theoretical explanations of how populations can be
essential in evolutionary dynamic optimisation in a general and natural
setting. We describe a natural class of dynamic optimisation problems where a
sufficiently large population is necessary to keep track of moving optima
reliably. We establish a relationship between the population-size and the
probability that the algorithm loses track of the optimum.
</summary>
    <author>
      <name>Duc-Cuong Dang</name>
    </author>
    <author>
      <name>Thomas Jansen</name>
    </author>
    <author>
      <name>Per Kristian Lehre</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03290v1</id>
    <updated>2016-07-12T09:58:24Z</updated>
    <published>2016-07-12T09:58:24Z</published>
    <title>Automatic Bridge Bidding Using Deep Reinforcement Learning</title>
    <summary>  Bridge is among the zero-sum games for which artificial intelligence has not
yet outperformed expert human players. The main difficulty lies in the bidding
phase of bridge, which requires cooperative decision making under partial
information. Existing artificial intelligence systems for bridge bidding rely
on and are thus restricted by human-designed bidding systems or features. In
this work, we propose a pioneering bridge bidding system without the aid of
human domain knowledge. The system is based on a novel deep reinforcement
learning model, which extracts sophisticated features and learns to bid
automatically based on raw card data. The model includes an
upper-confidence-bound algorithm and additional techniques to achieve a balance
between exploration and exploitation. Our experiments validate the promising
performance of our proposed model. In particular, the model advances from
having no knowledge about bidding to achieving superior performance when
compared with a champion-winning computer bridge program that implements a
human-designed bidding system.
</summary>
    <author>
      <name>Chih-Kuan Yeh</name>
    </author>
    <author>
      <name>Hsuan-Tien Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, 2016 ECAI accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.03290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03189v1</id>
    <updated>2016-07-11T22:37:36Z</updated>
    <published>2016-07-11T22:37:36Z</published>
    <title>A Framework for Estimating Long Term Driver Behavior</title>
    <summary>  The authors present a cyber-physical systems study on the estimation of
driver behavior in autonomous vehicles and vehicle safety systems. Extending
upon previous work, the approach described is suitable for the long term
estimation and tracking of autonomous vehicle behavior. The proposed system
makes use of a previously defined Hybrid State System and Hidden Markov Model
(HSS+HMM) system which has provided good results for driver behavior
estimation. The HSS+HMM system utilizes the hybrid characteristics of
decision-behavior coupling of many systems such as the driver and the vehicle,
uses Kalman Filter estimates of observable parameters to track the
instantaneous continuous state, and estimates the most likely driver state. The
HSS+HMM system is encompassed in a HSS structure and inter-system connectivity
is determined by using Signal Processing and Pattern Recognition techniques.
The proposed method is suitable for scenarios that involve unknown decisions of
other individuals, such as lane changes or intersection precedence/access. The
long term driver behavior estimation system involves an extended HSS+HMM
structure that is capable of including external information in the estimation
process. Through the grafting and pruning of metastates, the HSS+HMM system can
be dynamically updated to best represent driver choices given external
information. Three application examples are also provided to elucidate the
theoretical system.
</summary>
    <author>
      <name>Vijay Gadepally</name>
    </author>
    <author>
      <name>Ashok Krishnamurthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.03189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02902v1</id>
    <updated>2016-07-11T11:08:00Z</updated>
    <published>2016-07-11T11:08:00Z</published>
    <title>sk_p: a neural program corrector for MOOCs</title>
    <summary>  We present a novel technique for automatic program correction in MOOCs,
capable of fixing both syntactic and semantic errors without manual, problem
specific correction strategies. Given an incorrect student program, it
generates candidate programs from a distribution of likely corrections, and
checks each candidate for correctness against a test suite.
  The key observation is that in MOOCs many programs share similar code
fragments, and the seq2seq neural network model, used in the natural-language
processing task of machine translation, can be modified and trained to recover
these fragments.
  Experiment shows our scheme can correct 29% of all incorrect submissions and
out-performs state of the art approach which requires manual, problem specific
correction strategies.
</summary>
    <author>
      <name>Yewen Pu</name>
    </author>
    <author>
      <name>Karthik Narasimhan</name>
    </author>
    <author>
      <name>Armando Solar-Lezama</name>
    </author>
    <author>
      <name>Regina Barzilay</name>
    </author>
    <link href="http://arxiv.org/abs/1607.02902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02802v1</id>
    <updated>2016-07-11T01:20:57Z</updated>
    <published>2016-07-11T01:20:57Z</published>
    <title>Mapping distributional to model-theoretic semantic spaces: a baseline</title>
    <summary>  Word embeddings have been shown to be useful across state-of-the-art systems
in many natural language processing tasks, ranging from question answering
systems to dependency parsing. (Herbelot and Vecchi, 2015) explored word
embeddings and their utility for modeling language semantics. In particular,
they presented an approach to automatically map a standard distributional
semantic space onto a set-theoretic model using partial least squares
regression. We show in this paper that a simple baseline achieves a +51%
relative improvement compared to their model on one of the two datasets they
used, and yields competitive results on the second dataset.
</summary>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <link href="http://arxiv.org/abs/1607.02802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02784v1</id>
    <updated>2016-07-10T20:39:24Z</updated>
    <published>2016-07-10T20:39:24Z</published>
    <title>Open Information Extraction</title>
    <summary>  Open Information Extraction (Open IE) systems aim to obtain relation tuples
with highly scalable extraction in portable across domain by identifying a
variety of relation phrases and their arguments in arbitrary sentences. The
first generation of Open IE learns linear chain models based on unlexicalized
features such as Part-of-Speech (POS) or shallow tags to label the intermediate
words between pair of potential arguments for identifying extractable
relations. Open IE currently is developed in the second generation that is able
to extract instances of the most frequently observed relation types such as
Verb, Noun and Prep, Verb and Prep, and Infinitive with deep linguistic
analysis. They expose simple yet principled ways in which verbs express
relationships in linguistics such as verb phrase-based extraction or
clause-based extraction. They obtain a significantly higher performance over
previous systems in the first generation. In this paper, we describe an
overview of two Open IE generations including strengths, weaknesses and
application areas.
</summary>
    <author>
      <name>Duc-Thuan Vo</name>
    </author>
    <author>
      <name>Ebrahim Bagheri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will appear in the Encyclopedia for Semantic Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02763v1</id>
    <updated>2016-07-10T16:19:00Z</updated>
    <published>2016-07-10T16:19:00Z</published>
    <title>How to Allocate Resources For Features Acquisition?</title>
    <summary>  We study classification problems where features are corrupted by noise and
where the magnitude of the noise in each feature is influenced by the resources
allocated to its acquisition. This is the case, for example, when multiple
sensors share a common resource (power, bandwidth, attention, etc.). We develop
a method for computing the optimal resource allocation for a variety of
scenarios and derive theoretical bounds concerning the benefit that may arise
by non-uniform allocation. We further demonstrate the effectiveness of the
developed method in simulations.
</summary>
    <author>
      <name>Oran Richman</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <link href="http://arxiv.org/abs/1607.02763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02682v1</id>
    <updated>2016-07-10T02:56:33Z</updated>
    <published>2016-07-10T02:56:33Z</published>
    <title>Extending Weakly-Sticky Datalog+/-: Query-Answering Tractability and
  Optimizations</title>
    <summary>  Weakly-sticky (WS) Datalog+/- is an expressive member of the family of
Datalog+/- programs that is based on the syntactic notions of stickiness and
weak-acyclicity. Query answering over the WS programs has been investigated,
but there is still much work to do on the design and implementation of
practical query answering (QA) algorithms and their optimizations. Here, we
study sticky and WS programs from the point of view of the behavior of the
chase procedure, extending the stickiness property of the chase to that of
generalized stickiness of the chase (gsch-property). With this property we
specify the semantic class of GSCh programs, which includes sticky and WS
programs, and other syntactic subclasses that we identify. In particular, we
introduce joint-weakly-sticky (JWS) programs, that include WS programs. We also
propose a bottom-up QA algorithm for a range of subclasses of GSCh. The
algorithm runs in polynomial time (in data) for JWS programs. Unlike the WS
class, JWS is closed under a general magic-sets rewriting procedure for the
optimization of programs with existential rules. We apply the magic-sets
rewriting in combination with the proposed QA algorithm for the optimization of
QA over JWS programs.
</summary>
    <author>
      <name>Mostafa Milani</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of RR'16 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02660v1</id>
    <updated>2016-07-09T20:34:48Z</updated>
    <published>2016-07-09T20:34:48Z</published>
    <title>Augmenting Supervised Emotion Recognition with Rule-Based Decision Model</title>
    <summary>  The aim of this research is development of rule based decision model for
emotion recognition. This research also proposes using the rules for augmenting
inter-corporal recognition accuracy in multimodal systems that use supervised
learning techniques. The classifiers for such learning based recognition
systems are susceptible to over fitting and only perform well on intra-corporal
data. To overcome the limitation this research proposes using rule based model
as an additional modality. The rules were developed using raw feature data from
visual channel, based on human annotator agreement and existing studies that
have attributed movement and postures to emotions. The outcome of the rule
evaluations was combined during the decision phase of emotion recognition
system. The results indicate rule based emotion recognition augment recognition
accuracy of learning based systems and also provide better recognition rate
across inter corpus emotion test data.
</summary>
    <author>
      <name>Amol Patwardhan</name>
    </author>
    <author>
      <name>Gerald Knapp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, 23 tables, IEEE TAC (in review)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02576v2</id>
    <updated>2016-07-14T15:54:29Z</updated>
    <published>2016-07-09T07:11:43Z</published>
    <title>Analysis of opinionated text for opinion mining</title>
    <summary>  In sentiment analysis, the polarities of the opinions expressed on an
object/feature are determined to assess the sentiment of a sentence or document
whether it is positive/negative/neutral. Naturally, the object/feature is a
noun representation which refers to a product or a component of a product, let
us say, the "lens" in a camera and opinions emanating on it are captured in
adjectives, verbs, adverbs and noun words themselves. Apart from such words,
other meta-information and diverse effective features are also going to play an
important role in influencing the sentiment polarity and contribute
significantly to the performance of the system. In this paper, some of the
associated information/meta-data are explored and investigated in the sentiment
text. Based on the analysis results presented here, there is scope for further
assessment and utilization of the meta-information as features in text
categorization, ranking text document, identification of spam documents and
polarity classification problems.
</summary>
    <author>
      <name>K Paramesha</name>
    </author>
    <author>
      <name>K C Ravishankar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/mlaij.2016.3204</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/mlaij.2016.3204" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sentiment Analysis, Features, Feature Engineering, Emotions, Word
  Sense Disambiguation, Sentiment Lexicons, Meta-Information</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning and Applications: An International Journal
  (MLAIJ) Vol.3, No.2, June 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.02576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02480v1</id>
    <updated>2016-07-08T18:20:32Z</updated>
    <published>2016-07-08T18:20:32Z</published>
    <title>Real-Time Anomaly Detection for Streaming Analytics</title>
    <summary>  Much of the worlds data is streaming, time-series data, where anomalies give
significant information in critical situations. Yet detecting anomalies in
streaming data is a difficult task, requiring detectors to process data in
real-time, and learn while simultaneously making predictions. We present a
novel anomaly detection technique based on an on-line sequence memory algorithm
called Hierarchical Temporal Memory (HTM). We show results from a live
application that detects anomalies in financial metrics in real-time. We also
test the algorithm on NAB, a published benchmark for real-time anomaly
detection, where our algorithm achieves best-in-class results.
</summary>
    <author>
      <name>Subutai Ahmad</name>
    </author>
    <author>
      <name>Scott Purdy</name>
    </author>
    <link href="http://arxiv.org/abs/1607.02480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02467v1</id>
    <updated>2016-07-08T17:35:51Z</updated>
    <published>2016-07-08T17:35:51Z</published>
    <title>Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior
  Knowledge</title>
    <summary>  We introduce \emph{LL-RNNs} (Log-Linear RNNs), an extension of Recurrent
Neural Networks that replaces the softmax output layer by a log-linear output
layer, of which the softmax is a special case. This conceptually simple move
has two main advantages. First, it allows the learner to combat training data
sparsity by allowing it to model words (or more generally, output symbols) as
complex combinations of attributes without requiring that each combination is
directly observed in the training data (as the softmax does). Second, it
permits the inclusion of flexible prior knowledge in the form of \emph{a
priori} specified modular features, where the neural network component learns
to dynamically control the weights of a log-linear distribution exploiting
these features. We provide some motivating illustrations, and argue that the
log-linear and the neural-network components contribute complementary strengths
to the LL-RNN: the LL aspect allows the model to incorporate rich prior
knowledge, while the NN aspect, according to the "representation learning"
paradigm, allows the model to discover novel combination of characteristics.
</summary>
    <author>
      <name>Marc Dymetman</name>
    </author>
    <author>
      <name>Chunyang Xiao</name>
    </author>
    <link href="http://arxiv.org/abs/1607.02467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02466v2</id>
    <updated>2016-09-02T18:33:50Z</updated>
    <published>2016-07-08T17:32:58Z</published>
    <title>Solving finite-domain linear constraints in presence of the
  $\texttt{alldifferent}$</title>
    <summary>  In this paper, we investigate the possibility of improvement of the
widely-used filtering algorithm for the linear constraints in constraint
satisfaction problems in the presence of the alldifferent constraints. In many
cases, the fact that the variables in a linear constraint are also constrained
by some alldifferent constraints may help us to calculate stronger bounds of
the variables, leading to a stronger constraint propagation. We propose an
improved filtering algorithm that targets such cases. We provide a detailed
description of the proposed algorithm and prove its correctness. We evaluate
the approach on five different problems that involve combinations of the linear
and the alldifferent constraints. We also compare our algorithm to other
relevant approaches. The experimental results show a great potential of the
proposed improvement.
</summary>
    <author>
      <name>Milan Banković</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Belgrade</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Logical Methods in Computer Science, Volume 12, Issue 3 (September
  5, 2016) lmcs:2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.02466v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02466v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T27, 68T15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02444v1</id>
    <updated>2016-07-08T16:40:30Z</updated>
    <published>2016-07-08T16:40:30Z</published>
    <title>Explaining Deep Convolutional Neural Networks on Music Classification</title>
    <summary>  Deep convolutional neural networks (CNNs) have been actively adopted in the
field of music information retrieval, e.g. genre classification, mood
detection, and chord recognition. However, the process of learning and
prediction is little understood, particularly when it is applied to
spectrograms. We introduce auralisation of a CNN to understand its underlying
mechanism, which is based on a deconvolution procedure introduced in [2].
Auralisation of a CNN is converting the learned convolutional features that are
obtained from deconvolution into audio signals. In the experiments and
discussions, we explain trained features of a 5-layer CNN based on the
deconvolved spectrograms and auralised signals. The pairwise correlations per
layers with varying different musical attributes are also investigated to
understand the evolution of the learnt features. It is shown that in the deep
layers, the features are learnt to capture textures, the patterns of continuous
distributions, rather than shapes of lines.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02436v1</id>
    <updated>2016-07-08T16:17:12Z</updated>
    <published>2016-07-08T16:17:12Z</published>
    <title>Document Clustering Games in Static and Dynamic Scenarios</title>
    <summary>  In this work we propose a game theoretic model for document clustering. Each
document to be clustered is represented as a player and each cluster as a
strategy. The players receive a reward interacting with other players that they
try to maximize choosing their best strategies. The geometry of the data is
modeled with a weighted graph that encodes the pairwise similarity among
documents, so that similar players are constrained to choose similar
strategies, updating their strategy preferences at each iteration of the games.
We used different approaches to find the prototypical elements of the clusters
and with this information we divided the players into two disjoint sets, one
collecting players with a definite strategy and the other one collecting
players that try to learn from others the correct strategy to play. The latter
set of players can be considered as new data points that have to be clustered
according to previous information. This representation is useful in scenarios
in which the data are streamed continuously. The evaluation of the system was
conducted on 13 document datasets using different settings. It shows that the
proposed method performs well compared to different document clustering
algorithms.
</summary>
    <author>
      <name>Rocco Tripodi</name>
    </author>
    <author>
      <name>Marcello Pelillo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be published in the series Lecture Notes in Computer
  Science (LNCS) published by Springer, containing the ICPRAM 2016 best papers</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02431v1</id>
    <updated>2016-07-08T15:58:28Z</updated>
    <published>2016-07-08T15:58:28Z</published>
    <title>Learning opening books in partially observable games: using random seeds
  in Phantom Go</title>
    <summary>  Many artificial intelligences (AIs) are randomized. One can be lucky or
unlucky with the random seed; we quantify this effect and show that, maybe
contrarily to intuition, this is far from being negligible. Then, we apply two
different existing algorithms for selecting good seeds and good probability
distributions over seeds. This mainly leads to learning an opening book. We
apply this to Phantom Go, which, as all phantom games, is hard for opening book
learning. We improve the winning rate from 50% to 70% in 5x5 against the same
AI, and from approximately 0% to 40% in 5x5, 7x7 and 9x9 against a stronger
(learning) opponent.
</summary>
    <author>
      <name>Tristan Cazenave</name>
    </author>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Fabien Teytaud</name>
    </author>
    <author>
      <name>Olivier Teytaud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 15 figures. Accepted by CIG2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A05, 91A10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02399v1</id>
    <updated>2016-07-08T15:06:46Z</updated>
    <published>2016-07-08T15:06:46Z</published>
    <title>Translating Bayesian Networks into Entity Relationship Models, Extended
  Version</title>
    <summary>  Big data analytics applications drive the convergence of data management and
machine learning. But there is no conceptual language available that is spoken
in both worlds. The main contribution of the paper is a method to translate
Bayesian networks, a main conceptual language for probabilistic graphical
models, into usable entity relationship models. The transformed representation
of a Bayesian network leaves out mathematical details about probabilistic
relationships but unfolds all information relevant for data management tasks.
As a real world example, we present the TopicExplorer system that uses Bayesian
topic models as a core component in an interactive, database-supported web
application. Last, we sketch a conceptual framework that eases machine learning
specific development tasks while building big data analytics applications.
</summary>
    <author>
      <name>Frank Rosner</name>
    </author>
    <author>
      <name>Alexander Hinneburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of a short paper published in the
  Proceedings of the 35th International Conference on Conceptual Modeling, ER
  2016. In addition to a more detailed discussion of the method, this extended
  version describes a case study that applies the method as well as first ideas
  of a conceptual framework for developing big data analytics applications</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02306v2</id>
    <updated>2016-08-15T18:02:09Z</updated>
    <published>2016-07-08T10:42:43Z</published>
    <title>CaR-FOREST: Joint Classification-Regression Decision Forests for
  Overlapping Audio Event Detection</title>
    <summary>  This report describes our submissions to Task2 and Task3 of the DCASE 2016
challenge. The systems aim at dealing with the detection of overlapping audio
events in continuous streams, where the detectors are based on random decision
forests. The proposed forests are jointly trained for classification and
regression simultaneously. Initially, the training is classification-oriented
to encourage the trees to select discriminative features from overlapping
mixtures to separate positive audio segments from the negative ones. The
regression phase is then carried out to let the positive audio segments vote
for the event onsets and offsets, and therefore model the temporal structure of
audio events. One random decision forest is specifically trained for each event
category of interest. Experimental results on the development data show that
our systems significantly outperform the baseline on the Task2 evaluation while
they are inferior to the baseline in the Task3 evaluation.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Lars Hertel</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Task2 and Task3 technical report for the DCASE2016 challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02306v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02306v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02171v1</id>
    <updated>2016-07-07T21:01:06Z</updated>
    <published>2016-07-07T21:01:06Z</published>
    <title>Argumentation Models for Cyber Attribution</title>
    <summary>  A major challenge in cyber-threat analysis is combining information from
different sources to find the person or the group responsible for the
cyber-attack. It is one of the most important technical and policy challenges
in cyber-security. The lack of ground truth for an individual responsible for
an attack has limited previous studies. In this paper, we take a first step
towards overcoming this limitation by building a dataset from the
capture-the-flag event held at DEFCON, and propose an argumentation model based
on a formal reasoning framework called DeLP (Defeasible Logic Programming)
designed to aid an analyst in attributing a cyber-attack. We build models from
latent variables to reduce the search space of culprits (attackers), and show
that this reduction significantly improves the performance of
classification-based approaches from 37% to 62% in identifying the attacker.
</summary>
    <author>
      <name>Eric Nunes</name>
    </author>
    <author>
      <name>Paulo Shakarian</name>
    </author>
    <author>
      <name>Gerardo I. Simari</name>
    </author>
    <author>
      <name>Andrew Ruef</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages paper to be presented at International Symposium on
  Foundations of Open Source Intelligence and Security Informatics (FOSINT-SI)
  2016 In conjunction with ASONAM 2016 San Francisco, CA, USA, August 19-20,
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02061v1</id>
    <updated>2016-07-07T16:00:33Z</updated>
    <published>2016-07-07T16:00:33Z</published>
    <title>Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity</title>
    <summary>  Several studies on sentence processing suggest that the mental lexicon keeps
track of the mutual expectations between words. Current DSMs, however,
represent context words as separate features, which causes the loss of
important information for word expectations, such as word order and
interrelations. In this paper, we present a DSM which addresses the issue by
defining verb contexts as joint dependencies. We test our representation in a
verb similarity task on two datasets, showing that joint contexts are more
efficient than single dependencies, even with a relatively small amount of
training data.
</summary>
    <author>
      <name>Emmanuele Chersoni</name>
    </author>
    <author>
      <name>Enrico Santus</name>
    </author>
    <author>
      <name>Alessandro Lenci</name>
    </author>
    <author>
      <name>Philippe Blache</name>
    </author>
    <author>
      <name>Chu-Ren Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02018v1</id>
    <updated>2016-07-07T14:00:06Z</updated>
    <published>2016-07-07T14:00:06Z</published>
    <title>Mapping Data to Ontologies with Exceptions Using Answer Set Programming</title>
    <summary>  In ontology-based data access, databases are connected to an ontology via
mappings from queries over the database to queries over the ontology. In this
paper, we consider mappings from relational databases to first-order
ontologies, and define an ASP-based framework for GLAV mappings with queries
over the ontology in the mapping rule bodies. We show that this type of
mappings can be used to express constraints and exceptions, as well as being a
powerful mechanism for succinctly representing OBDA mappings. We give an
algorithm for brave reasoning in this setting, and show that this problem has
either the same data complexity as ASP (NP- complete), or it is at least as
hard as the complexity of checking entailment for the ontology queries.
Furthermore, we show that for ontologies with UCQ-rewritable queries there
exists a natural reduction from mapping programs to \exists-ASP, an extension
of ASP with existential variables that itself admits a natural reduction to
ASP.
</summary>
    <author>
      <name>Daniel P. Lupp</name>
    </author>
    <author>
      <name>Evgenij Thorstensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, ONTOLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01869v1</id>
    <updated>2016-07-07T03:43:12Z</updated>
    <published>2016-07-07T03:43:12Z</published>
    <title>Scalable Semantic Matching of Queries to Ads in Sponsored Search
  Advertising</title>
    <summary>  Sponsored search represents a major source of revenue for web search engines.
This popular advertising model brings a unique possibility for advertisers to
target users' immediate intent communicated through a search query, usually by
displaying their ads alongside organic search results for queries deemed
relevant to their products or services. However, due to a large number of
unique queries it is challenging for advertisers to identify all such relevant
queries. For this reason search engines often provide a service of advanced
matching, which automatically finds additional relevant queries for advertisers
to bid on. We present a novel advanced matching approach based on the idea of
semantic embeddings of queries and ads. The embeddings were learned using a
large data set of user search sessions, consisting of search queries, clicked
ads and search links, while utilizing contextual information such as dwell time
and skipped ads. To address the large-scale nature of our problem, both in
terms of data and vocabulary size, we propose a novel distributed algorithm for
training of the embeddings. Finally, we present an approach for overcoming a
cold-start problem associated with new ads and queries. We report results of
editorial evaluation and online tests on actual search traffic. The results
show that our approach significantly outperforms baselines in terms of
relevance, coverage, and incremental revenue. Lastly, we open-source learned
query embeddings to be used by researchers in computational advertising and
related fields.
</summary>
    <author>
      <name>Mihajlo Grbovic</name>
    </author>
    <author>
      <name>Nemanja Djuric</name>
    </author>
    <author>
      <name>Vladan Radosavljevic</name>
    </author>
    <author>
      <name>Fabrizio Silvestri</name>
    </author>
    <author>
      <name>Ricardo Baeza-Yates</name>
    </author>
    <author>
      <name>Andrew Feng</name>
    </author>
    <author>
      <name>Erik Ordentlich</name>
    </author>
    <author>
      <name>Lee Yang</name>
    </author>
    <author>
      <name>Gavin Owens</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2911451.2911538.</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2911451.2911538." rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 39th International ACM SIGIR Conference on
  Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">39th International ACM SIGIR Conference on Research and
  Development in Information Retrieval, SIGIR 2016, Pisa, Italy</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.01869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01730v1</id>
    <updated>2016-07-06T18:03:18Z</updated>
    <published>2016-07-06T18:03:18Z</published>
    <title>Rolling Horizon Coevolutionary Planning for Two-Player Video Games</title>
    <summary>  This paper describes a new algorithm for decision making in two-player
real-time video games. As with Monte Carlo Tree Search, the algorithm can be
used without heuristics and has been developed for use in general video game
AI. The approach is to extend recent work on rolling horizon evolutionary
planning, which has been shown to work well for single-player games, to two (or
in principle many) player games. To select an action the algorithm co-evolves
two (or in the general case N) populations, one for each player, where each
individual is a sequence of actions for the respective player. The fitness of
each individual is evaluated by playing it against a selection of
action-sequences from the opposing population. When choosing an action to take
in the game, the first action is chosen from the fittest member of the
population for that player. The new algorithm is compared with a number of
general video game AI algorithms on three variations of a two-player space
battle game, with promising results.
</summary>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Diego Pérez-Liébana</name>
    </author>
    <author>
      <name>Simon M. Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures, 1 table, 6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A05, 91A15, 68T20, 97R40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01729v2</id>
    <updated>2016-07-07T02:07:22Z</updated>
    <published>2016-07-06T18:02:33Z</published>
    <title>Cost-Optimal Algorithms for Planning with Procedural Control Knowledge</title>
    <summary>  There is an impressive body of work on developing heuristics and other
reasoning algorithms to guide search in optimal and anytime planning algorithms
for classical planning. However, very little effort has been directed towards
developing analogous techniques to guide search towards high-quality solutions
in hierarchical planning formalisms like HTN planning, which allows using
additional domain-specific procedural control knowledge. In lieu of such
techniques, this control knowledge often needs to provide the necessary search
guidance to the planning algorithm, which imposes a substantial burden on the
domain author and can yield brittle or error-prone domain models. We address
this gap by extending recent work on a new hierarchical goal-based planning
formalism called Hierarchical Goal Network (HGN) Planning to develop the
Hierarchically-Optimal Goal Decomposition Planner (HOpGDP), an HGN planning
algorithm that computes hierarchically-optimal plans. HOpGDP is guided by
$h_{HL}$, a new HGN planning heuristic that extends existing admissible
landmark-based heuristics from classical planning to compute admissible cost
estimates for HGN planning problems. Our experimental evaluation across three
benchmark planning domains shows that HOpGDP compares favorably to both optimal
classical planners due to its ability to use domain-specific procedural
knowledge, and a blind-search version of HOpGDP due to the search guidance
provided by $h_{HL}$.
</summary>
    <author>
      <name>Vikas Shivashankar</name>
    </author>
    <author>
      <name>Ron Alford</name>
    </author>
    <author>
      <name>Mark Roberts</name>
    </author>
    <author>
      <name>David W. Aha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proc. of ECAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01729v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01729v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01719v1</id>
    <updated>2016-07-06T17:35:55Z</updated>
    <published>2016-07-06T17:35:55Z</published>
    <title>Deep CORAL: Correlation Alignment for Deep Domain Adaptation</title>
    <summary>  Deep neural networks are able to learn powerful representations from large
quantities of labeled input data, however they cannot always generalize well
across changes in input distributions. Domain adaptation algorithms have been
proposed to compensate for the degradation in performance due to domain shift.
In this paper, we address the case when the target domain is unlabeled,
requiring unsupervised adaptation. CORAL is a "frustratingly easy" unsupervised
domain adaptation method that aligns the second-order statistics of the source
and target distributions with a linear transformation. Here, we extend CORAL to
learn a nonlinear transformation that aligns correlations of layer activations
in deep neural networks (Deep CORAL). Experiments on standard benchmark
datasets show state-of-the-art performance.
</summary>
    <author>
      <name>Baochen Sun</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended Abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01634v1</id>
    <updated>2016-07-06T16:27:42Z</updated>
    <published>2016-07-06T16:27:42Z</published>
    <title>Lattice Structure of Variable Precision Rough Sets</title>
    <summary>  The main purpose of this paper is to study the lattice structure of variable
precision rough sets. The notion of variation in precision of rough sets have
been further extended to variable precision rough set with variable
classification error and its algebraic properties are also studied.
</summary>
    <author>
      <name>Sumita Basu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01690v1</id>
    <updated>2016-07-06T16:00:43Z</updated>
    <published>2016-07-06T16:00:43Z</published>
    <title>A New Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes
  Classifier for Coping with Gene Ontology-based Features</title>
    <summary>  The Tree Augmented Naive Bayes classifier is a type of probabilistic
graphical model that can represent some feature dependencies. In this work, we
propose a Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes
(HRE-TAN) algorithm, which considers removing the hierarchical redundancy
during the classifier learning process, when coping with data containing
hierarchically structured features. The experiments showed that HRE-TAN obtains
significantly better predictive performance than the conventional Tree
Augmented Naive Bayes classifier, and enhanced the robustness against
imbalanced class distributions, in aging-related gene datasets with Gene
Ontology terms used as features.
</summary>
    <author>
      <name>Cen Wan</name>
    </author>
    <author>
      <name>Alex A. Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Machine Learning (ICML 2016)
  Computational Biology Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8; I.5.1; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01490v1</id>
    <updated>2016-07-06T06:58:31Z</updated>
    <published>2016-07-06T06:58:31Z</published>
    <title>Towards Self-explanatory Ontology Visualization with Contextual
  Verbalization</title>
    <summary>  Ontologies are one of the core foundations of the Semantic Web. To
participate in Semantic Web projects, domain experts need to be able to
understand the ontologies involved. Visual notations can provide an overview of
the ontology and help users to understand the connections among entities.
However, the users first need to learn the visual notation before they can
interpret it correctly. Controlled natural language representation would be
readable right away and might be preferred in case of complex axioms, however,
the structure of the ontology would remain less apparent. We propose to combine
ontology visualizations with contextual ontology verbalizations of selected
ontology (diagram) elements, displaying controlled natural language (CNL)
explanations of OWL axioms corresponding to the selected visual notation
elements. Thus, the domain experts will benefit from both the high-level
overview provided by the graphical notation and the detailed textual
explanations of particular elements in the diagram.
</summary>
    <author>
      <name>Renārs Liepiņš</name>
    </author>
    <author>
      <name>Uldis Bojārs</name>
    </author>
    <author>
      <name>Normunds Grūzītis</name>
    </author>
    <author>
      <name>Kārlis Čerāns</name>
    </author>
    <author>
      <name>Edgars Celms</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-40180-5_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-40180-5_1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Databases and Information Systems, Communications in Computer and
  Information Science, Vol. 615, Springer, 2016, pp. 3-17</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.01490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01478v1</id>
    <updated>2016-07-06T04:23:36Z</updated>
    <published>2016-07-06T04:23:36Z</published>
    <title>Mixed Strategy for Constrained Stochastic Optimal Control</title>
    <summary>  Choosing control inputs randomly can result in a reduced expected cost in
optimal control problems with stochastic constraints, such as stochastic model
predictive control (SMPC). We consider a controller with initial randomization,
meaning that the controller randomly chooses from K+1 control sequences at the
beginning (called K-randimization).It is known that, for a finite-state,
finite-action Markov Decision Process (MDP) with K constraints, K-randimization
is sufficient to achieve the minimum cost. We found that the same result holds
for stochastic optimal control problems with continuous state and action
spaces.Furthermore, we show the randomization of control input can result in
reduced cost when the optimization problem is nonconvex, and the cost reduction
is equal to the duality gap. We then provide the necessary and sufficient
conditions for the optimality of a randomized solution, and develop an
efficient solution method based on dual optimization. Furthermore, in a special
case with K=1 such as a joint chance-constrained problem, the dual optimization
can be solved even more efficiently by root finding. Finally, we test the
theories and demonstrate the solution method on multiple practical problems
ranging from path planning to the planning of entry, descent, and landing (EDL)
for future Mars missions.
</summary>
    <author>
      <name>Masahiro Ono</name>
    </author>
    <author>
      <name>Mahmoud El Chamie</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <author>
      <name>Behcet Acikmese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. 9 figures.Preliminary version of a working journal paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01381v1</id>
    <updated>2016-07-05T19:40:56Z</updated>
    <published>2016-07-05T19:40:56Z</published>
    <title>One-Shot Session Recommendation Systems with Combinatorial Items</title>
    <summary>  In recent years, content recommendation systems in large websites (or
\emph{content providers}) capture an increased focus. While the type of content
varies, e.g.\ movies, articles, music, advertisements, etc., the high level
problem remains the same. Based on knowledge obtained so far on the user,
recommend the most desired content. In this paper we present a method to handle
the well known user-cold-start problem in recommendation systems. In this
scenario, a recommendation system encounters a new user and the objective is to
present items as relevant as possible with the hope of keeping the user's
session as long as possible. We formulate an optimization problem aimed to
maximize the length of this initial session, as this is believed to be the key
to have the user come back and perhaps register to the system. In particular,
our model captures the fact that a single round with low quality recommendation
is likely to terminate the session. In such a case, we do not proceed to the
next round as the user leaves the system, possibly never to seen again. We
denote this phenomenon a \emph{One-Shot Session}. Our optimization problem is
formulated as an MDP where the action space is of a combinatorial nature as we
recommend in each round, multiple items. This huge action space presents a
computational challenge making the straightforward solution intractable. We
analyze the structure of the MDP to prove monotone and submodular like
properties that allow a computationally efficient solution via a method denoted
by \emph{Greedy Value Iteration} (G-VI).
</summary>
    <author>
      <name>Yahel David</name>
    </author>
    <author>
      <name>Dotan Di Castro</name>
    </author>
    <author>
      <name>Zohar Karnin</name>
    </author>
    <link href="http://arxiv.org/abs/1607.01381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02419v1</id>
    <updated>2016-07-05T19:25:02Z</updated>
    <published>2016-07-05T19:25:02Z</published>
    <title>Divisive-agglomerative algorithm and complexity of automatic
  classification problems</title>
    <summary>  An algorithm of solution of the Automatic Classification (AC for brevity)
problem is set forth in the paper. In the AC problem, it is required to find
one or several artitions, starting with the given pattern matrix or
dissimilarity, similarity matrix.
</summary>
    <author>
      <name>Alexander Rubchinsky</name>
    </author>
    <link href="http://arxiv.org/abs/1607.02419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01337v1</id>
    <updated>2016-07-05T17:22:18Z</updated>
    <published>2016-07-05T17:22:18Z</published>
    <title>Can mobile usage predict illiteracy in a developing country?</title>
    <summary>  The present study provides the first evidence that illiteracy can be reliably
predicted from standard mobile phone logs. By deriving a broad set of mobile
phone indicators reflecting users financial, social and mobility patterns we
show how supervised machine learning can be used to predict individual
illiteracy in an Asian developing country, externally validated against a
large-scale survey. On average the model performs 10 times better than random
guessing with a 70% accuracy. Further we show how individual illiteracy can be
aggregated and mapped geographically at cell tower resolution. Geographical
mapping of illiteracy is crucial to know where the illiterate people are, and
where to put in resources. In underdeveloped countries such mappings are often
based on out-dated household surveys with low spatial and temporal resolution.
One in five people worldwide struggle with illiteracy, and it is estimated that
illiteracy costs the global economy more than 1 trillion dollars each year.
These results potentially enable costeffective, questionnaire-free
investigation of illiteracy-related questions on an unprecedented scale
</summary>
    <author>
      <name>Pål Sundsøy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01254v2</id>
    <updated>2016-07-14T03:04:48Z</updated>
    <published>2016-07-05T14:05:29Z</published>
    <title>An extended MABAC for multi-attribute decision making using trapezoidal
  interval type-2 fuzzy numbers</title>
    <summary>  The purpose of this paper is to study a type-2 fuzzy multi-attribute decision
making (MADM) methodology. Multi-Attributive Border Approximation area
Comparison (MABAC) method is extended for MADM based on trapezoidal interval
type-2 fuzzy numbers (TrIT2FNs).This method is a pragmatic and reliable tool
for rational decision making due to its own characteristics. A systematic
evaluation and assessment method is developed in this paper by integrating
TrIT2FNs and used for evaluation and selection of the most suitable candidate
for a software company which is heading to hire a system analysis engineer
based on few attributes. The performance ratings of alternative candidates and
the weights of the criteria are evaluated based on decision makers from
different expertise. The validity and feasibility of the proposed method are
illustrated by an example and finally compared with two other MADM methods.
</summary>
    <author>
      <name>Jagannath Roy</name>
    </author>
    <author>
      <name>Ananta Ranjan</name>
    </author>
    <author>
      <name>Animesh Debnath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01254v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01254v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01202v1</id>
    <updated>2016-07-05T11:53:13Z</updated>
    <published>2016-07-05T11:53:13Z</published>
    <title>Optimal control for a robotic exploration, pick-up and delivery problem</title>
    <summary>  This paper addresses an optimal control problem for a robot that has to find
and collect a finite number of objects and move them to a depot in minimum
time. The robot has fourth-order dynamics that change instantaneously at any
pick-up or drop-off of an object. The objects are modeled by point masses with
a-priori unknown locations in a bounded two-dimensional space that may contain
unknown obstacles. For this hybrid system, an Optimal Control Problem (OCP) is
approximately solved by a receding horizon scheme, where the derived lower
bound for the cost-to-go is evaluated for the worst and for a probabilistic
case, assuming a uniform distribution of the objects. First, a time-driven
approximate solution based on time and position space discretization and mixed
integer programming is presented. Due to the high computational cost of this
solution, an alternative event-driven approximate approach based on a suitable
motion parameterization and gradient-based optimization is proposed. The
solutions are compared in a numerical example, suggesting that the latter
approach offers a significant computational advantage while yielding similar
qualitative results compared to the former. The methods are particularly
relevant for various robotic applications like automated cleaning, search and
rescue, harvesting or manufacturing.
</summary>
    <author>
      <name>Vladislav Nenchev</name>
    </author>
    <author>
      <name>Christos G. Cassandras</name>
    </author>
    <author>
      <name>Jörg Raisch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 23 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01115v1</id>
    <updated>2016-07-05T05:35:22Z</updated>
    <published>2016-07-05T05:35:22Z</published>
    <title>Click Carving: Segmenting Objects in Video with Point Clicks</title>
    <summary>  We present a novel form of interactive video object segmentation where a few
clicks by the user helps the system produce a full spatio-temporal segmentation
of the object of interest. Whereas conventional interactive pipelines take the
user's initialization as a starting point, we show the value in the system
taking the lead even in initialization. In particular, for a given video frame,
the system precomputes a ranked list of thousands of possible segmentation
hypotheses (also referred to as object region proposals) using image and motion
cues. Then, the user looks at the top ranked proposals, and clicks on the
object boundary to carve away erroneous ones. This process iterates (typically
2-3 times), and each time the system revises the top ranked proposal set, until
the user is satisfied with a resulting segmentation mask. Finally, the mask is
propagated across the video to produce a spatio-temporal object tube. On three
challenging datasets, we provide extensive comparisons with both existing work
and simpler alternative methods. In all, the proposed Click Carving approach
strikes an excellent balance of accuracy and human effort. It outperforms all
similarly fast methods, and is competitive or better than those requiring 2 to
12 times the effort.
</summary>
    <author>
      <name>Suyog Dutt Jain</name>
    </author>
    <author>
      <name>Kristen Grauman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of the material in this document was filed as
  University of Texas technical report no. UT AI16-01</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01050v1</id>
    <updated>2016-07-04T21:21:59Z</updated>
    <published>2016-07-04T21:21:59Z</published>
    <title>Application of Statistical Relational Learning to Hybrid Recommendation
  Systems</title>
    <summary>  Recommendation systems usually involve exploiting the relations among known
features and content that describe items (content-based filtering) or the
overlap of similar users who interacted with or rated the target item
(collaborative filtering). To combine these two filtering approaches, current
model-based hybrid recommendation systems typically require extensive feature
engineering to construct a user profile. Statistical Relational Learning (SRL)
provides a straightforward way to combine the two approaches. However, due to
the large scale of the data used in real world recommendation systems, little
research exists on applying SRL models to hybrid recommendation systems, and
essentially none of that research has been applied on real big-data-scale
systems. In this paper, we proposed a way to adapt the state-of-the-art in SRL
learning approaches to construct a real hybrid recommendation system.
Furthermore, in order to satisfy a common requirement in recommendation systems
(i.e. that false positives are more undesirable and therefore penalized more
harshly than false negatives), our approach can also allow tuning the trade-off
between the precision and recall of the system in a principled way. Our
experimental results demonstrate the efficiency of our proposed approach as
well as its improved performance on recommendation precision.
</summary>
    <author>
      <name>Shuo Yang</name>
    </author>
    <author>
      <name>Mohammed Korayem</name>
    </author>
    <author>
      <name>Khalifeh AlJadda</name>
    </author>
    <author>
      <name>Trey Grainger</name>
    </author>
    <author>
      <name>Sriraam Natarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Relational AI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01036v1</id>
    <updated>2016-07-04T20:12:41Z</updated>
    <published>2016-07-04T20:12:41Z</published>
    <title>Bootstrap Model Aggregation for Distributed Statistical Learning</title>
    <summary>  In distributed, or privacy-preserving learning, we are often given a set of
probabilistic models estimated from different local repositories, and asked to
combine them into a single model that gives efficient statistical estimation. A
simple method is to linearly average the parameters of the local models, which,
however, tends to be degenerate or not applicable on non-convex models, or
models with different parameter dimensions. One more practical strategy is to
generate bootstrap samples from the local models, and then learn a joint model
based on the combined bootstrap set. Unfortunately, the bootstrap procedure
introduces additional noise and can significantly deteriorate the performance.
In this work, we propose two variance reduction methods to correct the
bootstrap noise, including a weighted M-estimator that is both statistically
efficient and practically powerful. Both theoretical and empirical analysis is
provided to demonstrate our methods.
</summary>
    <author>
      <name>Jun Han</name>
    </author>
    <author>
      <name>Qiang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is about variance reduction on Monte Carol estimation of
  KL divergence</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00992v1</id>
    <updated>2016-07-04T19:02:47Z</updated>
    <published>2016-07-04T19:02:47Z</published>
    <title>Generic Statistical Relational Entity Resolution in Knowledge Graphs</title>
    <summary>  Entity resolution, the problem of identifying the underlying entity of
references found in data, has been researched for many decades in many
communities. A common theme in this research has been the importance of
incorporating relational features into the resolution process. Relational
entity resolution is particularly important in knowledge graphs (KGs), which
have a regular structure capturing entities and their interrelationships. We
identify three major problems in KG entity resolution: (1) intra-KG reference
ambiguity; (2) inter-KG reference ambiguity; and (3) ambiguity when extending
KGs with new facts. We implement a framework that generalizes across these
three settings and exploits this regular structure of KGs. Our framework has
many advantages over custom solutions widely deployed in industry, including
collective inference, scalability, and interpretability. We apply our framework
to two real-world KG entity resolution problems, ambiguity in NELL and merging
data from Freebase and MusicBrainz, demonstrating the importance of relational
features.
</summary>
    <author>
      <name>Jay Pujara</name>
    </author>
    <author>
      <name>Lise Getoor</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In the Sixth International Workshop on Statistical Relational AI,
  2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.00992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00976v2</id>
    <updated>2016-07-05T02:27:41Z</updated>
    <published>2016-07-04T18:04:18Z</published>
    <title>Modelling Context with User Embeddings for Sarcasm Detection in Social
  Media</title>
    <summary>  We introduce a deep neural network for automated sarcasm detection. Recent
work has emphasized the need for models to capitalize on contextual features,
beyond lexical and syntactic cues present in utterances. For example, different
speakers will tend to employ sarcasm regarding different subjects and, thus,
sarcasm detection models ought to encode such speaker information. Current
methods have achieved this by way of laborious feature engineering. By
contrast, we propose to automatically learn and then exploit user embeddings,
to be used in concert with lexical signals to recognize sarcasm. Our approach
does not require elaborate feature engineering (and concomitant data scraping);
fitting user embeddings requires only the text from their previous posts. The
experimental results show that our model outperforms a state-of-the-art
approach leveraging an extensive set of carefully crafted features.
</summary>
    <author>
      <name>Silvio Amir</name>
    </author>
    <author>
      <name>Byron C. Wallace</name>
    </author>
    <author>
      <name>Hao Lyu</name>
    </author>
    <author>
      <name>Paula Carvalho Mário J. Silva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published as a conference paper at CONLL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00976v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00976v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00913v1</id>
    <updated>2016-07-04T14:44:21Z</updated>
    <published>2016-07-04T14:44:21Z</published>
    <title>Superintelligence cannot be contained: Lessons from Computability Theory</title>
    <summary>  Superintelligence is a hypothetical agent that possesses intelligence far
surpassing that of the brightest and most gifted human minds. In light of
recent advances in machine intelligence, a number of scientists, philosophers
and technologists have revived the discussion about the potential catastrophic
risks entailed by such an entity. In this article, we trace the origins and
development of the neo-fear of superintelligence, and some of the major
proposals for its containment. We argue that such containment is, in principle,
impossible, due to fundamental limits inherent to computing itself. Assuming
that a superintelligence will contain a program that includes all the programs
that can be executed by a universal Turing machine on input potentially as
complex as the state of the world, strict containment requires simulations of
such a program, something theoretically (and practically) infeasible.
</summary>
    <author>
      <name>Manuel Alfonseca</name>
    </author>
    <author>
      <name>Manuel Cebrian</name>
    </author>
    <author>
      <name>Antonio Fernandez Anta</name>
    </author>
    <author>
      <name>Lorenzo Coviello</name>
    </author>
    <author>
      <name>Andres Abeliuk</name>
    </author>
    <author>
      <name>Iyad Rahwan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00888v1</id>
    <updated>2016-07-04T13:57:35Z</updated>
    <published>2016-07-04T13:57:35Z</published>
    <title>Encoding Cryptographic Functions to SAT Using Transalg System</title>
    <summary>  In this paper we propose the technology for constructing propositional
encodings of discrete functions. It is aimed at solving inversion problems of
considered functions using state-of-the-art SAT solvers. We implemented this
technology in the form of the software system called Transalg, and used it to
construct SAT encodings for a number of cryptanalysis problems. By applying SAT
solvers to these encodings we managed to invert several cryptographic
functions. In particular, we used the SAT encodings produced by Transalg to
construct the family of two-block MD5 collisions in which the first 10 bytes
are zeros. Also we used Transalg encoding for the widely known A5/1 keystream
generator to solve several dozen of its cryptanalysis instances in a
distributed computing environment. In the paper we compare in detail the
functionality of Transalg with that of similar software systems.
</summary>
    <author>
      <name>Ilya Otpuschennikov</name>
    </author>
    <author>
      <name>Alexander Semenov</name>
    </author>
    <author>
      <name>Irina Gribanova</name>
    </author>
    <author>
      <name>Oleg Zaikin</name>
    </author>
    <author>
      <name>Stepan Kochemazov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Short variant of this paper was accepted to ECAI2016 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00872v1</id>
    <updated>2016-07-04T13:08:19Z</updated>
    <published>2016-07-04T13:08:19Z</published>
    <title>Neighborhood Features Help Detecting Electricity Theft in Big Data Sets</title>
    <summary>  Electricity theft is a major problem around the world in both developed and
developing countries and may range up to 40% of the total electricity
distributed. More generally, electricity theft belongs to non-technical losses
(NTL), which are losses that occur during the distribution of electricity in
power grids. In this paper, we build features from the neighborhood of
customers. We first split the area in which the customers are located into
grids of different sizes. For each grid cell we then compute the proportion of
inspected customers and the proportion of NTL found among the inspected
customers. We then analyze the distributions of features generated and show why
they are useful to predict NTL. In addition, we compute features from the
consumption time series of customers. We also use master data features of
customers, such as their customer class and voltage of their connection. We
compute these features for a Big Data base of 31M meter readings, 700K
customers and 400K inspection results. We then use these features to train four
machine learning algorithms that are particularly suitable for Big Data sets
because of their parallelizable structure: logistic regression, k-nearest
neighbors, linear support vector machine and random forest. Using the
neighborhood features instead of only analyzing the time series has resulted in
appreciable results for Big Data sets for varying NTL proportions of 1%-90%.
This work can therefore be deployed to a wide range of different regions around
the world.
</summary>
    <author>
      <name>Patrick Glauner</name>
    </author>
    <author>
      <name>Jorge Meira</name>
    </author>
    <author>
      <name>Lautaro Dolberg</name>
    </author>
    <author>
      <name>Radu State</name>
    </author>
    <author>
      <name>Franck Bettinger</name>
    </author>
    <author>
      <name>Yves Rangoni</name>
    </author>
    <author>
      <name>Diogo Duarte</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00869v1</id>
    <updated>2016-07-04T13:05:55Z</updated>
    <published>2016-07-04T13:05:55Z</published>
    <title>Modeling of Item-Difficulty for Ontology-based MCQs</title>
    <summary>  Multiple choice questions (MCQs) that can be generated from a domain ontology
can significantly reduce human effort &amp; time required for authoring &amp;
administering assessments in an e-Learning environment. Even though here are
various methods for generating MCQs from ontologies, methods for determining
the difficulty-levels of such MCQs are less explored. In this paper, we study
various aspects and factors that are involved in determining the
difficulty-score of an MCQ, and propose an ontology-based model for the
prediction. This model characterizes the difficulty values associated with the
stem and choice set of the MCQs, and describes a measure which combines both
the scores. Further more, the notion of assigning difficultly-scores based on
the skill level of the test taker is utilized for predicating difficulty-score
of a stem. We studied the effectiveness of the predicted difficulty-scores with
the help of a psychometric model from the Item Response Theory, by involving
real-students and domain experts. Our results show that, the predicated
difficulty-levels of the MCQs are having high correlation with their actual
difficulty-levels.
</summary>
    <author>
      <name>Vinu E. V</name>
    </author>
    <author>
      <name>Tahani Alsubait</name>
    </author>
    <author>
      <name>P. Sreenivasa Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00819v1</id>
    <updated>2016-07-04T10:52:57Z</updated>
    <published>2016-07-04T10:52:57Z</published>
    <title>Understanding the Abstract Dialectical Framework (Preliminary Report)</title>
    <summary>  Among the most general structures extending the framework by Dung are the
abstract dialectical frameworks (ADFs). They come equipped with various types
of semantics, with the most prominent - the labeling-based one - analyzed in
the context of computational complexity, signatures, instantiations and
software support. This makes the abstract dialectical frameworks valuable tools
for argumentation. However, there are fewer results available concerning the
relation between the ADFs and other argumentation frameworks. In this paper we
would like to address this issue by introducing a number of translations from
various formalisms into ADFs. The results of our study show the similarities
and differences between them, thus promoting the use and understanding of ADFs.
Moreover, our analysis also proves their capability to model many of the
existing frameworks, including those that go beyond the attack relation.
Finally, translations allow other structures to benefit from the research on
ADFs in general and from the existing software in particular.
</summary>
    <author>
      <name>Sylwia Polberg</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00791v1</id>
    <updated>2016-07-04T09:20:29Z</updated>
    <published>2016-07-04T09:20:29Z</published>
    <title>Formal analysis of HTM Spatial Pooler performance under predefined
  operation conditions</title>
    <summary>  This paper introduces mathematical formalism for Spatial (SP) of Hierarchical
Temporal Memory (HTM) with a spacial consideration for its hardware
implementation. Performance of HTM network and its ability to learn and adjust
to a problem at hand is governed by a large set of parameters. Most of
parameters are codependent which makes creating efficient HTM-based solutions
challenging. It requires profound knowledge of the settings and their impact on
the performance of system. Consequently, this paper introduced a set of
formulas which are to facilitate the design process by enhancing tedious
trial-and-error method with a tool for choosing initial parameters which enable
quick learning convergence. This is especially important in hardware
implementations which are constrained by the limited resources of a platform.
The authors focused especially on a formalism of Spatial Pooler and derive at
the formulas for quality and convergence of the model. This may be considered
as recipes for designing efficient HTM models for given input patterns.
</summary>
    <author>
      <name>M. Pietron</name>
    </author>
    <author>
      <name>M. Wielgosz</name>
    </author>
    <author>
      <name>K. Wiatr</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00715v1</id>
    <updated>2016-07-04T01:13:32Z</updated>
    <published>2016-07-04T01:13:32Z</published>
    <title>Path planning with Inventory-driven Jump-Point-Search</title>
    <summary>  In many navigational domains the traversability of cells is conditioned on
the path taken. This is often the case in video-games, in which a character may
need to acquire a certain object (i.e., a key or a flying suit) to be able to
traverse specific locations (e.g., doors or high walls). In order for
non-player characters to handle such scenarios we present invJPS, an
"inventory-driven" pathfinding approach based on the highly successful
grid-based Jump-Point-Search (JPS) algorithm. We show, formally and
experimentally, that the invJPS preserves JPS's optimality guarantees and its
symmetry breaking advantages in inventory-based variants of game maps.
</summary>
    <author>
      <name>Davide Aversa</name>
    </author>
    <author>
      <name>Sebastian Sardina</name>
    </author>
    <author>
      <name>Stavros Vassos</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the AAAI Conference on Artificial Intelligence
  and Interactive Digital Entertainment (AIIDE), pp. 2-8, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.00715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00695v1</id>
    <updated>2016-07-03T22:44:57Z</updated>
    <published>2016-07-03T22:44:57Z</published>
    <title>Can we reach Pareto optimal outcomes using bottom-up approaches?</title>
    <summary>  Traditionally, researchers in decision making have focused on attempting to
reach Pareto Optimality using horizontal approaches, where optimality is
calculated taking into account every participant at the same time. Sometimes,
this may prove to be a difficult task (e.g., conflict, mistrust, no information
sharing, etc.). In this paper, we explore the possibility of achieving Pareto
Optimal outcomes in a group by using a bottom-up approach: discovering Pareto
optimal outcomes by interacting in subgroups. We analytically show that Pareto
optimal outcomes in a subgroup are also Pareto optimal in a supergroup of those
agents in the case of strict, transitive, and complete preferences. Then, we
empirically analyze the prospective usability and practicality of bottom-up
approaches in a variety of decision making domains.
</summary>
    <author>
      <name>Victor Sanchez-Anguix</name>
    </author>
    <author>
      <name>Reyhan Aydogan</name>
    </author>
    <author>
      <name>Tim Baarslag</name>
    </author>
    <author>
      <name>Catholijn M. Jonker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2nd Workshop on Conflict Resolution in Decision Making
  (COREDEMA@ECAI2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00656v1</id>
    <updated>2016-07-03T17:11:52Z</updated>
    <published>2016-07-03T17:11:52Z</published>
    <title>A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning
  and Plan Caching</title>
    <summary>  This article presents an agent architecture for controlling an autonomous
agent in stochastic environments. The architecture combines the partially
observable Markov decision process (POMDP) model with the
belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent
architecture takes the best features from the two approaches, that is, the
online generation of reward-maximizing courses of action from POMDP theory, and
sophisticated multiple goal management from BDI theory. We introduce the
advances made since the introduction of the basic architecture, including (i)
the ability to pursue multiple goals simultaneously and (ii) a plan library for
storing pre-written plans and for storing recently generated plans for future
reuse. A version of the architecture without the plan library is implemented
and is evaluated using simulations. The results of the simulation experiments
indicate that the approach is feasible.
</summary>
    <author>
      <name>Gavin Rens</name>
    </author>
    <author>
      <name>Deshendran Moodley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 3 figures, unpublished version</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00623v1</id>
    <updated>2016-07-03T10:30:40Z</updated>
    <published>2016-07-03T10:30:40Z</published>
    <title>Visualizing Natural Language Descriptions: A Survey</title>
    <summary>  A natural language interface exploits the conceptual simplicity and
naturalness of the language to create a high-level user-friendly communication
channel between humans and machines. One of the promising applications of such
interfaces is generating visual interpretations of semantic content of a given
natural language that can be then visualized either as a static scene or a
dynamic animation. This survey discusses requirements and challenges of
developing such systems and reports 26 graphical systems that exploit natural
language interfaces and addresses both artificial intelligence and
visualization aspects. This work serves as a frame of reference to researchers
and to enable further advances in the field.
</summary>
    <author>
      <name>Kaveh Hassani</name>
    </author>
    <author>
      <name>Won-Sook Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2932710</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2932710" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Due to copyright most of the figures only appear in the journal
  version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Computing Surveys, Volume 49 Issue 1, Article No. 17, June
  2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.00623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00474v1</id>
    <updated>2016-07-02T07:41:45Z</updated>
    <published>2016-07-02T07:41:45Z</published>
    <title>Adaptive Neighborhood Graph Construction for Inference in
  Multi-Relational Networks</title>
    <summary>  A neighborhood graph, which represents the instances as vertices and their
relations as weighted edges, is the basis of many semi-supervised and
relational models for node labeling and link prediction. Most methods employ a
sequential process to construct the neighborhood graph. This process often
consists of generating a candidate graph, pruning the candidate graph to make a
neighborhood graph, and then performing inference on the variables (i.e.,
nodes) in the neighborhood graph. In this paper, we propose a framework that
can dynamically adapt the neighborhood graph based on the states of variables
from intermediate inference results, as well as structural properties of the
relations connecting them. A key strength of our framework is its ability to
handle multi-relational data and employ varying amounts of relations for each
instance based on the intermediate inference results. We formulate the link
prediction task as inference on neighborhood graphs, and include preliminary
results illustrating the effects of different strategies in our proposed
framework.
</summary>
    <author>
      <name>Shobeir Fakhraei</name>
    </author>
    <author>
      <name>Dhanya Sridhar</name>
    </author>
    <author>
      <name>Jay Pujara</name>
    </author>
    <author>
      <name>Lise Getoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at SIGKDD 12th International Workshop on Mining and
  Learning with Graphs (MLG'16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00446v1</id>
    <updated>2016-07-02T01:33:00Z</updated>
    <published>2016-07-02T01:33:00Z</published>
    <title>A Greedy Approach to Adapting the Trace Parameter for Temporal
  Difference Learning</title>
    <summary>  One of the main obstacles to broad application of reinforcement learning
methods is the parameter sensitivity of our core learning algorithms. In many
large-scale applications, online computation and function approximation
represent key strategies in scaling up reinforcement learning algorithms. In
this setting, we have effective and reasonably well understood algorithms for
adapting the learning-rate parameter, online during learning. Such
meta-learning approaches can improve robustness of learning and enable
specialization to current task, improving learning speed. For
temporal-difference learning algorithms which we study here, there is yet
another parameter, $\lambda$, that similarly impacts learning speed and
stability in practice. Unfortunately, unlike the learning-rate parameter,
$\lambda$ parametrizes the objective function that temporal-difference methods
optimize. Different choices of $\lambda$ produce different fixed-point
solutions, and thus adapting $\lambda$ online and characterizing the
optimization is substantially more complex than adapting the learning-rate
parameter. There are no meta-learning method for $\lambda$ that can achieve (1)
incremental updating, (2) compatibility with function approximation, and (3)
maintain stability of learning under both on and off-policy sampling. In this
paper we contribute a novel objective function for optimizing $\lambda$ as a
function of state rather than time. We derive a new incremental, linear
complexity $\lambda$-adaption algorithm that does not require offline batch
updating or access to a model of the world, and present a suite of experiments
illustrating the practicality of our new algorithm in three different settings.
Taken together, our contributions represent a concrete step towards black-box
application of temporal-difference learning methods in real world problems.
</summary>
    <author>
      <name>Martha White</name>
    </author>
    <author>
      <name>Adam White</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00428v1</id>
    <updated>2016-07-01T22:52:57Z</updated>
    <published>2016-07-01T22:52:57Z</published>
    <title>Situated Structure Learning of a Bayesian Logic Network for Commonsense
  Reasoning</title>
    <summary>  This paper details the implementation of an algorithm for automatically
generating a high-level knowledge network to perform commonsense reasoning,
specifically with the application of robotic task repair. The network is
represented using a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz
2009), which combines a set of directed relations between abstract concepts,
including IsA, AtLocation, HasProperty, and UsedFor, with a corresponding
probability distribution that models the uncertainty inherent in these
relations. Inference over this network enables reasoning over the abstract
concepts in order to perform appropriate object substitution or to locate
missing objects in the robot's environment. The structure of the network is
generated by combining information from two existing knowledge sources:
ConceptNet (Speer and Havasi 2012), and WordNet (Miller 1995). This is done in
a "situated" manner by only including information relevant a given context.
Results show that the generated network is able to accurately predict object
categories, locations, properties, and affordances in three different household
scenarios.
</summary>
    <author>
      <name>Haley Garrison</name>
    </author>
    <author>
      <name>Sonia Chernova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Joint Conference on Artificial Intelligence (IJCAI),
  StarAI workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00424v1</id>
    <updated>2016-07-01T22:11:38Z</updated>
    <published>2016-07-01T22:11:38Z</published>
    <title>Learning Relational Dependency Networks for Relation Extraction</title>
    <summary>  We consider the task of KBP slot filling -- extracting relation information
from newswire documents for knowledge base construction. We present our
pipeline, which employs Relational Dependency Networks (RDNs) to learn
linguistic patterns for relation extraction. Additionally, we demonstrate how
several components such as weak supervision, word2vec features, joint learning
and the use of human advice, can be incorporated in this relational framework.
We evaluate the different components in the benchmark KBP 2015 task and show
that RDNs effectively model a diverse set of features and perform competitively
with current state-of-the-art relation extraction.
</summary>
    <author>
      <name>Dileep Viswanathan</name>
    </author>
    <author>
      <name>Ameet Soni</name>
    </author>
    <author>
      <name>Jude Shavlik</name>
    </author>
    <author>
      <name>Sriraam Natarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of Sixth International Workshop on Statistical
  Relational AI at the 25th International Joint Conference on Artificial
  Intelligence (IJCAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00410v1</id>
    <updated>2016-07-01T21:24:21Z</updated>
    <published>2016-07-01T21:24:21Z</published>
    <title>Domain Adaptation for Neural Networks by Parameter Augmentation</title>
    <summary>  We propose a simple domain adaptation method for neural networks in a
supervised setting. Supervised domain adaptation is a way of improving the
generalization performance on the target domain by using the source domain
dataset, assuming that both of the datasets are labeled. Recently, recurrent
neural networks have been shown to be successful on a variety of NLP tasks such
as caption generation; however, the existing domain adaptation techniques are
limited to (1) tune the model parameters by the target dataset after the
training by the source dataset, or (2) design the network to have dual output,
one for the source domain and the other for the target domain. Reformulating
the idea of the domain adaptation technique proposed by Daume (2007), we
propose a simple domain adaptation method, which can be applied to neural
networks trained with a cross-entropy loss. On captioning datasets, we show
performance improvements over other domain adaptation methods.
</summary>
    <author>
      <name>Yusuke Watanabe</name>
    </author>
    <author>
      <name>Kazuma Hashimoto</name>
    </author>
    <author>
      <name>Yoshimasa Tsuruoka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 page. To appear in the first ACL Workshop on Representation
  Learning for NLP</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00279v1</id>
    <updated>2016-07-01T15:07:52Z</updated>
    <published>2016-07-01T15:07:52Z</published>
    <title>Meaningful Models: Utilizing Conceptual Structure to Improve Machine
  Learning Interpretability</title>
    <summary>  The last decade has seen huge progress in the development of advanced machine
learning models; however, those models are powerless unless human users can
interpret them. Here we show how the mind's construction of concepts and
meaning can be used to create more interpretable machine learning models. By
proposing a novel method of classifying concepts, in terms of 'form' and
'function', we elucidate the nature of meaning and offer proposals to improve
model understandability. As machine learning begins to permeate daily life,
interpretable models may serve as a bridge between domain-expert authors and
non-expert users.
</summary>
    <author>
      <name>Nick Condry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, presented at 2016 ICML Workshop on Human
  Interpretability in Machine Learning (WHI 2016), New York, NY</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00215v2</id>
    <updated>2016-07-22T22:43:10Z</updated>
    <published>2016-07-01T11:58:28Z</published>
    <title>Why is Posterior Sampling Better than Optimism for Reinforcement
  Learning</title>
    <summary>  Computational results demonstrate that posterior sampling for reinforcement
learning (PSRL) dramatically outperforms algorithms driven by optimism, such as
UCRL2. We provide insight into the extent of this performance boost and the
phenomenon that drives it. We leverage this insight to establish an
$\tilde{O}(H\sqrt{SAT})$ Bayesian expected regret bound for PSRL in
finite-horizon episodic Markov decision processes, where $H$ is the horizon,
$S$ is the number of states, $A$ is the number of actions and $T$ is the time
elapsed. This improves upon the best previous bound of $\tilde{O}(H S
\sqrt{AT})$ for any reinforcement learning algorithm.
</summary>
    <author>
      <name>Ian Osband</name>
    </author>
    <author>
      <name>Benjamin Van Roy</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00215v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00215v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00186v1</id>
    <updated>2016-07-01T10:01:11Z</updated>
    <published>2016-07-01T10:01:11Z</published>
    <title>Throwing fuel on the embers: Probability or Dichotomy, Cognitive or
  Linguistic?</title>
    <summary>  Prof. Robert Berwick's abstract for his forthcoming invited talk at the
ACL2016 workshop on Cognitive Aspects of Computational Language Learning
revives an ancient debate. Entitled "Why take a chance?", Berwick seems to
refer implicitly to Chomsky's critique of the statistical approach of Harris as
well as the currently dominant paradigms in CoNLL.
  Berwick avoids Chomsky's use of "innate" but states that "the debate over the
existence of sophisticated mental grammars was settled with Chomsky's Logical
Structure of Linguistic Theory (1957/1975)", acknowledging that "this debate
has often been revived".
  This paper agrees with the view that this debate has long since been settled,
but with the opposite outcome! Given the embers have not yet died away, and the
questions remain fundamental, perhaps it is appropriate to refuel the debate,
so I would like to join Bob in throwing fuel on this fire by reviewing the
evidence against the Chomskian position!
</summary>
    <author>
      <name>David M. W. Powers</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00148v2</id>
    <updated>2016-07-11T09:33:48Z</updated>
    <published>2016-07-01T08:25:48Z</published>
    <title>LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection</title>
    <summary>  Mechanical devices such as engines, vehicles, aircrafts, etc., are typically
instrumented with numerous sensors to capture the behavior and health of the
machine. However, there are often external factors or variables which are not
captured by sensors leading to time-series which are inherently unpredictable.
For instance, manual controls and/or unmonitored environmental conditions or
load may lead to inherently unpredictable time-series. Detecting anomalies in
such scenarios becomes challenging using standard approaches based on
mathematical models that rely on stationarity, or prediction models that
utilize prediction errors to detect anomalies. We propose a Long Short Term
Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD)
that learns to reconstruct 'normal' time-series behavior, and thereafter uses
reconstruction error to detect anomalies. We experiment with three publicly
available quasi predictable time-series datasets: power demand, space shuttle,
and ECG, and two real-world engine datasets with both predictive and
unpredictable behavior. We show that EncDec-AD is robust and can detect
anomalies from predictable, unpredictable, periodic, aperiodic, and
quasi-periodic time-series. Further, we show that EncDec-AD is able to detect
anomalies from short time-series (length as small as 30) as well as long
time-series (length as large as 500).
</summary>
    <author>
      <name>Pankaj Malhotra</name>
    </author>
    <author>
      <name>Anusha Ramakrishnan</name>
    </author>
    <author>
      <name>Gaurangi Anand</name>
    </author>
    <author>
      <name>Lovekesh Vig</name>
    </author>
    <author>
      <name>Puneet Agarwal</name>
    </author>
    <author>
      <name>Gautam Shroff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICML 2016 Anomaly Detection Workshop, New York, NY, USA,
  2016. Reference update in this version (v2)</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00136v1</id>
    <updated>2016-07-01T07:34:50Z</updated>
    <published>2016-07-01T07:34:50Z</published>
    <title>Missing Data Estimation in High-Dimensional Datasets: A Swarm
  Intelligence-Deep Neural Network Approach</title>
    <summary>  In this paper, we examine the problem of missing data in high-dimensional
datasets by taking into consideration the Missing Completely at Random and
Missing at Random mechanisms, as well as theArbitrary missing pattern.
Additionally, this paper employs a methodology based on Deep Learning and Swarm
Intelligence algorithms in order to provide reliable estimates for missing
data. The deep learning technique is used to extract features from the input
data via an unsupervised learning approach by modeling the data distribution
based on the input. This deep learning technique is then used as part of the
objective function for the swarm intelligence technique in order to estimate
the missing data after a supervised fine-tuning phase by minimizing an error
function based on the interrelationship and correlation between features in the
dataset. The investigated methodology in this paper therefore has longer
running times, however, the promising potential outcomes justify the trade-off.
Also, basic knowledge of statistics is presumed.
</summary>
    <author>
      <name>Collins Leke</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00087v1</id>
    <updated>2016-07-01T00:54:10Z</updated>
    <published>2016-07-01T00:54:10Z</published>
    <title>Fractal Dimension Pattern Based Multiresolution Analysis for Rough
  Estimator of Person-Dependent Audio Emotion Recognition</title>
    <summary>  As a general means of expression, audio analysis and recognition has
attracted much attentions for its wide applications in real-life world. Audio
emotion recognition (AER) attempts to understand emotional states of human with
the given utterance signals, and has been studied abroad for its further
development on friendly human-machine interfaces. Distinguish from other
existing works, the person-dependent patterns of audio emotions are conducted,
and fractal dimension features are calculated for acoustic feature extraction.
Furthermore, it is able to efficiently learn intrinsic characteristics of
auditory emotions, while the utterance features are learned from fractal
dimensions of each sub-bands. Experimental results show the proposed method is
able to provide comparative performance for audio emotion recognition.
</summary>
    <author>
      <name>Miao Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00061v1</id>
    <updated>2016-06-30T22:04:26Z</updated>
    <published>2016-06-30T22:04:26Z</published>
    <title>Towards A Virtual Assistant That Can Be Taught New Tasks In Any Domain
  By Its End-Users</title>
    <summary>  The challenge stated in the title can be divided into two main problems. The
first problem is to reliably mimic the way that users interact with user
interfaces. The second problem is to build an instructible agent, i.e. one that
can be taught to execute tasks expressed as previously unseen natural language
commands. This paper proposes a solution to the second problem, a system we
call Helpa. End-users can teach Helpa arbitrary new tasks whose level of
complexity is similar to the tasks available from today's most popular virtual
assistants. Teaching Helpa does not involve any programming. Instead, users
teach Helpa by providing just one example of a command paired with a
demonstration of how to execute that command. Helpa does not rely on any
pre-existing domain-specific knowledge. It is therefore completely
domain-independent. Our usability study showed that end-users can teach Helpa
many new tasks in less than a minute each, often much less.
</summary>
    <author>
      <name>I. Dan Melamed</name>
    </author>
    <author>
      <name>Nobal B. Niraula</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09637v1</id>
    <updated>2016-06-30T19:50:33Z</updated>
    <published>2016-06-30T19:50:33Z</published>
    <title>Lifted Region-Based Belief Propagation</title>
    <summary>  Due to the intractable nature of exact lifted inference, research has
recently focused on the discovery of accurate and efficient approximate
inference algorithms in Statistical Relational Models (SRMs), such as Lifted
First-Order Belief Propagation. FOBP simulates propositional factor graph
belief propagation without constructing the ground factor graph by identifying
and lifting over redundant message computations. In this work, we propose a
generalization of FOBP called Lifted Generalized Belief Propagation, in which
both the region structure and the message structure can be lifted. This
approach allows more of the inference to be performed intra-region (in the
exact inference step of BP), thereby allowing simulation of propagation on a
graph structure with larger region scopes and fewer edges, while still
maintaining tractability. We demonstrate that the resulting algorithm converges
in fewer iterations to more accurate results on a variety of SRMs.
</summary>
    <author>
      <name>David Smith</name>
    </author>
    <author>
      <name>Parag Singla</name>
    </author>
    <author>
      <name>Vibhav Gogate</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sixth International Workshop on Statistical Relational AI</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09632v1</id>
    <updated>2016-06-30T19:40:56Z</updated>
    <published>2016-06-30T19:40:56Z</published>
    <title>A Permutation-based Model for Crowd Labeling: Optimal Estimation and
  Robustness</title>
    <summary>  The aggregation and denoising of crowd labeled data is a task that has gained
increased significance with the advent of crowdsourcing platforms and massive
datasets. In this paper, we propose a permutation-based model for crowd labeled
data that is a significant generalization of the common Dawid-Skene model, and
introduce a new error metric by which to compare different estimators. Working
in a high-dimensional non-asymptotic framework that allows both the number of
workers and tasks to scale, we derive optimal rates of convergence for the
permutation-based model. We show that the permutation-based model offers
significant robustness in estimation due to its richness, while surprisingly
incurring only a small additional statistical penalty as compared to the
Dawid-Skene model. Finally, we propose a computationally-efficient method,
called the OBI-WAN estimator, that is uniformly optimal over a class
intermediate between the permutation-based and the Dawid-Skene models, and is
uniformly consistent over the entire permutation-based model class. In
contrast, the guarantees for estimators available in prior literature are
sub-optimal over the original Dawid-Skene model.
</summary>
    <author>
      <name>Nihar B. Shah</name>
    </author>
    <author>
      <name>Sivaraman Balakrishnan</name>
    </author>
    <author>
      <name>Martin J. Wainwright</name>
    </author>
    <link href="http://arxiv.org/abs/1606.09632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09594v1</id>
    <updated>2016-06-30T18:03:42Z</updated>
    <published>2016-06-30T18:03:42Z</published>
    <title>Contextual Symmetries in Probabilistic Graphical Models</title>
    <summary>  An important approach for efficient inference in probabilistic graphical
models exploits symmetries among objects in the domain. Symmetric variables
(states) are collapsed into meta-variables (meta-states) and inference
algorithms are run over the lifted graphical model instead of the flat one. Our
paper extends existing definitions of symmetry by introducing the novel notion
of contextual symmetry. Two states that are not globally symmetric, can be
contextually symmetric under some specific assignment to a subset of variables,
referred to as the context variables. Contextual symmetry subsumes previous
symmetry definitions and can rep resent a large class of symmetries not
representable earlier. We show how to compute contextual symmetries by reducing
it to the problem of graph isomorphism. We extend previous work on exploiting
symmetries in the MCMC framework to the case of contextual symmetries. Our
experiments on several domains of interest demonstrate that exploiting
contextual symmetries can result in significant computational gains.
</summary>
    <author>
      <name>Ankit Anand</name>
    </author>
    <author>
      <name>Aditya Grover</name>
    </author>
    <author>
      <name> Mausam</name>
    </author>
    <author>
      <name>Parag Singla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.09594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09577v1</id>
    <updated>2016-06-30T17:06:30Z</updated>
    <published>2016-06-30T17:06:30Z</published>
    <title>Ordering as privileged information</title>
    <summary>  We propose to accelerate the rate of convergence of the pattern recognition
task by directly minimizing the variance diameters of certain hypothesis
spaces, which are critical quantities in fast-convergence results.We show that
the variance diameters can be controlled by dividing hypothesis spaces into
metric balls based on a new order metric. This order metric can be minimized as
an ordinal regression problem, leading to a LUPI (Learning Using Privileged
Information) application where we take the privileged information as some
desired ordering, and construct a faster-converging hypothesis space by
empirically restricting some larger hypothesis space according to that
ordering. We give a risk analysis of the approach. We discuss the difficulties
with model selection and give an innovative technique for selecting multiple
model parameters. Finally, we provide some data experiments.
</summary>
    <author>
      <name>Thomas Vacek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 table, 2 page appendix giving proofs</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09521v1</id>
    <updated>2016-06-30T14:49:01Z</updated>
    <published>2016-06-30T14:49:01Z</published>
    <title>Probabilistic Reasoning in the Description Logic ALCP with the Principle
  of Maximum Entropy (Full Version)</title>
    <summary>  A central question for knowledge representation is how to encode and handle
uncertain knowledge adequately. We introduce the probabilistic description
logic ALCP that is designed for representing context-dependent knowledge, where
the actual context taking place is uncertain. ALCP allows the expression of
logical dependencies on the domain and probabilistic dependencies on the
possible contexts. In order to draw probabilistic conclusions, we employ the
principle of maximum entropy. We provide reasoning algorithms for this logic,
and show that it satisfies several desirable properties of probabilistic
logics.
</summary>
    <author>
      <name>Rafael Peñaloza</name>
    </author>
    <author>
      <name>Nico Potyka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Full version of paper accepted at the Tenth International Conference
  on Scalable Uncertainty Management (SUM 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09449v1</id>
    <updated>2016-06-30T12:14:33Z</updated>
    <published>2016-06-30T12:14:33Z</published>
    <title>Clique-Width and Directed Width Measures for Answer-Set Programming</title>
    <summary>  Disjunctive Answer Set Programming (ASP) is a powerful declarative
programming paradigm whose main decision problems are located on the second
level of the polynomial hierarchy. Identifying tractable fragments and
developing efficient algorithms for such fragments are thus important
objectives in order to complement the sophisticated ASP systems available to
date. Hard problems can become tractable if some problem parameter is bounded
by a fixed constant; such problems are then called fixed-parameter tractable
(FPT). While several FPT results for ASP exist, parameters that relate to
directed or signed graphs representing the program at hand have been neglected
so far. In this paper, we first give some negative observations showing that
directed width measures on the dependency graph of a program do not lead to FPT
results. We then consider the graph parameter of signed clique-width and
present a novel dynamic programming algorithm that is FPT w.r.t. this
parameter. Clique-width is more general than the well-known treewidth, and, to
the best of our knowledge, ours is the first FPT algorithm for bounded
clique-width for reasoning problems beyond SAT.
</summary>
    <author>
      <name>Bernhard Bliem</name>
    </author>
    <author>
      <name>Sebastian Ordyniak</name>
    </author>
    <author>
      <name>Stefan Woltran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short version of this paper has been accepted to ECAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09403v1</id>
    <updated>2016-06-30T09:18:53Z</updated>
    <published>2016-06-30T09:18:53Z</published>
    <title>Learning Crosslingual Word Embeddings without Bilingual Corpora</title>
    <summary>  Crosslingual word embeddings represent lexical items from different languages
in the same vector space, enabling transfer of NLP tools. However, previous
attempts had expensive resource requirements, difficulty incorporating
monolingual data or were unable to handle polysemy. We address these drawbacks
in our method which takes advantage of a high coverage dictionary in an EM
style training algorithm over monolingual corpora in two languages. Our model
achieves state-of-the-art performance on bilingual lexicon induction task
exceeding models using large bilingual corpora, and competitive results on the
monolingual word similarity and cross-lingual document classification task.
</summary>
    <author>
      <name>Long Duong</name>
    </author>
    <author>
      <name>Hiroshi Kanayama</name>
    </author>
    <author>
      <name>Tengfei Ma</name>
    </author>
    <author>
      <name>Steven Bird</name>
    </author>
    <author>
      <name>Trevor Cohn</name>
    </author>
    <link href="http://arxiv.org/abs/1606.09403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09242v1</id>
    <updated>2016-06-30T08:30:54Z</updated>
    <published>2016-06-30T08:30:54Z</published>
    <title>Swift: Compiled Inference for Probabilistic Programming Languages</title>
    <summary>  A probabilistic program defines a probability measure over its semantic
structures. One common goal of probabilistic programming languages (PPLs) is to
compute posterior probabilities for arbitrary models and queries, given
observed evidence, using a generic inference engine. Most PPL inference
engines---even the compiled ones---incur significant runtime interpretation
overhead, especially for contingent and open-universe models. This paper
describes Swift, a compiler for the BLOG PPL. Swift-generated code incorporates
optimizations that eliminate interpretation overhead, maintain dynamic
dependencies efficiently, and handle memory management for possible worlds of
varying sizes. Experiments comparing Swift with other PPL engines on a variety
of inference problems demonstrate speedups ranging from 12x to 326x.
</summary>
    <author>
      <name>Yi Wu</name>
    </author>
    <author>
      <name>Lei Li</name>
    </author>
    <author>
      <name>Stuart Russell</name>
    </author>
    <author>
      <name>Rastislav Bodik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00234v1</id>
    <updated>2016-06-30T02:17:59Z</updated>
    <published>2016-06-30T02:17:59Z</published>
    <title>Neutrosophic Overset, Neutrosophic Underset, and Neutrosophic Offset.
  Similarly for Neutrosophic Over-/Under-/Off- Logic, Probability, and
  Statistics</title>
    <summary>  Neutrosophic Over-/Under-/Off-Set and -Logic were defined by the author in
1995 and published for the first time in 2007. We extended the neutrosophic set
respectively to Neutrosophic Overset {when some neutrosophic component is over
1}, Neutrosophic Underset {when some neutrosophic component is below 0}, and to
Neutrosophic Offset {when some neutrosophic components are off the interval [0,
1], i.e. some neutrosophic component over 1 and other neutrosophic component
below 0}. This is no surprise with respect to the classical fuzzy set/logic,
intuitionistic fuzzy set/logic, or classical/imprecise probability, where the
values are not allowed outside the interval [0, 1], since our real-world has
numerous examples and applications of over-/under-/off-neutrosophic components.
For example, person working overtime deserves a membership degree over 1, while
a person producing more damage than benefit to a company deserves a membership
below 0. Then, similarly, the Neutrosophic Logic/Measure/Probability/Statistics
etc. were extended to respectively Neutrosophic Over-/Under-/Off-Logic,
-Measure, -Probability, -Statistics etc. [Smarandache, 2007].
</summary>
    <author>
      <name>Florentin Smarandache</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">170 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pons Editions, Bruxelles, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.00234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94D05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09296v1</id>
    <updated>2016-06-29T21:35:24Z</updated>
    <published>2016-06-29T21:35:24Z</published>
    <title>How Many Folders Do You Really Need?</title>
    <summary>  Email classification is still a mostly manual task. Consequently, most Web
mail users never define a single folder. Recently however, automatic
classification offering the same categories to all users has started to appear
in some Web mail clients, such as AOL or Gmail. We adopt this approach, rather
than previous (unsuccessful) personalized approaches because of the change in
the nature of consumer email traffic, which is now dominated by (non-spam)
machine-generated email. We propose here a novel approach for (1) automatically
distinguishing between personal and machine-generated email and (2) classifying
messages into latent categories, without requiring users to have defined any
folder. We report how we have discovered that a set of 6 "latent" categories
(one for human- and the others for machine-generated messages) can explain a
significant portion of email traffic. We describe in details the steps involved
in building a Web-scale email categorization system, from the collection of
ground-truth labels, the selection of features to the training of models.
Experimental evaluation was performed on more than 500 billion messages
received during a period of six months by users of Yahoo mail service, who
elected to be part of such research studies. Our system achieved precision and
recall rates close to 90% and the latent categories we discovered were shown to
cover 70% of both email traffic and email search queries. We believe that these
results pave the way for a change of approach in the Web mail industry, and
could support the invention of new large-scale email discovery paradigms that
had not been possible before.
</summary>
    <author>
      <name>Mihajlo Grbovic</name>
    </author>
    <author>
      <name>Guy Halawi</name>
    </author>
    <author>
      <name>Zohar Karnin</name>
    </author>
    <author>
      <name>Yoelle Maarek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2661829.2662018.</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2661829.2662018." rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures, Proceedings of the 23rd ACM International
  Conference on Information and Knowledge Management (CIKM 2014), Shanghai,
  China</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 23rd ACM International Conference on
  Information and Knowledge Management (CIKM 2014), Shanghai, China</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.09296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09274v1</id>
    <updated>2016-06-29T20:36:23Z</updated>
    <published>2016-06-29T20:36:23Z</published>
    <title>Compression of Neural Machine Translation Models via Pruning</title>
    <summary>  Neural Machine Translation (NMT), like many other deep learning domains,
typically suffers from over-parameterization, resulting in large storage sizes.
This paper examines three simple magnitude-based pruning schemes to compress
NMT models, namely class-blind, class-uniform, and class-distribution, which
differ in terms of how pruning thresholds are computed for the different
classes of weights in the NMT architecture. We demonstrate the efficacy of
weight pruning as a compression technique for a state-of-the-art NMT system. We
show that an NMT model with over 200 million parameters can be pruned by 40%
with very little performance loss as measured on the WMT'14 English-German
translation task. This sheds light on the distribution of redundancy in the NMT
architecture. Our main result is that with retraining, we can recover and even
surpass the original performance with an 80%-pruned model.
</summary>
    <author>
      <name>Abigail See</name>
    </author>
    <author>
      <name>Minh-Thang Luong</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CoNLL 2016. 9 pages plus references</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09140v2</id>
    <updated>2016-07-21T11:44:51Z</updated>
    <published>2016-06-29T15:00:48Z</published>
    <title>Algebraic foundations for qualitative calculi and networks</title>
    <summary>  A qualitative representation $\phi$ is like an ordinary representation of a
relation algebra, but instead of requiring $(a; b)^\phi = a^\phi | b^\phi$, as
we do for ordinary representations, we only require that $c^\phi\supseteq
a^\phi | b^\phi \iff c\geq a ; b$, for each $c$ in the algebra. A constraint
network is qualitatively satisfiable if its nodes can be mapped to elements of
a qualitative representation, preserving the constraints. If a constraint
network is satisfiable then it is clearly qualitatively satisfiable, but the
converse can fail. However, for a wide range of relation algebras including the
point algebra, the Allen Interval Algebra, RCC8 and many others, a network is
satisfiable if and only if it is qualitatively satisfiable.
  Unlike ordinary composition, the weak composition arising from qualitative
representations need not be associative, so we can generalise by considering
network satisfaction problems over non-associative algebras. We prove that
computationally, qualitative representations have many advantages over ordinary
representations: whereas many finite relation algebras have only infinite
representations, every finite qualitatively representable algebra has a finite
qualitative representation; the representability problem for (the atom
structures of) finite non-associative algebras is NP-complete; the network
satisfaction problem over a finite qualitatively representable algebra is
always in NP; the validity of equations over qualitative representations is
co-NP-complete. On the other hand we prove that there is no finite
axiomatisation of the class of qualitatively representable algebras.
</summary>
    <author>
      <name>Robin Hirsch</name>
    </author>
    <author>
      <name>Marcel Jackson</name>
    </author>
    <author>
      <name>Tomasz Kowalski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09140v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09140v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08965v3</id>
    <updated>2016-08-09T18:44:35Z</updated>
    <published>2016-06-29T06:13:22Z</published>
    <title>Credibilistic TOPSIS Model for Evaluation and Selection of Municipal
  Solid Waste Disposal Methods</title>
    <summary>  Municipal solid waste management (MSWM) is a challenging issue of urban
development in developing countries. Each country having different
socio-economic-environmental background, might not accept a particular disposal
method as the optimal choice. Selection of suitable disposal method in MSWM,
under vague and imprecise information can be considered as multi criteria
decision making problem (MCDM). In the present paper, TOPSIS (Technique for
Order Preference by Similarity to Ideal Solution) methodology is extended based
on credibility theory for evaluating the performances of MSW disposal methods
under some criteria fixed by experts. The proposed model helps decision makers
to choose a preferable alternative for their municipal area. A sensitivity
analysis by our proposed model confirms this fact.
</summary>
    <author>
      <name>Jagannath Roy</name>
    </author>
    <author>
      <name>Krishnendu Adhikary</name>
    </author>
    <author>
      <name>Samarjit Kar</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08965v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08965v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08963v1</id>
    <updated>2016-06-29T06:00:35Z</updated>
    <published>2016-06-29T06:00:35Z</published>
    <title>Non-linear Label Ranking for Large-scale Prediction of Long-Term User
  Interests</title>
    <summary>  We consider the problem of personalization of online services from the
viewpoint of ad targeting, where we seek to find the best ad categories to be
shown to each user, resulting in improved user experience and increased
advertisers' revenue. We propose to address this problem as a task of ranking
the ad categories depending on a user's preference, and introduce a novel label
ranking approach capable of efficiently learning non-linear, highly accurate
models in large-scale settings. Experiments on a real-world advertising data
set with more than 3.2 million users show that the proposed algorithm
outperforms the existing solutions in terms of both rank loss and top-K
retrieval performance, strongly suggesting the benefit of using the proposed
model on large-scale ranking problems.
</summary>
    <author>
      <name>Nemanja Djuric</name>
    </author>
    <author>
      <name>Mihajlo Grbovic</name>
    </author>
    <author>
      <name>Vladan Radosavljevic</name>
    </author>
    <author>
      <name>Narayan Bhamidipati</name>
    </author>
    <author>
      <name>Slobodan Vucetic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28th AAAI Conference on Artificial Intelligence (AAAI-14)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08962v2</id>
    <updated>2016-08-25T07:07:30Z</updated>
    <published>2016-06-29T06:00:32Z</published>
    <title>Evaluation and selection of Medical Tourism sites: A rough AHP based
  MABAC approach</title>
    <summary>  In this paper, a novel multiple criteria decision making (MCDM) methodology
is presented for assessing and prioritizing medical tourism destinations in
uncertain environment. A systematic evaluation and assessment method is
proposed by integrating rough number based AHP (Analytic Hierarchy Process) and
rough number based MABAC (Multi-Attributive Border Approximation area
Comparison). Rough number is used to aggregate individual judgments and
preferences to deal with vagueness in decision making due to limited data.
Rough AHP analyzes the relative importance of criteria based on their
preferences given by experts. Rough MABAC evaluates the alternative sites based
on the criteria weights. The proposed methodology is explained through a case
study considering different cities for healthcare service in India. The
validity of the obtained ranking for the given decision making problem is
established by testing criteria proposed by Wang and Triantaphyllou (2008)
along with further analysis and discussion.
</summary>
    <author>
      <name>Jagannath Roy</name>
    </author>
    <author>
      <name>Kajal Chatterjee</name>
    </author>
    <author>
      <name>Abhirup Bandhopadhyay</name>
    </author>
    <author>
      <name>Samarjit Kar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08962v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08962v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08954v1</id>
    <updated>2016-06-29T05:01:56Z</updated>
    <published>2016-06-29T05:01:56Z</published>
    <title>Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs</title>
    <summary>  We present a transition-based parser that jointly produces syntactic and
semantic dependencies. It learns a representation of the entire algorithm
state, using stack long short-term memories. Our greedy inference algorithm has
linear time, including feature extraction. On the CoNLL 2008--9 English shared
tasks, we obtain the best published parsing performance among models that
jointly learn syntax and semantics.
</summary>
    <author>
      <name>Swabha Swayamdipta</name>
    </author>
    <author>
      <name>Miguel Ballesteros</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Noah A. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, accepted to CoNLL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08928v1</id>
    <updated>2016-06-29T01:05:36Z</updated>
    <published>2016-06-29T01:05:36Z</published>
    <title>subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs
  from Large Graphs</title>
    <summary>  In this paper, we present subgraph2vec, a novel approach for learning latent
representations of rooted subgraphs from large graphs inspired by recent
advancements in Deep Learning and Graph Kernels. These latent representations
encode semantic substructure dependencies in a continuous vector space, which
is easily exploited by statistical models for tasks such as graph
classification, clustering, link prediction and community detection.
subgraph2vec leverages on local information obtained from neighbourhoods of
nodes to learn their latent representations in an unsupervised fashion. We
demonstrate that subgraph vectors learnt by our approach could be used in
conjunction with classifiers such as CNNs, SVMs and relational data clustering
algorithms to achieve significantly superior accuracies. Also, we show that the
subgraph vectors could be used for building a deep learning variant of
Weisfeiler-Lehman graph kernel. Our experiments on several benchmark and
large-scale real-world datasets reveal that subgraph2vec achieves significant
improvements in accuracies over existing graph kernels on both supervised and
unsupervised learning tasks. Specifically, on two realworld program analysis
tasks, namely, code clone and malware detection, subgraph2vec outperforms
state-of-the-art kernels by more than 17% and 4%, respectively.
</summary>
    <author>
      <name>Annamalai Narayanan</name>
    </author>
    <author>
      <name>Mahinthan Chandramohan</name>
    </author>
    <author>
      <name>Lihui Chen</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Santhoshkumar Saminathan</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08906v1</id>
    <updated>2016-06-28T22:36:38Z</updated>
    <published>2016-06-28T22:36:38Z</published>
    <title>Exploring high-level Perspectives on Self-Configuration Capabilities of
  Systems</title>
    <summary>  Optimization of product performance repetitively introduces the need to make
products adaptive in a more general sense. This more general idea is often
captured under the term 'self-configuration'. Despite the importance of such
capability, research work on this feature appears isolated by technical
domains. It is not easy to tell quickly whether the approaches chosen in
different technological domains introduce new ideas or whether the differences
just reflect domain idiosyncrasies. For the sake of easy identification of key
differences between systems with self-configuring capabilities, I will explore
higher level concepts for understanding self-configuration, such as the
{\Omega}-units, in order to provide theoretical instruments for connecting
different areas of technology and research.
</summary>
    <author>
      <name>Aleksander Lodwich</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.2945.6885</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.2945.6885" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 62 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08896v1</id>
    <updated>2016-06-28T21:43:19Z</updated>
    <published>2016-06-28T21:43:19Z</published>
    <title>On the Semantic Relationship between Probabilistic Soft Logic and Markov
  Logic</title>
    <summary>  Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL) are widely
applied formalisms in Statistical Relational Learning, an emerging area in
Artificial Intelligence that is concerned with combining logical and
statistical AI. Despite their resemblance, the relationship has not been
formally stated. In this paper, we describe the precise semantic relationship
between them from a logical perspective. This is facilitated by first extending
fuzzy logic to allow weights, which can be also viewed as a generalization of
PSL, and then relate that generalization to MLN. We observe that the
relationship between PSL and MLN is analogous to the known relationship between
fuzzy logic and Boolean logic, and furthermore the weight scheme of PSL is
essentially a generalization of the weight scheme of MLN for the many-valued
setting.
</summary>
    <author>
      <name>Joohyung Lee</name>
    </author>
    <author>
      <name>Yi Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Working Notes of the 6th International Workshop on Statistical
  Relational AI</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08866v1</id>
    <updated>2016-06-28T20:04:07Z</updated>
    <published>2016-06-28T20:04:07Z</published>
    <title>Technical Report: Towards a Universal Code Formatter through Machine
  Learning</title>
    <summary>  There are many declarative frameworks that allow us to implement code
formatters relatively easily for any specific language, but constructing them
is cumbersome. The first problem is that "everybody" wants to format their code
differently, leading to either many formatter variants or a ridiculous number
of configuration options. Second, the size of each implementation scales with a
language's grammar size, leading to hundreds of rules.
  In this paper, we solve the formatter construction problem using a novel
approach, one that automatically derives formatters for any given language
without intervention from a language expert. We introduce a code formatter
called CodeBuff that uses machine learning to abstract formatting rules from a
representative corpus, using a carefully designed feature set. Our experiments
on Java, SQL, and ANTLR grammars show that CodeBuff is efficient, has excellent
accuracy, and is grammar invariant for a given language. It also generalizes to
a 4th language tested during manuscript preparation.
</summary>
    <author>
      <name>Terence Parr</name>
    </author>
    <author>
      <name>Jurgin Vinju</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08842v1</id>
    <updated>2016-06-28T19:59:52Z</updated>
    <published>2016-06-28T19:59:52Z</published>
    <title>Active Ranking from Pairwise Comparisons and the Futility of Parametric
  Assumptions</title>
    <summary>  We consider sequential or active ranking of a set of n items based on noisy
pairwise comparisons. Items are ranked according to the probability that a
given item beats a randomly chosen item, and ranking refers to partitioning the
items into sets of pre-specified sizes according to their scores. This notion
of ranking includes as special cases the identification of the top-k items and
the total ordering of the items. We first analyze a sequential ranking
algorithm that counts the number of comparisons won, and uses these counts to
decide whether to stop, or to compare another pair of items, chosen based on
confidence intervals specified by the data collected up to that point. We prove
that this algorithm succeeds in recovering the ranking using a number of
comparisons that is optimal up to logarithmic factors. This guarantee does not
require any structural properties of the underlying pairwise probability
matrix, unlike a significant body of past work on pairwise ranking based on
parametric models such as the Thurstone or Bradley-Terry-Luce models. It has
been a long-standing open question as to whether or not imposing these
parametric assumptions allow for improved ranking algorithms. Our second
contribution settles this issue in the context of the problem of active ranking
from pairwise comparisons: by means of tight lower bounds, we prove that
perhaps surprisingly, these popular parametric modeling choices offer little
statistical advantage.
</summary>
    <author>
      <name>Reinhard Heckel</name>
    </author>
    <author>
      <name>Nihar B. Shah</name>
    </author>
    <author>
      <name>Kannan Ramchandran</name>
    </author>
    <author>
      <name>Martin J. Wainwright</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08808v1</id>
    <updated>2016-06-28T18:15:32Z</updated>
    <published>2016-06-28T18:15:32Z</published>
    <title>Adaptive Training of Random Mapping for Data Quantization</title>
    <summary>  Data quantization learns encoding results of data with certain requirements,
and provides a broad perspective of many real-world applications to data
handling. Nevertheless, the results of encoder is usually limited to
multivariate inputs with the random mapping, and side information of binary
codes are hardly to mostly depict the original data patterns as possible. In
the literature, cosine based random quantization has attracted much attentions
due to its intrinsic bounded results. Nevertheless, it usually suffers from the
uncertain outputs, and information of original data fails to be fully preserved
in the reduced codes. In this work, a novel binary embedding method, termed
adaptive training quantization (ATQ), is proposed to learn the ideal transform
of random encoder, where the limitation of cosine random mapping is tackled. As
an adaptive learning idea, the reduced mapping is adaptively calculated with
idea of data group, while the bias of random transform is to be improved to
hold most matching information. Experimental results show that the proposed
method is able to obtain outstanding performance compared with other random
quantization methods.
</summary>
    <author>
      <name>Miao Cheng</name>
    </author>
    <author>
      <name>Ah Chung Tsoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, 15.8</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08777v1</id>
    <updated>2016-06-28T16:31:50Z</updated>
    <published>2016-06-28T16:31:50Z</published>
    <title>"Show me the cup": Reference with Continuous Representations</title>
    <summary>  One of the most basic functions of language is to refer to objects in a
shared scene. Modeling reference with continuous representations is challenging
because it requires individuation, i.e., tracking and distinguishing an
arbitrary number of referents. We introduce a neural network model that, given
a definite description and a set of objects represented by natural images,
points to the intended object if the expression has a unique referent, or
indicates a failure, if it does not. The model, directly trained on reference
acts, is competitive with a pipeline manually engineered to perform the same
task, both when referents are purely visual, and when they are characterized by
a combination of visual and linguistic properties.
</summary>
    <author>
      <name>Gemma Boleda</name>
    </author>
    <author>
      <name>Sebastian Padó</name>
    </author>
    <author>
      <name>Marco Baroni</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09581v2</id>
    <updated>2016-07-18T08:14:43Z</updated>
    <published>2016-06-28T07:00:07Z</published>
    <title>Performance Based Evaluation of Various Machine Learning Classification
  Techniques for Chronic Kidney Disease Diagnosis</title>
    <summary>  Areas where Artificial Intelligence (AI) &amp; related fields are finding their
applications are increasing day by day, moving from core areas of computer
science they are finding their applications in various other domains.In recent
times Machine Learning i.e. a sub-domain of AI has been widely used in order to
assist medical experts and doctors in the prediction, diagnosis and prognosis
of various diseases and other medical disorders. In this manuscript the authors
applied various machine learning algorithms to a problem in the domain of
medical diagnosis and analyzed their efficiency in predicting the results. The
problem selected for the study is the diagnosis of the Chronic Kidney
Disease.The dataset used for the study consists of 400 instances and 24
attributes. The authors evaluated 12 classification techniques by applying them
to the Chronic Kidney Disease data. In order to calculate efficiency, results
of the prediction by candidate methods were compared with the actual medical
results of the subject.The various metrics used for performance evaluation are
predictive accuracy, precision, sensitivity and specificity. The results
indicate that decision-tree performed best with nearly the accuracy of 98.6%,
sensitivity of 0.9720, precision of 1 and specificity of 1.
</summary>
    <author>
      <name>Sahil Sharma</name>
    </author>
    <author>
      <name>Vinod Sharma</name>
    </author>
    <author>
      <name>Atul Sharma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Modern Computer Science, Vol.4, Issue3,
  June 2016, pp.11-16</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.09581v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09581v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08538v1</id>
    <updated>2016-06-28T02:23:58Z</updated>
    <published>2016-06-28T02:23:58Z</published>
    <title>A Local Density-Based Approach for Local Outlier Detection</title>
    <summary>  This paper presents a simple but effective density-based outlier detection
approach with the local kernel density estimation (KDE). A Relative
Density-based Outlier Score (RDOS) is introduced to measure the local
outlierness of objects, in which the density distribution at the location of an
object is estimated with a local KDE method based on extended nearest neighbors
of the object. Instead of using only $k$ nearest neighbors, we further consider
reverse nearest neighbors and shared nearest neighbors of an object for density
distribution estimation. Some theoretical properties of the proposed RDOS
including its expected value and false alarm probability are derived. A
comprehensive experimental study on both synthetic and real-life data sets
demonstrates that our approach is more effective than state-of-the-art outlier
detection methods.
</summary>
    <author>
      <name>Bo Tang</name>
    </author>
    <author>
      <name>Haibo He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 14 figures, submitted to Pattern Recognition Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08531v1</id>
    <updated>2016-06-28T01:43:38Z</updated>
    <published>2016-06-28T01:43:38Z</published>
    <title>A Learning Algorithm for Relational Logistic Regression: Preliminary
  Results</title>
    <summary>  Relational logistic regression (RLR) is a representation of conditional
probability in terms of weighted formulae for modelling multi-relational data.
In this paper, we develop a learning algorithm for RLR models. Learning an RLR
model from data consists of two steps: 1- learning the set of formulae to be
used in the model (a.k.a. structure learning) and learning the weight of each
formula (a.k.a. parameter learning). For structure learning, we deploy Schmidt
and Murphy's hierarchical assumption: first we learn a model with simple
formulae, then more complex formulae are added iteratively only if all their
sub-formulae have proven effective in previous learned models. For parameter
learning, we convert the problem into a non-relational learning problem and use
an off-the-shelf logistic regression learning algorithm from Weka, an
open-source machine learning tool, to learn the weights. We also indicate how
hidden features about the individuals can be incorporated into RLR to boost the
learning performance. We compare our learning algorithm to other structure and
parameter learning algorithms in the literature, and compare the performance of
RLR models to standard logistic regression and RDN-Boost on a modified version
of the MovieLens data-set.
</summary>
    <author>
      <name>Bahare Fatemi</name>
    </author>
    <author>
      <name>Seyed Mehran Kazemi</name>
    </author>
    <author>
      <name>David Poole</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In IJCAI-16 Statistical Relational AI Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08514v2</id>
    <updated>2016-07-02T06:27:03Z</updated>
    <published>2016-06-27T23:51:04Z</published>
    <title>Towards Verified Artificial Intelligence</title>
    <summary>  Verified artificial intelligence (AI) is the goal of designing AI-based
systems that are provably correct with respect to mathematically-specified
requirements. This paper considers Verified AI from a formal methods
perspective. We describe five challenges for achieving Verified AI, and five
corresponding principles for addressing these challenges.
</summary>
    <author>
      <name>Sanjit A. Seshia</name>
    </author>
    <author>
      <name>Dorsa Sadigh</name>
    </author>
    <author>
      <name>S. Shankar Sastry</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08514v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08514v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08362v1</id>
    <updated>2016-06-27T16:44:44Z</updated>
    <published>2016-06-27T16:44:44Z</published>
    <title>A Reduction for Optimizing Lattice Submodular Functions with Diminishing
  Returns</title>
    <summary>  A function $f: \mathbb{Z}_+^E \rightarrow \mathbb{R}_+$ is DR-submodular if
it satisfies $f(\bx + \chi_i) -f (\bx) \ge f(\by + \chi_i) - f(\by)$ for all
$\bx\le \by, i\in E$. Recently, the problem of maximizing a DR-submodular
function $f: \mathbb{Z}_+^E \rightarrow \mathbb{R}_+$ subject to a budget
constraint $\|\bx\|_1 \leq B$ as well as additional constraints has received
significant attention \cite{SKIK14,SY15,MYK15,SY16}.
  In this note, we give a generic reduction from the DR-submodular setting to
the submodular setting. The running time of the reduction and the size of the
resulting submodular instance depends only \emph{logarithmically} on $B$. Using
this reduction, one can translate the results for unconstrained and constrained
submodular maximization to the DR-submodular setting for many types of
constraints in a unified manner.
</summary>
    <author>
      <name>Alina Ene</name>
    </author>
    <author>
      <name>Huy L. Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08359v1</id>
    <updated>2016-06-27T16:39:23Z</updated>
    <published>2016-06-27T16:39:23Z</published>
    <title>Lifted Rule Injection for Relation Embeddings</title>
    <summary>  Methods based on representation learning currently hold the state-of-the-art
in many natural language processing and knowledge base inference tasks. Yet, a
major challenge is how to efficiently incorporate commonsense knowledge into
such models. A recent approach regularizes relation and entity representations
by propositionalization of first-order logic rules. However,
propositionalization does not scale beyond domains with only few entities and
rules. In this paper we present a highly efficient method for incorporating
implication rules into distributed representations for automated knowledge base
construction. We map entity-tuple embeddings into an approximately Boolean
space and encourage a partial ordering over relation embeddings based on
implication rules mined from WordNet. Surprisingly, we find that the strong
restriction of the entity-tuple embedding space does not hurt the
expressiveness of the model and even acts as a regularizer that improves
generalization. By incorporating few commonsense rules, we achieve an increase
of 2 percentage points mean average precision over a matrix factorization
baseline, while observing a negligible increase in runtime.
</summary>
    <author>
      <name>Thomas Demeester</name>
    </author>
    <author>
      <name>Tim Rocktäschel</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08333v1</id>
    <updated>2016-06-27T15:59:32Z</updated>
    <published>2016-06-27T15:59:32Z</published>
    <title>True Lies</title>
    <summary>  A true lie is a lie that becomes true when announced. In a logic of
announcements, where the announcing agent is not modelled, a true lie is a
formula (that is false and) that becomes true when announced. We investigate
true lies and other types of interaction between announced formulas, their
preconditions and their postconditions, in the setting Gerbrandy's logic of
believed announcements, wherein agents may have or obtain incorrect beliefs.
Our results are on the satisfiability and validity of instantiations of these
semantically defined categories, on iterated announcements, including
arbitrarily often iterated announcements, and on syntactic characterization. We
close with results for iterated announcements in the logic of knowledge
(instead of belief), and for lying as private announcements (instead of public
announcements) to different agents. Detailed examples illustrate our lying
concepts.
</summary>
    <author>
      <name>Thomas Ågotnes</name>
    </author>
    <author>
      <name>Hans van Ditmarsch</name>
    </author>
    <author>
      <name>Yanjing Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08140v2</id>
    <updated>2016-07-21T16:24:49Z</updated>
    <published>2016-06-27T06:50:10Z</published>
    <title>STransE: a novel embedding model of entities and relationships in
  knowledge bases</title>
    <summary>  Knowledge bases of real-world facts about entities and their relationships
are useful resources for a variety of natural language processing tasks.
However, because knowledge bases are typically incomplete, it is useful to be
able to perform link prediction, i.e., predict whether a relationship not in
the knowledge base is likely to be true. This paper combines insights from
several previous link prediction models into a new embedding model STransE that
represents each entity as a low-dimensional vector, and each relation by two
matrices and a translation vector. STransE is a simple combination of the SE
and TransE models, but it obtains better link prediction performance on two
benchmark datasets than previous embedding models. Thus, STransE can serve as a
new baseline for the more complex models in the link prediction task.
</summary>
    <author>
      <name>Dat Quoc Nguyen</name>
    </author>
    <author>
      <name>Kairit Sirts</name>
    </author>
    <author>
      <name>Lizhen Qu</name>
    </author>
    <author>
      <name>Mark Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">V1: In Proceedings of NAACL-HLT 2016. V2: Corrected citation to
  (Krompa{\ss} et al., 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08140v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08140v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08130v1</id>
    <updated>2016-06-27T05:53:57Z</updated>
    <published>2016-06-27T05:53:57Z</published>
    <title>Propagators and Solvers for the Algebra of Modular Systems</title>
    <summary>  Solving complex problems can involve non-trivial combinations of distinct
knowledge bases and problem solvers. The Algebra of Modular Systems is a
knowledge representation framework that provides a method for formally
specifying such systems in purely semantic terms. Formally, an expression of
the algebra defines a class of structures. Many expressive formalism used in
practice solve the model expansion task, where a structure is given on the
input and an expansion of this structure in the defined class of structures is
searched (this practice overcomes the common undecidability problem for
expressive logics). In this paper, we construct a solver for the model
expansion task for a complex modular systems from an expression in the algebra
and black-box propagators or solvers for the primitive modules. To this end, we
define a general notion of propagators equipped with an explanation mechanism,
an extension of the alge- bra to propagators, and a lazy conflict-driven
learning algorithm. The result is a framework for seamlessly combining solving
technology from different domains to produce a solver for a combined system.
</summary>
    <author>
      <name>Bart Bogaerts</name>
    </author>
    <author>
      <name>Eugenia Ternovska</name>
    </author>
    <author>
      <name>David Mitchell</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08109v1</id>
    <updated>2016-06-27T01:53:02Z</updated>
    <published>2016-06-27T01:53:02Z</published>
    <title>Can Turing machine be curious about its Turing test results? Three
  informal lectures on physics of intelligence</title>
    <summary>  What is the nature of curiosity? Is there any scientific way to understand
the origin of this mysterious force that drives the behavior of even the
stupidest naturally intelligent systems and is completely absent in their
smartest artificial analogs? Can we build AI systems that could be curious
about something, systems that would have an intrinsic motivation to learn? Is
such a motivation quantifiable? Is it implementable? I will discuss this
problem from the standpoint of physics. The relationship between physics and
intelligence is a consequence of the fact that correctly predicted information
is nothing but an energy resource, and the process of thinking can be viewed as
a process of accumulating and spending this resource through the acts of
perception and, respectively, decision making. The natural motivation of any
autonomous system to keep this accumulation/spending balance as high as
possible allows one to treat the problem of describing the dynamics of thinking
processes as a resource optimization problem. Here I will propose and discuss a
simple theoretical model of such an autonomous system which I call the
Autonomous Turing Machine (ATM). The potential attractiveness of ATM lies in
the fact that it is the model of a self-propelled AI for which the only
available energy resource is the information itself. For ATM, the problem of
optimal thinking, learning, and decision-making becomes conceptually simple and
mathematically well tractable. This circumstance makes the ATM an ideal
playground for studying the dynamics of intelligent behavior and allows one to
quantify many seemingly unquantifiable features of genuine intelligence.
</summary>
    <author>
      <name>Alex Ushveridze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">79 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08104v1</id>
    <updated>2016-06-27T00:58:16Z</updated>
    <published>2016-06-27T00:58:16Z</published>
    <title>Content-Based Top-N Recommendation using Heterogeneous Relations</title>
    <summary>  Top-$N$ recommender systems have been extensively studied. However, the
sparsity of user-item activities has not been well resolved. While many hybrid
systems were proposed to address the cold-start problem, the profile
information has not been sufficiently leveraged. Furthermore, the heterogeneity
of profiles between users and items intensifies the challenge. In this paper,
we propose a content-based top-$N$ recommender system by learning the global
term weights in profiles. To achieve this, we bring in PathSim, which could
well measures the node similarity with heterogeneous relations (between users
and items). Starting from the original TF-IDF value, the global term weights
gradually converge, and eventually reflect both profile and activity
information. To facilitate training, the derivative is reformulated into matrix
form, which could easily be paralleled. We conduct extensive experiments, which
demonstrate the superiority of the proposed method.
</summary>
    <author>
      <name>Yifan Chen</name>
    </author>
    <author>
      <name>Xiang Zhao</name>
    </author>
    <author>
      <name>Junjiao Gan</name>
    </author>
    <author>
      <name>Junkai Ren</name>
    </author>
    <author>
      <name>Yang Fang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures, ADC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07955v1</id>
    <updated>2016-06-25T20:04:42Z</updated>
    <published>2016-06-25T20:04:42Z</published>
    <title>X575: writing rengas with web services</title>
    <summary>  Our software system simulates the classical collaborative Japanese poetry
form, renga, made of linked haikus. We used NLP methods wrapped up as web
services. Our experiments were only a partial success, since results fail to
satisfy classical constraints. To gather ideas for future work, we examine
related research in semiotics, linguistics, and computing.
</summary>
    <author>
      <name>Daniel Winterstein</name>
    </author>
    <author>
      <name>Joseph Corneli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages; submitted to CC-NLG - Computational Creativity in Natural
  Language Generation</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07908v2</id>
    <updated>2016-07-26T11:42:20Z</updated>
    <published>2016-06-25T12:57:44Z</published>
    <title>Label Tree Embeddings for Acoustic Scene Classification</title>
    <summary>  We present in this paper an efficient approach for acoustic scene
classification by exploring the structure of class labels. Given a set of class
labels, a category taxonomy is automatically learned by collectively optimizing
a clustering of the labels into multiple meta-classes in a tree structure. An
acoustic scene instance is then embedded into a low-dimensional feature
representation which consists of the likelihoods that it belongs to the
meta-classes. We demonstrate state-of-the-art results on two different datasets
for the acoustic scene classification task, including the DCASE 2013 and LITIS
Rouen datasets.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Lars Hertel</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2964284.2967268</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2964284.2967268" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in the Proceedings of ACM Multimedia 2016 (ACMMM 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07908v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07908v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07860v2</id>
    <updated>2016-06-28T18:21:10Z</updated>
    <published>2016-06-25T01:02:30Z</published>
    <title>Non-Monotonic Spatial Reasoning with Answer Set Programming Modulo
  Theories</title>
    <summary>  The systematic modelling of dynamic spatial systems is a key requirement in a
wide range of application areas such as commonsense cognitive robotics,
computer-aided architecture design, and dynamic geographic information systems.
We present ASPMT(QS), a novel approach and fully-implemented prototype for
non-monotonic spatial reasoning -a crucial requirement within dynamic spatial
systems- based on Answer Set Programming Modulo Theories (ASPMT).
  ASPMT(QS) consists of a (qualitative) spatial representation module (QS) and
a method for turning tight ASPMT instances into Satisfiability Modulo Theories
(SMT) instances in order to compute stable models by means of SMT solvers. We
formalise and implement concepts of default spatial reasoning and spatial frame
axioms. Spatial reasoning is performed by encoding spatial relations as systems
of polynomial constraints, and solving via SMT with the theory of real
nonlinear arithmetic. We empirically evaluate ASPMT(QS) in comparison with
other contemporary spatial reasoning systems both within and outside the
context of logic programming. ASPMT(QS) is currently the only existing system
that is capable of reasoning about indirect spatial effects (i.e., addressing
the ramification problem), and integrating geometric and qualitative spatial
information within a non-monotonic spatial reasoning context.
  This paper is under consideration for publication in TPLP.
</summary>
    <author>
      <name>Przemysław Andrzej Wałęga</name>
    </author>
    <author>
      <name>Carl Schultz</name>
    </author>
    <author>
      <name>Mehul Bhatt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 6 figures, Under consideration for publication in TPLP</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07860v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07860v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07841v1</id>
    <updated>2016-06-24T21:54:28Z</updated>
    <published>2016-06-24T21:54:28Z</published>
    <title>Proactive Decision Support using Automated Planning</title>
    <summary>  Proactive decision support (PDS) helps in improving the decision making
experience of human decision makers in human-in-the-loop planning environments.
Here both the quality of the decisions and the ease of making them are
enhanced. In this regard, we propose a PDS framework, named RADAR, based on the
research in Automated Planning in AI, that aids the human decision maker with
her plan to achieve her goals by providing alerts on: whether such a plan can
succeed at all, whether there exist any resource constraints that may foil her
plan, etc. This is achieved by generating and analyzing the landmarks that must
be accomplished by any successful plan on the way to achieving the goals. Note
that, this approach also supports naturalistic decision making which is being
acknowledged as a necessary element in proactive decision support, since it
only aids the human decision maker through suggestions and alerts rather than
enforcing fixed plans or decisions. We demonstrate the utility of the proposed
framework through search-and-rescue examples in a fire-fighting domain.
</summary>
    <author>
      <name>Satya Gautam Vadlamudi</name>
    </author>
    <author>
      <name>Tathagata Chakraborti</name>
    </author>
    <author>
      <name>Yu Zhang</name>
    </author>
    <author>
      <name>Subbarao Kambhampati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07722v1</id>
    <updated>2016-06-24T15:25:55Z</updated>
    <published>2016-06-24T15:25:55Z</published>
    <title>Neural Network Based Next-Song Recommendation</title>
    <summary>  Recently, the next-item/basket recommendation system, which considers the
sequential relation between bought items, has drawn attention of researchers.
The utilization of sequential patterns has boosted performance on several kinds
of recommendation tasks. Inspired by natural language processing (NLP)
techniques, we propose a novel neural network (NN) based next-song recommender,
CNN-rec, in this paper. Then, we compare the proposed system with several NN
based and classic recommendation systems on the next-song recommendation task.
Verification results indicate the proposed system outperforms classic systems
and has comparable performance with the state-of-the-art system.
</summary>
    <author>
      <name>Kai-Chun Hsu</name>
    </author>
    <author>
      <name>Szu-Yu Chou</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <author>
      <name>Tai-Shih Chi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, the 1st Workshop on Deep Learning for Recommender
  Systems (DLRS 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07711v4</id>
    <updated>2016-07-04T13:19:29Z</updated>
    <published>2016-06-24T14:45:27Z</published>
    <title>A Game-Theoretic Approach to Word Sense Disambiguation</title>
    <summary>  This paper presents a new model for word sense disambiguation formulated in
terms of evolutionary game theory, where each word to be disambiguated is
represented as a node on a graph whose edges represent word relations and
senses are represented as classes. The words simultaneously update their class
membership preferences according to the senses that neighboring words are
likely to choose. We use distributional information to weigh the influence that
each word has on the decisions of the others and semantic similarity
information to measure the strength of compatibility among the choices. With
this information we can formulate the word sense disambiguation problem as a
constraint satisfaction problem and solve it using tools derived from game
theory, maintaining the textual coherence. The model is based on two ideas:
similar words should be assigned to similar classes and the meaning of a word
does not depend on all the words in a text but just on some of them. The paper
provides an in-depth motivation of the idea of modeling the word sense
disambiguation problem in terms of game theory, which is illustrated by an
example. The conclusion presents an extensive analysis on the combination of
similarity measures to use in the framework and a comparison with
state-of-the-art systems. The results show that our model outperforms
state-of-the-art algorithms and can be applied to different tasks and in
different scenarios.
</summary>
    <author>
      <name>Rocco Tripodi</name>
    </author>
    <author>
      <name>Marcello Pelillo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in Computational Linguistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07711v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07711v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07695v2</id>
    <updated>2016-08-13T10:39:29Z</updated>
    <published>2016-06-24T14:17:34Z</published>
    <title>Fully DNN-based Multi-label regression for audio tagging</title>
    <summary>  Acoustic event detection for content analysis in most cases relies on lots of
labeled data. However, manually annotating data is a time-consuming task, which
thus makes few annotated resources available so far. Unlike audio event
detection, automatic audio tagging, a multi-label acoustic event classification
task, only relies on weakly labeled data. This is highly desirable to some
practical applications using audio analysis. In this paper we propose to use a
fully deep neural network (DNN) framework to handle the multi-label
classification task in a regression way. Considering that only chunk-level
rather than frame-level labels are available, the whole or almost whole frames
of the chunk were fed into the DNN to perform a multi-label regression for the
expected tags. The fully DNN, which is regarded as an encoding function, can
well map the audio features sequence to a multi-tag vector. A deep pyramid
structure was also designed to extract more robust high-level features related
to the target tags. Further improved methods were adopted, such as the Dropout
and background noise aware training, to enhance its generalization capability
for new audio recordings in mismatched environments. Compared with the
conventional Gaussian Mixture Model (GMM) and support vector machine (SVM)
methods, the proposed fully DNN-based method could well utilize the long-term
temporal information with the whole chunk as the input. The results show that
our approach obtained a 15% relative improvement compared with the official
GMM-based method of DCASE 2016 challenge.
</summary>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Qiang Huang</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Philip J. B. Jackson</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to DCASE2016 Workshop which is as a satellite event to the
  2016 European Signal Processing Conference (EUSIPCO)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07695v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07695v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07572v1</id>
    <updated>2016-06-24T06:00:42Z</updated>
    <published>2016-06-24T06:00:42Z</published>
    <title>Detecting New and Arbitrary Relations among Linked Data Entities using
  Pattern Extraction</title>
    <summary>  Although several RDF knowledge bases are available through the LOD
initiative, often these data sources remain isolated, lacking schemata and
links to other datasets. While there are numerous works that focus on
establishing that two resources are identical and on adding more instances of
an already existing relation, the problem of finding new relations between any
two given datasets has not been investigated in detail. In this paper, given
two entity sets, we present an unsupervised approach to enrich the LOD cloud
with new relations between them by exploiting the web corpus. During the first
phase we gather prospective relations from the corpus through pattern
extraction and paraphrase detection. In the second phase, we perform actual
enrichment by extracting instances of these relations. We have empirically
evaluated our approach on several dataset pairs and found that the system can
indeed be used for enriching the existing datasets with new relations.
</summary>
    <author>
      <name>S Subhashree</name>
    </author>
    <author>
      <name>P Sreenivasa Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07533v1</id>
    <updated>2016-06-24T01:02:30Z</updated>
    <published>2016-06-24T01:02:30Z</published>
    <title>Translucent Players: Explaining Cooperative Behavior in Social Dilemmas</title>
    <summary>  In the last few decades, numerous experiments have shown that humans do not
always behave so as to maximize their material payoff. Cooperative behavior
when non-cooperation is a dominant strategy (with respect to the material
payoffs) is particularly puzzling. Here we propose a novel approach to explain
cooperation, assuming what Halpern and Pass call translucent players.
Typically, players are assumed to be opaque, in the sense that a deviation by
one player in a normal-form game does not affect the strategies used by other
players. But a player may believe that if he switches from one strategy to
another, the fact that he chooses to switch may be visible to the other
players. For example, if he chooses to defect in Prisoner's Dilemma, the other
player may sense his guilt. We show that by assuming translucent players, we
can recover many of the regularities observed in human behavior in well-studied
games such as Prisoner's Dilemma, Traveler's Dilemma, Bertrand Competition, and
the Public Goods game.
</summary>
    <author>
      <name>Valerio Capraro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Y. Halpern</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cornell University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295. The full version of the
  paper is also on arxiv at arXiv:1410.3363</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 114-126</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07529v1</id>
    <updated>2016-06-24T00:39:09Z</updated>
    <published>2016-06-24T00:39:09Z</published>
    <title>The optimality of coarse categories in decision-making and information
  storage</title>
    <summary>  An agent who lacks preferences and instead makes decisions using criteria
that are costly to create should select efficient sets of criteria, where the
cost of making a given number of choice distinctions is minimized. Under mild
conditions, efficiency requires that binary criteria with only two categories
per criterion are chosen. When applied to the problem of determining the
optimal number of digits in an information storage device, this result implies
that binary digits (bits) are the efficient solution, even when the marginal
cost of using additional digits declines rapidly to 0. This short paper pays
particular attention to the symmetry conditions entailed when sets of criteria
are efficient.
</summary>
    <author>
      <name>Michael Mandler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Royal Holloway College, University of London</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.16" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 227-230</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07528v1</id>
    <updated>2016-06-24T00:33:19Z</updated>
    <published>2016-06-24T00:33:19Z</published>
    <title>A Dynamic Epistemic Framework for Conformant Planning</title>
    <summary>  In this paper, we introduce a lightweight dynamic epistemic logical framework
for automated planning under initial uncertainty. We reduce plan verification
and conformant planning to model checking problems of our logic. We show that
the model checking problem of the iteration-free fragment is PSPACE-complete.
By using two non-standard (but equivalent) semantics, we give novel model
checking algorithms to the full language and the iteration-free language.
</summary>
    <author>
      <name>Quan Yu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sun Yat-sen University, Qiannan Normal College for Nationalities</arxiv:affiliation>
    </author>
    <author>
      <name>Yanjun Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University, University of Groningen</arxiv:affiliation>
    </author>
    <author>
      <name>Yanjing Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Peking University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.21</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.21" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 298-318</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07526v1</id>
    <updated>2016-06-24T00:32:51Z</updated>
    <published>2016-06-24T00:32:51Z</published>
    <title>Parameterized Complexity Results for a Model of Theory of Mind Based on
  Dynamic Epistemic Logic</title>
    <summary>  In this paper we introduce a computational-level model of theory of mind
(ToM) based on dynamic epistemic logic (DEL), and we analyze its computational
complexity. The model is a special case of DEL model checking. We provide a
parameterized complexity analysis, considering several aspects of DEL (e.g.,
number of agents, size of preconditions, etc.) as parameters. We show that
model checking for DEL is PSPACE-hard, also when restricted to single-pointed
models and S5 relations, thereby solving an open problem in the literature. Our
approach is aimed at formalizing current intractability claims in the cognitive
science literature regarding computational models of ToM.
</summary>
    <author>
      <name>Iris van de Pol</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Amsterdam, ILLC</arxiv:affiliation>
    </author>
    <author>
      <name>Iris van Rooij</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Radboud University Nijmegen, Donders Institute for Brain, Cognition and Behaviour</arxiv:affiliation>
    </author>
    <author>
      <name>Jakub Szymanik</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Amsterdam, ILLC</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.18</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.18" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 246-263</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.3; F.4.1; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07525v1</id>
    <updated>2016-06-24T00:32:41Z</updated>
    <published>2016-06-24T00:32:41Z</published>
    <title>Relating Knowledge and Coordinated Action: The Knowledge of
  Preconditions Principle</title>
    <summary>  The Knowledge of Preconditions principle (KoP) is proposed as a widely
applicable connection between knowledge and action in multi-agent systems.
Roughly speaking, it asserts that if some condition is a necessary condition
for performing a given action A, then knowing that this condition holds is also
a necessary condition for performing A. Since the specifications of tasks often
involve necessary conditions for actions, the KoP principle shows that such
specifications induce knowledge preconditions for the actions. Distributed
protocols or multi-agent plans that satisfy the specifications must ensure that
this knowledge be attained, and that it is detected by the agents as a
condition for action. The knowledge of preconditions principle is formalised in
the runs and systems framework, and is proven to hold in a wide class of
settings. Well-known connections between knowledge and coordinated action are
extended and shown to derive directly from the KoP principle: a "common
knowledge of preconditions" principle is established showing that common
knowledge is a necessary condition for performing simultaneous actions, and a
"nested knowledge of preconditions" principle is proven, showing that
coordinating actions to be performed in linear temporal order requires a
corresponding form of nested knowledge.
</summary>
    <author>
      <name>Yoram Moses</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.17" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 231-245</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07524v1</id>
    <updated>2016-06-24T00:32:31Z</updated>
    <published>2016-06-24T00:32:31Z</published>
    <title>Preference at First Sight</title>
    <summary>  We consider decision-making and game scenarios in which an agent is limited
by his/her computational ability to foresee all the available moves towards the
future - that is, we study scenarios with short sight. We focus on how short
sight affects the logical properties of decision making in multi-agent
settings. We start with single-agent sequential decision making (SSDM)
processes, modeling them by a new structure of "preference-sight trees". Using
this model, we first explore the relation between a new natural solution
concept of Sight-Compatible Backward Induction (SCBI) and the histories
produced by classical Backward Induction (BI). In particular, we find necessary
and sufficient conditions for the two analyses to be equivalent. Next, we study
whether larger sight always contributes to better outcomes. Then we develop a
simple logical special-purpose language to formally express some key properties
of our preference-sight models. Lastly, we show how short-sight SSDM scenarios
call for substantial enrichments of existing fixed-point logics that have been
developed for the classical BI solution concept. We also discuss changes in
earlier modal logics expressing "surface reasoning" about best actions in the
presence of short sight. Our analysis may point the way to logical and
computational analysis of more realistic game models.
</summary>
    <author>
      <name>Chanjuan Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.15" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 207-226</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07523v1</id>
    <updated>2016-06-24T00:32:22Z</updated>
    <published>2016-06-24T00:32:22Z</published>
    <title>An Axiomatic Approach to Routing</title>
    <summary>  Information delivery in a network of agents is a key issue for large, complex
systems that need to do so in a predictable, efficient manner. The delivery of
information in such multi-agent systems is typically implemented through
routing protocols that determine how information flows through the network.
Different routing protocols exist each with its own benefits, but it is
generally unclear which properties can be successfully combined within a given
algorithm. We approach this problem from the axiomatic point of view, i.e., we
try to establish what are the properties we would seek to see in such a system,
and examine the different properties which uniquely define common routing
algorithms used today.
  We examine several desirable properties, such as robustness, which ensures
adding nodes and edges does not change the routing in a radical, unpredictable
ways; and properties that depend on the operating environment, such as an
"economic model", where nodes choose their paths based on the cost they are
charged to pass information to the next node. We proceed to fully characterize
minimal spanning tree, shortest path, and weakest link routing algorithms,
showing a tight set of axioms for each.
</summary>
    <author>
      <name>Omer Lev</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hebrew University and Microsoft Research, Israel</arxiv:affiliation>
    </author>
    <author>
      <name>Moshe Tennenholtz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technion</arxiv:affiliation>
    </author>
    <author>
      <name>Aviv Zohar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hebrew University and Microsoft Research, Israel</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.14" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 194-206</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07522v1</id>
    <updated>2016-06-24T00:32:05Z</updated>
    <published>2016-06-24T00:32:05Z</published>
    <title>Ceteris paribus logic in counterfactual reasoning</title>
    <summary>  The semantics for counterfactuals due to David Lewis has been challenged on
the basis of unlikely, or impossible, events. Such events may skew a given
similarity order in favour of those possible worlds which exhibit them. By
updating the relational structure of a model according to a ceteris paribus
clause one forces out, in a natural manner, those possible worlds which do not
satisfy the requirements of the clause. We develop a ceteris paribus logic for
counterfactual reasoning capable of performing such actions, and offer several
alternative (relaxed) interpretations of ceteris paribus. We apply this
framework in a way which allows us to reason counterfactually without having
our similarity order skewed by unlikely events. This continues the
investigation of formal ceteris paribus reasoning, which has previously been
applied to preferences, logics of game forms, and questions in decision-making,
among other areas.
</summary>
    <author>
      <name>Patrick Girard</name>
    </author>
    <author>
      <name>Marcus Anthony Triplett</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.13</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.13" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 176-193</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07520v1</id>
    <updated>2016-06-24T00:31:42Z</updated>
    <published>2016-06-24T00:31:42Z</published>
    <title>Standard State Space Models of Unawareness (Extended Abstract)</title>
    <summary>  The impossibility theorem of Dekel, Lipman and Rustichini has been thought to
demonstrate that standard state-space models cannot be used to represent
unawareness. We first show that Dekel, Lipman and Rustichini do not establish
this claim. We then distinguish three notions of awareness, and argue that
although one of them may not be adequately modeled using standard state spaces,
there is no reason to think that standard state spaces cannot provide models of
the other two notions. In fact, standard space models of these forms of
awareness are attractively simple. They allow us to prove completeness and
decidability results with ease, to carry over standard techniques from decision
theory, and to add propositional quantifiers straightforwardly.
</summary>
    <author>
      <name>Peter Fritz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Oslo</arxiv:affiliation>
    </author>
    <author>
      <name>Harvey Lederman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">New York University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.11" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 141-158</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07516v1</id>
    <updated>2016-06-24T00:30:33Z</updated>
    <published>2016-06-24T00:30:33Z</published>
    <title>Epistemic Protocols for Distributed Gossiping</title>
    <summary>  Gossip protocols aim at arriving, by means of point-to-point or group
communications, at a situation in which all the agents know each other's
secrets. We consider distributed gossip protocols which are expressed by means
of epistemic logic. We provide an operational semantics of such protocols and
set up an appropriate framework to argue about their correctness. Then we
analyze specific protocols for complete graphs and for directed rings.
</summary>
    <author>
      <name>Krzysztof R. Apt</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Centrum Wiskunde Informatica</arxiv:affiliation>
    </author>
    <author>
      <name>Davide Grossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Liverpool</arxiv:affiliation>
    </author>
    <author>
      <name>Wiebe van der Hoek</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Liverpool</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 51-66</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07515v1</id>
    <updated>2016-06-24T00:30:18Z</updated>
    <published>2016-06-24T00:30:18Z</published>
    <title>Resolving Distributed Knowledge</title>
    <summary>  Distributed knowledge is the sum of the knowledge in a group; what someone
who is able to discern between two possible worlds whenever any member of the
group can discern between them, would know. Sometimes distributed knowledge is
referred to as the potential knowledge of a group, or the joint knowledge they
could obtain if they had unlimited means of communication. In epistemic logic,
the formula D_G{\phi} is intended to express the fact that group G has
distributed knowledge of {\phi}, that there is enough information in the group
to infer {\phi}. But this is not the same as reasoning about what happens if
the members of the group share their information. In this paper we introduce an
operator R_G, such that R_G{\phi} means that {\phi} is true after G have shared
all their information with each other - after G's distributed knowledge has
been resolved. The R_G operators are called resolution operators. Semantically,
we say that an expression R_G{\phi} is true iff {\phi} is true in what van
Benthem [11, p. 249] calls (G's) communication core; the model update obtained
by removing links to states for members of G that are not linked by all members
of G. We study logics with different combinations of resolution operators and
operators for common and distributed knowledge. Of particular interest is the
relationship between distributed and common knowledge. The main results are
sound and complete axiomatizations.
</summary>
    <author>
      <name>Thomas Ågotnes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bergen</arxiv:affiliation>
    </author>
    <author>
      <name>Yì N. Wáng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Zhejiang University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 31-50</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07514v1</id>
    <updated>2016-06-24T00:30:07Z</updated>
    <published>2016-06-24T00:30:07Z</published>
    <title>Human-Agent Decision-making: Combining Theory and Practice</title>
    <summary>  Extensive work has been conducted both in game theory and logic to model
strategic interaction. An important question is whether we can use these
theories to design agents for interacting with people? On the one hand, they
provide a formal design specification for agent strategies. On the other hand,
people do not necessarily adhere to playing in accordance with these
strategies, and their behavior is affected by a multitude of social and
psychological factors. In this paper we will consider the question of whether
strategies implied by theories of strategic behavior can be used by automated
agents that interact proficiently with people. We will focus on automated
agents that we built that need to interact with people in two negotiation
settings: bargaining and deliberation. For bargaining we will study game-theory
based equilibrium agents and for argumentation we will discuss logic-based
argumentation theory. We will also consider security games and persuasion games
and will discuss the benefits of using equilibrium based agents.
</summary>
    <author>
      <name>Sarit Kraus</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Bar-Ilan University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv:1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016, pp. 13-27</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07493v3</id>
    <updated>2016-07-06T19:56:36Z</updated>
    <published>2016-06-23T21:54:44Z</published>
    <title>Sort Story: Sorting Jumbled Images and Captions into Stories</title>
    <summary>  Temporal common sense has applications in AI tasks such as QA, multi-document
summarization, and human-AI communication. We propose the task of sequencing --
given a jumbled set of aligned image-caption pairs that belong to a story, the
task is to sort them such that the output sequence forms a coherent story. We
present multiple approaches, via unary (position) and pairwise (order)
predictions, and their ensemble-based combinations, achieving strong results on
this task. As features, we use both text-based and image-based features, which
depict complementary improvements. Using qualitative examples, we demonstrate
that our models have learnt interesting aspects of temporal common sense.
</summary>
    <author>
      <name>Harsh Agrawal</name>
    </author>
    <author>
      <name>Arjun Chandrasekaran</name>
    </author>
    <author>
      <name>Dhruv Batra</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <author>
      <name>Mohit Bansal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07493v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07493v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07487v2</id>
    <updated>2016-07-03T20:04:55Z</updated>
    <published>2016-06-23T21:36:36Z</published>
    <title>The VGLC: The Video Game Level Corpus</title>
    <summary>  Levels are a key component of many different video games, and a large body of
work has been produced on how to procedurally generate game levels. Recently,
Machine Learning techniques have been applied to video game level generation
towards the purpose of automatically generating levels that have the properties
of the training corpus. Towards that end we have made available a corpora of
video game levels in an easy to parse format ideal for different machine
learning and other game AI research purposes.
</summary>
    <author>
      <name>Adam James Summerville</name>
    </author>
    <author>
      <name>Sam Snodgrass</name>
    </author>
    <author>
      <name>Michael Mateas</name>
    </author>
    <author>
      <name>Santiago Ontañón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in proceedings of the 7th Workshop on Procedural Content
  Generation</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07461v1</id>
    <updated>2016-06-23T20:20:39Z</updated>
    <published>2016-06-23T20:20:39Z</published>
    <title>Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks, and in particular long short-term memory networks
(LSTMs), are a remarkably effective tool for sequence modeling that learn a
dense black-box hidden representation of their sequential input. Researchers
interested in better understanding these models have studied the changes in
hidden state representations over time and noticed some interpretable patterns
but also significant noise. In this work, we present LSTMVis a visual analysis
tool for recurrent neural networks with a focus on understanding these hidden
state dynamics. The tool allows a user to select a hypothesis input range to
focus on local state changes, to match these states changes to similar patterns
in a large data set, and to align these results with domain specific structural
annotations. We further show several use cases of the tool for analyzing
specific hidden state properties on datasets containing nesting, phrase
structure, and chord progressions, and demonstrate how the tool can be used to
isolate patterns for further statistical analysis.
</summary>
    <author>
      <name>Hendrik Strobelt</name>
    </author>
    <author>
      <name>Sebastian Gehrmann</name>
    </author>
    <author>
      <name>Bernd Huber</name>
    </author>
    <author>
      <name>Hanspeter Pfister</name>
    </author>
    <author>
      <name>Alexander M. Rush</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07419v1</id>
    <updated>2016-06-23T19:42:57Z</updated>
    <published>2016-06-23T19:42:57Z</published>
    <title>Learning to Poke by Poking: Experiential Learning of Intuitive Physics</title>
    <summary>  We investigate an experiential learning paradigm for acquiring an internal
model of intuitive physics. Our model is evaluated on a real-world robotic
manipulation task that requires displacing objects to target locations by
poking. The robot gathered over 400 hours of experience by executing more than
50K pokes on different objects. We propose a novel approach based on deep
neural networks for modeling the dynamics of robot's interactions directly from
images, by jointly estimating forward and inverse models of dynamics. The
inverse model objective provides supervision to construct informative visual
features, which the forward model can then predict and in turn regularize the
feature space for the inverse model. The interplay between these two objectives
creates useful, accurate models that can then be used for multi-step decision
making. This formulation has the additional benefit that it is possible to
learn forward models in an abstract feature space and thus alleviate the need
of predicting pixels. Our experiments show that this joint modeling approach
outperforms alternative methods. We also demonstrate that active data
collection using the learned model further improves performance.
</summary>
    <author>
      <name>Pulkit Agrawal</name>
    </author>
    <author>
      <name>Ashvin Nair</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07384v1</id>
    <updated>2016-06-23T17:47:13Z</updated>
    <published>2016-06-23T17:47:13Z</published>
    <title>Robust Learning of Fixed-Structure Bayesian Networks</title>
    <summary>  We investigate the problem of learning Bayesian networks in an agnostic model
where an $\epsilon$-fraction of the samples are adversarially corrupted. Our
agnostic learning model is similar to -- in fact, stronger than -- Huber's
contamination model in robust statistics. In this work, we study the fully
observable Bernoulli case where the structure of the network is given. Even in
this basic setting, previous learning algorithms either run in exponential time
or lose dimension-dependent factors in their error guarantees. We provide the
first computationally efficient agnostic learning algorithm for this problem
with dimension-independent error guarantees. Our algorithm has polynomial
sample complexity, runs in polynomial time, and achieves error that scales
nearly-linearly with the fraction of adversarially corrupted samples.
</summary>
    <author>
      <name>Ilias Diakonikolas</name>
    </author>
    <author>
      <name>Daniel Kane</name>
    </author>
    <author>
      <name>Alistair Stewart</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07295v1</id>
    <updated>2016-06-23T12:50:36Z</updated>
    <published>2016-06-23T12:50:36Z</published>
    <title>Proceedings Fifteenth Conference on Theoretical Aspects of Rationality
  and Knowledge</title>
    <summary>  The 15th Conference on Theoretical Aspects of Rationality and Knowledge
(TARK) took place in Carnegie Mellon University, Pittsburgh, USA from June 4 to
6, 2015.
  The mission of the TARK conferences is to bring together researchers from a
wide variety of fields, including Artificial Intelligence, Cryptography,
Distributed Computing, Economics and Game Theory, Linguistics, Philosophy, and
Psychology, in order to further our understanding of interdisciplinary issues
involving reasoning about rationality and knowledge.
  These proceedings consist of a subset of the papers / abstracts presented at
the TARK conference.
</summary>
    <author>
      <name>R Ramanujam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMSc, Chennai, India</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07282v2</id>
    <updated>2016-07-29T16:27:04Z</updated>
    <published>2016-06-23T12:12:20Z</published>
    <title>A review of undirected and acyclic directed Gaussian Markov model
  selection and estimation</title>
    <summary>  Markov models lie at the interface between statistical independence in a
probability distribution and graph separation properties. We review model
selection and estimation in directed and undirected Markov models with Gaussian
parametrization, emphasizing the main similarities and differences. These two
model types are foundationally similar but not equivalent, as we highlight. We
report existing results with a unified notation and terminology, taking into
account literature from both the artificial intelligence and statistics
research communities, which first developed these models. Finally, we point out
the main active research areas and open problems now existing with regard to
these traditional, albeit rich, Markov models.
</summary>
    <author>
      <name>Irene Córdoba Sánchez</name>
    </author>
    <author>
      <name>Concha Bielza</name>
    </author>
    <author>
      <name>Pedro Larrañaga</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07282v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07282v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07259v1</id>
    <updated>2016-06-23T10:29:48Z</updated>
    <published>2016-06-23T10:29:48Z</published>
    <title>Log-based Evaluation of Label Splits for Process Models</title>
    <summary>  Process mining techniques aim to extract insights in processes from event
logs. One of the challenges in process mining is identifying interesting and
meaningful event labels that contribute to a better understanding of the
process. Our application area is mining data from smart homes for elderly,
where the ultimate goal is to signal deviations from usual behavior and provide
timely recommendations in order to extend the period of independent living.
Extracting individual process models showing user behavior is an important
instrument in achieving this goal. However, the interpretation of sensor data
at an appropriate abstraction level is not straightforward. For example, a
motion sensor in a bedroom can be triggered by tossing and turning in bed or by
getting up. We try to derive the actual activity depending on the context
(time, previous events, etc.). In this paper we introduce the notion of label
refinements, which links more abstract event descriptions with their more
refined counterparts. We present a statistical evaluation method to determine
the usefulness of a label refinement for a given event log from a process
perspective. Based on data from smart homes, we show how our statistical
evaluation method for label refinements can be used in practice. Our method was
able to select two label refinements out of a set of candidate label
refinements that both had a positive effect on model precision.
</summary>
    <author>
      <name>Niek Tax</name>
    </author>
    <author>
      <name>Natalia Sidorova</name>
    </author>
    <author>
      <name>Reinder Haakma</name>
    </author>
    <author>
      <name>Wil M. P. van der Aalst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.procs.2016.08.096</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.procs.2016.08.096" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted at the 20th International Conference on
  Knowledge-Based and Intelligent Information &amp; Engineering Systems, to appear
  in Procedia Computer Science</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07233v1</id>
    <updated>2016-06-23T09:09:49Z</updated>
    <published>2016-06-23T09:09:49Z</published>
    <title>Adaptive Task Assignment in Online Learning Environments</title>
    <summary>  With the increasing popularity of online learning, intelligent tutoring
systems are regaining increased attention. In this paper, we introduce adaptive
algorithms for personalized assignment of learning tasks to student so that to
improve his performance in online learning environments. As main contribution
of this paper, we propose a a novel Skill-Based Task Selector (SBTS) algorithm
which is able to approximate a student's skill level based on his performance
and consequently suggest adequate assignments. The SBTS is inspired by the
class of multi-armed bandit algorithms. However, in contrast to standard
multi-armed bandit approaches, the SBTS aims at acquiring two criteria related
to student learning, namely: which topics should the student work on, and what
level of difficulty should the task be. The SBTS centers on innovative reward
and punishment schemes in a task and skill matrix based on the student
behaviour.
  To verify the algorithm, the complex student behaviour is modelled using a
neighbour node selection approach based on empirical estimations of a students
learning curve. The algorithm is evaluated with a practical scenario from a
basic java programming course. The SBTS is able to quickly and accurately adapt
to the composite student competency --- even with a multitude of student
models.
</summary>
    <author>
      <name>Per-Arne Andersen</name>
    </author>
    <author>
      <name>Christian Kråkevik</name>
    </author>
    <author>
      <name>Morten Goodwin</name>
    </author>
    <author>
      <name>Anis Yazidi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6th International Conference on Web Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07154v1</id>
    <updated>2016-06-23T01:20:59Z</updated>
    <published>2016-06-23T01:20:59Z</published>
    <title>E-commerce in Your Inbox: Product Recommendations at Scale</title>
    <summary>  In recent years online advertising has become increasingly ubiquitous and
effective. Advertisements shown to visitors fund sites and apps that publish
digital content, manage social networks, and operate e-mail services. Given
such large variety of internet resources, determining an appropriate type of
advertising for a given platform has become critical to financial success.
Native advertisements, namely ads that are similar in look and feel to content,
have had great success in news and social feeds. However, to date there has not
been a winning formula for ads in e-mail clients. In this paper we describe a
system that leverages user purchase history determined from e-mail receipts to
deliver highly personalized product ads to Yahoo Mail users. We propose to use
a novel neural language-based algorithm specifically tailored for delivering
effective product recommendations, which was evaluated against baselines that
included showing popular products and products predicted based on
co-occurrence. We conducted rigorous offline testing using a large-scale
product purchase data set, covering purchases of more than 29 million users
from 172 e-commerce websites. Ads in the form of product recommendations were
successfully tested on online traffic, where we observed a steady 9% lift in
click-through rates over other ad formats in mail, as well as comparable lift
in conversion rates. Following successful tests, the system was launched into
production during the holiday season of 2014.
</summary>
    <author>
      <name>Mihajlo Grbovic</name>
    </author>
    <author>
      <name>Vladan Radosavljevic</name>
    </author>
    <author>
      <name>Nemanja Djuric</name>
    </author>
    <author>
      <name>Narayan Bhamidipati</name>
    </author>
    <author>
      <name>Jaikit Savla</name>
    </author>
    <author>
      <name>Varun Bhagwan</name>
    </author>
    <author>
      <name>Doug Sharp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2783258.2788627.</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2783258.2788627." rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures, Proceedings of the 21th ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining (KDD 2015),
  Sydney, Australia</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 21th ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining (KDD 2015), Sydney, Australia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07149v1</id>
    <updated>2016-06-23T01:07:27Z</updated>
    <published>2016-06-23T01:07:27Z</published>
    <title>An Approach to Stable Gradient Descent Adaptation of Higher-Order Neural
  Units</title>
    <summary>  Stability evaluation of a weight-update system of higher-order neural units
(HONUs) with polynomial aggregation of neural inputs (also known as classes of
polynomial neural networks) for adaptation of both feedforward and recurrent
HONUs by a gradient descent method is introduced. An essential core of the
approach is based on spectral radius of a weight-update system, and it allows
stability monitoring and its maintenance at every adaptation step individually.
Assuring stability of the weight-update system (at every single adaptation
step) naturally results in adaptation stability of the whole neural
architecture that adapts to target data. As an aside, the used approach
highlights the fact that the weight optimization of HONU is a linear problem,
so the proposed approach can be generally extended to any neural architecture
that is linear in its adaptable parameters.
</summary>
    <author>
      <name>Ivo Bukovsky</name>
    </author>
    <author>
      <name>Noriyasu Homma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2016.2572310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2016.2572310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2016, 13 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks and Learning Systems,ISSN:
  2162-237X,2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07137v1</id>
    <updated>2016-06-22T23:35:59Z</updated>
    <published>2016-06-22T23:35:59Z</published>
    <title>Automated Extraction of Number of Subjects in Randomised Controlled
  Trials</title>
    <summary>  We present a simple approach for automatically extracting the number of
subjects involved in randomised controlled trials (RCT). Our approach first
applies a set of rule-based techniques to extract candidate study sizes from
the abstracts of the articles. Supervised classification is then performed over
the candidates with support vector machines, using a small set of lexical,
structural, and contextual features. With only a small annotated training set
of 201 RCTs, we obtained an accuracy of 88\%. We believe that this system will
aid complex medical text processing tasks such as summarisation and question
answering.
</summary>
    <author>
      <name>Abeed Sarker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">unpublished</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07095v1</id>
    <updated>2016-06-22T20:39:23Z</updated>
    <published>2016-06-22T20:39:23Z</published>
    <title>Finding Proofs in Tarskian Geometry</title>
    <summary>  We report on a project to use a theorem prover to find proofs of the theorems
in Tarskian geometry. These theorems start with fundamental properties of
betweenness, proceed through the derivations of several famous theorems due to
Gupta and end with the derivation from Tarski's axioms of Hilbert's 1899 axioms
for geometry. They include the four challenge problems left unsolved by Quaife,
who two decades ago found some \Otter proofs in Tarskian geometry (solving
challenges issued in Wos's 1998 book). There are 212 theorems in this
collection. We were able to find \Otter proofs of all these theorems. We
developed a methodology for the automated preparation and checking of the input
files for those theorems, to ensure that no human error has corrupted the
formal development of an entire theory as embodied in two hundred input files
and proofs. We distinguish between proofs that were found completely
mechanically (without reference to the steps of a book proof) and proofs that
were constructed by some technique that involved a human knowing the steps of a
book proof. Proofs of length 40--100, roughly speaking, are difficult exercises
for a human, and proofs of 100-250 steps belong in a Ph.D. thesis or
publication. 29 of the proofs in our collection are longer than 40 steps, and
ten are longer than 90 steps. We were able to derive completely mechanically
all but 26 of the 183 theorems that have "short" proofs (40 or fewer deduction
steps). We found proofs of the rest, as well as the 29 "hard" theorems, using a
method that requires consulting the book proof at the outset. Our "subformula
strategy" enabled us to prove four of the 29 hard theorems completely
mechanically. These are Ph.D. level proofs, of length up to 108.
</summary>
    <author>
      <name>Michael Beeson</name>
    </author>
    <author>
      <name>Larry Wos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 4 figures, 4 tables. Supplementary computer code published
  separately</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07056v1</id>
    <updated>2016-06-22T19:55:24Z</updated>
    <published>2016-06-22T19:55:24Z</published>
    <title>Emulating Human Conversations using Convolutional Neural Network-based
  IR</title>
    <summary>  Conversational agents ("bots") are beginning to be widely used in
conversational interfaces. To design a system that is capable of emulating
human-like interactions, a conversational layer that can serve as a fabric for
chat-like interaction with the agent is needed. In this paper, we introduce a
model that employs Information Retrieval by utilizing convolutional deep
structured semantic neural network-based features in the ranker to present
human-like responses in ongoing conversation with a user. In conversations,
accounting for context is critical to the retrieval model; we show that our
context-sensitive approach using a Convolutional Deep Structured Semantic Model
(cDSSM) with character trigrams significantly outperforms several conventional
baselines in terms of the relevance of responses retrieved.
</summary>
    <author>
      <name>Abhay Prakash</name>
    </author>
    <author>
      <name>Chris Brockett</name>
    </author>
    <author>
      <name>Puneet Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval,
  July 21, 2016, Pisa, Italy</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07035v1</id>
    <updated>2016-06-22T18:26:27Z</updated>
    <published>2016-06-22T18:26:27Z</published>
    <title>Ancestral Causal Inference</title>
    <summary>  Constraint-based causal discovery from limited data is a notoriously
difficult challenge due to the many borderline independence test decisions.
Several approaches to improve the reliability of the predictions by exploiting
redundancy in the independence information have been proposed recently. Though
promising, existing approaches can still be greatly improved in terms of
accuracy and scalability. We present a novel method that reduces the
combinatorial explosion of the search space by using a more coarse-grained
representation of causal information, drastically reducing computation time.
Additionally, we propose a method to score causal predictions based on their
confidence. Crucially, our implementation also allows one to easily combine
observational and interventional data and to incorporate various types of
available background knowledge. We prove soundness and asymptotic consistency
of our method and demonstrate that it can outperform the state-of-the-art on
synthetic data, achieving a speedup of several orders of magnitude. We
illustrate its practical feasibility by applying it on a challenging protein
data set.
</summary>
    <author>
      <name>Sara Magliacane</name>
    </author>
    <author>
      <name>Tom Claassen</name>
    </author>
    <author>
      <name>Joris M. Mooij</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07025v1</id>
    <updated>2016-06-22T17:48:17Z</updated>
    <published>2016-06-22T17:48:17Z</published>
    <title>Efficient Attack Graph Analysis through Approximate Inference</title>
    <summary>  Attack graphs provide compact representations of the attack paths that an
attacker can follow to compromise network resources by analysing network
vulnerabilities and topology. These representations are a powerful tool for
security risk assessment. Bayesian inference on attack graphs enables the
estimation of the risk of compromise to the system's components given their
vulnerabilities and interconnections, and accounts for multi-step attacks
spreading through the system. Whilst static analysis considers the risk posture
at rest, dynamic analysis also accounts for evidence of compromise, e.g. from
SIEM software or forensic investigation. However, in this context, exact
Bayesian inference techniques do not scale well. In this paper we show how
Loopy Belief Propagation - an approximate inference technique - can be applied
to attack graphs, and that it scales linearly in the number of nodes for both
static and dynamic analysis, making such analyses viable for larger networks.
We experiment with different topologies and network clustering on synthetic
Bayesian attack graphs with thousands of nodes to show that the algorithm's
accuracy is acceptable and converge to a stable solution. We compare sequential
and parallel versions of Loopy Belief Propagation with exact inference
techniques for both static and dynamic analysis, showing the advantages of
approximate inference techniques to scale to larger attack graphs.
</summary>
    <author>
      <name>Luis Muñoz-González</name>
    </author>
    <author>
      <name>Daniele Sgandurra</name>
    </author>
    <author>
      <name>Andrea Paudice</name>
    </author>
    <author>
      <name>Emil C. Lupu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06979v1</id>
    <updated>2016-06-22T15:09:04Z</updated>
    <published>2016-06-22T15:09:04Z</published>
    <title>Simultaneous Control and Human Feedback in the Training of a Robotic
  Agent with Actor-Critic Reinforcement Learning</title>
    <summary>  This paper contributes a preliminary report on the advantages and
disadvantages of incorporating simultaneous human control and feedback signals
in the training of a reinforcement learning robotic agent. While robotic
human-machine interfaces have become increasingly complex in both form and
function, control remains challenging for users. This has resulted in an
increasing gap between user control approaches and the number of robotic motors
which can be controlled. One way to address this gap is to shift some autonomy
to the robot. Semi-autonomous actions of the robotic agent can then be shaped
by human feedback, simplifying user control. Most prior work on agent shaping
by humans has incorporated training with feedback, or has included indirect
control signals. By contrast, in this paper we explore how a human can provide
concurrent feedback signals and real-time myoelectric control signals to train
a robot's actor-critic reinforcement learning control system. Using both a
physical and a simulated robotic system, we compare training performance on a
simple movement task when reward is derived from the environment, when reward
is provided by the human, and combinations of these two approaches. Our results
indicate that some benefit can be gained with the inclusion of human generated
feedback.
</summary>
    <author>
      <name>Kory W. Mathewson</name>
    </author>
    <author>
      <name>Patrick M. Pilarski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, Accepted at the Interactive Machine Learning
  Workshop at IJCAI 2016 (IML): Connecting Humans and Machines</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06900v1</id>
    <updated>2016-06-22T11:07:43Z</updated>
    <published>2016-06-22T11:07:43Z</published>
    <title>Inferring Logical Forms From Denotations</title>
    <summary>  A core problem in learning semantic parsers from denotations is picking out
consistent logical forms--those that yield the correct denotation--from a
combinatorially large space. To control the search space, previous work relied
on restricted set of rules, which limits expressivity. In this paper, we
consider a much more expressive class of logical forms, and show how to use
dynamic programming to efficiently represent the complete set of consistent
logical forms. Expressivity also introduces many more spurious logical forms
which are consistent with the correct denotation but do not represent the
meaning of the utterance. To address this, we generate fictitious worlds and
use crowdsourced denotations on these worlds to filter out spurious logical
forms. On the WikiTableQuestions dataset, we increase the coverage of
answerable questions from 53.5% to 76%, and the additional crowdsourced
supervision lets us rule out 92.1% of spurious logical forms.
</summary>
    <author>
      <name>Panupong Pasupat</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06888v1</id>
    <updated>2016-06-22T10:41:04Z</updated>
    <published>2016-06-22T10:41:04Z</published>
    <title>Structure in the Value Function of Two-Player Zero-Sum Games of
  Incomplete Information</title>
    <summary>  Zero-sum stochastic games provide a rich model for competitive decision
making. However, under general forms of state uncertainty as considered in the
Partially Observable Stochastic Game (POSG), such decision making problems are
still not very well understood. This paper makes a contribution to the theory
of zero-sum POSGs by characterizing structure in their value function. In
particular, we introduce a new formulation of the value function for zs-POSGs
as a function of the "plan-time sufficient statistics" (roughly speaking the
information distribution in the POSG), which has the potential to enable
generalization over such information distributions. We further delineate this
generalization capability by proving a structural result on the shape of value
function: it exhibits concavity and convexity with respect to appropriately
chosen marginals of the statistic space. This result is a key pre-cursor for
developing solution methods that may be able to exploit such structure.
Finally, we show how these results allow us to reduce a zs-POSG to a
"centralized" model with shared observations, thereby transferring results for
the latter, narrower class, to games with individual (private) observations.
</summary>
    <author>
      <name>Auke J. Wiggers</name>
    </author>
    <author>
      <name>Frans A. Oliehoek</name>
    </author>
    <author>
      <name>Diederik M. Roijers</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06797v1</id>
    <updated>2016-06-22T01:34:53Z</updated>
    <published>2016-06-22T01:34:53Z</published>
    <title>Étude de Problèmes d'Optimisation Combinatoire à Multiples
  Composantes Interdépendantes</title>
    <summary>  This extended abstract presents an overview on NP-hard optimization problems
with multiple interdependent components. These problems occur in many
real-world applications: industrial applications, engineering, and logistics.
The fact that these problems are composed of many sub-problems that are NP-hard
makes them even more challenging to solve using exact algorithms. This is
mainly due to the high complexity of this class of algorithms and the hardness
of the problems themselves. The main source of difficulty of these problems is
the presence of internal dependencies between sub-problems. This aspect of
interdependence of components is presented, and some outlines on solving
approaches are briefly introduced from a (meta)heuristics and evolutionary
computation perspective.
</summary>
    <author>
      <name>Mohamed El Yafrani</name>
    </author>
    <author>
      <name>Belaïd Ahiod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French. Extended abstract presented at the URAC days meeting in
  Rabat, Morocco. The meeting website is available at
  https://sites.google.com/site/lriturac29/j-urac-2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06565v2</id>
    <updated>2016-07-25T17:23:29Z</updated>
    <published>2016-06-21T13:37:05Z</published>
    <title>Concrete Problems in AI Safety</title>
    <summary>  Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function ("avoiding side effects" and "avoiding reward hacking"), an
objective function that is too expensive to evaluate frequently ("scalable
supervision"), or undesirable behavior during the learning process ("safe
exploration" and "distributional shift"). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI.
</summary>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Jacob Steinhardt</name>
    </author>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Dan Mané</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06565v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06565v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06512v1</id>
    <updated>2016-06-21T11:04:10Z</updated>
    <published>2016-06-21T11:04:10Z</published>
    <title>Graphical Models for Optimal Power Flow</title>
    <summary>  Optimal power flow (OPF) is the central optimization problem in electric
power grids. Although solved routinely in the course of power grid operations,
it is known to be strongly NP-hard in general, and weakly NP-hard over tree
networks. In this paper, we formulate the optimal power flow problem over tree
networks as an inference problem over a tree-structured graphical model where
the nodal variables are low-dimensional vectors. We adapt the standard dynamic
programming algorithm for inference over a tree-structured graphical model to
the OPF problem. Combining this with an interval discretization of the nodal
variables, we develop an approximation algorithm for the OPF problem. Further,
we use techniques from constraint programming (CP) to perform interval
computations and adaptive bound propagation to obtain practically efficient
algorithms. Compared to previous algorithms that solve OPF with optimality
guarantees using convex relaxations, our approach is able to work for arbitrary
distribution networks and handle mixed-integer optimization problems. Further,
it can be implemented in a distributed message-passing fashion that is scalable
and is suitable for "smart grid" applications like control of distributed
energy resources. We evaluate our technique numerically on several benchmark
networks and show that practical OPF problems can be solved effectively using
this approach.
</summary>
    <author>
      <name>Krishnamurthy Dvijotham</name>
    </author>
    <author>
      <name>Pascal Van Hentenryck</name>
    </author>
    <author>
      <name>Michael Chertkov</name>
    </author>
    <author>
      <name>Sidhant Misra</name>
    </author>
    <author>
      <name>Marc Vuffray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 22nd International Conference on
  Principles and Practice of Constraint Programming (CP 2016(</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06461v2</id>
    <updated>2016-07-21T16:08:32Z</updated>
    <published>2016-06-21T07:54:35Z</published>
    <title>Neighborhood Mixture Model for Knowledge Base Completion</title>
    <summary>  Knowledge bases are useful resources for many natural language processing
tasks, however, they are far from complete. In this paper, we define a novel
entity representation as a mixture of its neighborhood in the knowledge base
and apply this technique on TransE-a well-known embedding model for knowledge
base completion. Experimental results show that the neighborhood information
significantly helps to improve the results of the TransE, leading to better
performance than obtained by other state-of-the-art embedding models on three
benchmark datasets for triple classification, entity prediction and relation
prediction tasks.
</summary>
    <author>
      <name>Dat Quoc Nguyen</name>
    </author>
    <author>
      <name>Kairit Sirts</name>
    </author>
    <author>
      <name>Lizhen Qu</name>
    </author>
    <author>
      <name>Mark Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">V1: To appear in Proceedings of CoNLL 2016. V2: Corrected citation to
  (Krompa{\ss} et al., 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06461v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06461v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06434v1</id>
    <updated>2016-06-21T06:20:22Z</updated>
    <published>2016-06-21T06:20:22Z</published>
    <title>The Schema Editor of OpenIoT for Semantic Sensor Networks</title>
    <summary>  Ontologies provide conceptual abstractions over data, in domains such as the
Internet of Things, in a way that sensor data can be harvested and interpreted
by people and applications. The Semantic Sensor Network (SSN) ontology is the
de-facto standard for semantic representation of sensor observations and
metadata, and it is used at the core of the open source platform for the
Internet of Things, OpenIoT. In this paper we present a Schema Editor that
provides an intuitive web interface for defining new types of sensors, and
concrete instances of them, using the SSN ontology as the core model. This
editor is fully integrated with the OpenIoT platform for generating virtual
sensor descriptions and automating their semantic annotation and registration
process.
</summary>
    <author>
      <name>Prem Prakash Jayaraman</name>
    </author>
    <author>
      <name>Jean-Paul Calbimonte</name>
    </author>
    <author>
      <name>Hoan Nguyen Mau Quoc</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First Joint International Workshop on SEMANTIC SENSOR NETWORKS AND
  TERRA COGNITA, The 14th International Semantic Web Conference Workshops,
  October 11-15, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06368v2</id>
    <updated>2016-06-23T07:33:01Z</updated>
    <published>2016-06-20T23:59:25Z</published>
    <title>Unanimous Prediction for 100% Precision with Application to Learning
  Semantic Mappings</title>
    <summary>  Can we train a system that, on any new input, either says "don't know" or
makes a prediction that is guaranteed to be correct? We answer the question in
the affirmative provided our model family is well-specified. Specifically, we
introduce the unanimity principle: only predict when all models consistent with
the training data predict the same output. We operationalize this principle for
semantic parsing, the task of mapping utterances to logical forms. We develop a
simple, efficient method that reasons over the infinite set of all consistent
models by only checking two of the models. We prove that our method obtains
100% precision even with a modest amount of training data from a possibly
adversarial distribution. Empirically, we demonstrate the effectiveness of our
approach on the standard GeoQuery dataset.
</summary>
    <author>
      <name>Fereshte Khani</name>
    </author>
    <author>
      <name>Martin Rinard</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2016, Removed the duplicate author name of the previous version</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06368v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06368v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06357v1</id>
    <updated>2016-06-20T22:52:48Z</updated>
    <published>2016-06-20T22:52:48Z</published>
    <title>Complex Embeddings for Simple Link Prediction</title>
    <summary>  In statistical relational learning, the link prediction problem is key to
automatically understand the structure of large knowledge bases. As in previous
studies, we propose to solve this problem through latent factorization.
However, here we make use of complex valued embeddings. The composition of
complex embeddings can handle a large variety of binary relations, among them
symmetric and antisymmetric relations. Compared to state-of-the-art models such
as Neural Tensor Network and Holographic Embeddings, our approach based on
complex embeddings is arguably simpler, as it only uses the Hermitian dot
product, the complex counterpart of the standard dot product between real
vectors. Our approach is scalable to large datasets as it remains linear in
both space and time, while consistently outperforming alternative approaches on
standard link prediction benchmarks.
</summary>
    <author>
      <name>Théo Trouillon</name>
    </author>
    <author>
      <name>Johannes Welbl</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <author>
      <name>Éric Gaussier</name>
    </author>
    <author>
      <name>Guillaume Bouchard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10+2 pages, accepted at ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06355v1</id>
    <updated>2016-06-20T22:43:29Z</updated>
    <published>2016-06-20T22:43:29Z</published>
    <title>A Hierarchical Reinforcement Learning Method for Persistent
  Time-Sensitive Tasks</title>
    <summary>  Reinforcement learning has been applied to many interesting problems such as
the famous TD-gammon and the inverted helicopter flight. However, little effort
has been put into developing methods to learn policies for complex persistent
tasks and tasks that are time-sensitive. In this paper, we take a step towards
solving this problem by using signal temporal logic (STL) as task
specification, and taking advantage of the temporal abstraction feature that
the options framework provide. We show via simulation that a relatively easy to
implement algorithm that combines STL and options can learn a satisfactory
policy with a small number of training cases
</summary>
    <author>
      <name>Xiao Li</name>
    </author>
    <author>
      <name>Calin Belta</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06269v2</id>
    <updated>2016-09-11T18:28:29Z</updated>
    <published>2016-06-20T19:48:20Z</published>
    <title>The Founded Semantics and Constraint Semantics of Logic Rules</title>
    <summary>  This paper describes a simple new semantics for logic rules, the founded
semantics, and its straightforward extension to another simple new semantics,
the constraint semantics. The new semantics support unrestricted negation, as
well as unrestricted existential and universal quantifications. They are
uniquely expressive and intuitive by allowing assumptions about the predicates
and rules to be specified explicitly, are completely declarative and easy to
understand, and relate cleanly to prior semantics. In addition, founded
semantics can be computed in linear time in the size of the ground program.
</summary>
    <author>
      <name>Yanhong A. Liu</name>
    </author>
    <author>
      <name>Scott D. Stoller</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06269v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06269v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06197v2</id>
    <updated>2016-06-21T11:43:11Z</updated>
    <published>2016-06-20T16:27:12Z</published>
    <title>Polymetric Rhythmic Feel for a Cognitive Drum Computer</title>
    <summary>  This paper addresses a question about music cognition: how do we derive
polymetric structures. A preference rule system is presented which is
implemented into a drum computer. The preference rule system allows inferring
local polymetric structures, like two-over-three and three-over-two. By
analyzing the micro-timing of West African percussion music a timing pattern
consisting of six pulses was discovered. It integrates binary and ternary
rhythmic feels. The presented drum computer integrates the discovered
superimposed polymetric swing (timing and velocity) appropriate to the rhythmic
sequence the user inputs. For binary sequences, the amount of binary swing is
increased and for ternary sequences, the ternary swing is increased.
</summary>
    <author>
      <name>Oliver Weede</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 14th Int Conf on Culture and Computer Science, Berlin,
  Germany, May 26-27, 2016, pp. 281-295</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.06197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06126v1</id>
    <updated>2016-06-20T14:06:22Z</updated>
    <published>2016-06-20T14:06:22Z</published>
    <title>High Confidence Off-Policy Evaluation with Models</title>
    <summary>  In many reinforcement learning applications executing a poor policy may be
costly or even dangerous. Thus, it is desirable to determine confidence
interval lower bounds on the performance of any given policy without executing
said policy. Current methods for high confidence off-policy evaluation require
a substantial amount of data to achieve a tight lower bound, while existing
model-based methods only address the problem in discrete state spaces. We
propose two bootstrapping approaches combined with learned MDP transition
models in order to efficiently estimate lower confidence bounds on policy
performance with limited data in both continuous and discrete state spaces.
Since direct use of a model may introduce bias, we derive a theoretical upper
bound on model bias when we estimate the model transitions with i.i.d. sampled
trajectories. This bound can be used to guide selection between the two
methods. Finally, we empirically validate the data-efficiency of our proposed
methods across three domains and analyze the settings where one method is
preferable to the other.
</summary>
    <author>
      <name>Josiah P. Hanna</name>
    </author>
    <author>
      <name>Peter Stone</name>
    </author>
    <author>
      <name>Scott Niekum</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06083v2</id>
    <updated>2016-07-25T10:38:52Z</updated>
    <published>2016-06-20T12:26:21Z</published>
    <title>Product Classification in E-Commerce using Distributional Semantics</title>
    <summary>  Product classification is the task of automatically predicting a taxonomy
path for a product in a predefined taxonomy hierarchy given a textual product
description or title. For efficient product classification we require a
suitable representation for a document (the textual description of a product)
feature vector and efficient and fast algorithms for prediction. To address the
above challenges, we propose a new distributional semantics representation for
document vector formation. We also develop a new two-level ensemble approach
utilizing (with respect to the taxonomy tree) a path-wise, node-wise and
depth-wise classifiers for error reduction in the final product classification.
Our experiments show the effectiveness of the distributional representation and
the ensemble approach on data sets from a leading e-commerce platform and
achieve better results on various evaluation metrics compared to earlier
approaches.
</summary>
    <author>
      <name>Vivek Gupta</name>
    </author>
    <author>
      <name>Harish Karnick</name>
    </author>
    <author>
      <name>Ashendra Bansal</name>
    </author>
    <author>
      <name>Pradhuman Jhala</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06083v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06083v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06041v1</id>
    <updated>2016-06-20T09:53:29Z</updated>
    <published>2016-06-20T09:53:29Z</published>
    <title>Bandit-Based Random Mutation Hill-Climbing</title>
    <summary>  The Random Mutation Hill-Climbing algorithm is a direct search technique
mostly used in discrete domains. It repeats the process of randomly selecting a
neighbour of a best-so-far solution and accepts the neighbour if it is better
than or equal to it. In this work, we propose to use a novel method to select
the neighbour solution using a set of independent multi- armed bandit-style
selection units which results in a bandit-based Random Mutation Hill-Climbing
algorithm. The new algorithm significantly outperforms Random Mutation
Hill-Climbing in both OneMax (in noise-free and noisy cases) and Royal Road
problems (in the noise-free case). The algorithm shows particular promise for
discrete optimisation problems where each fitness evaluation is expensive.
</summary>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Diego Peŕez-Liebana</name>
    </author>
    <author>
      <name>Simon M. Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06031v1</id>
    <updated>2016-06-20T09:37:17Z</updated>
    <published>2016-06-20T09:37:17Z</published>
    <title>The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
    <summary>  We introduce LAMBADA, a dataset to evaluate the capabilities of computational
models for text understanding by means of a word prediction task. LAMBADA is a
collection of narrative passages sharing the characteristic that human subjects
are able to guess their last word if they are exposed to the whole passage, but
not if they only see the last sentence preceding the target word. To succeed on
LAMBADA, computational models cannot simply rely on local context, but must be
able to keep track of information in the broader discourse. We show that
LAMBADA exemplifies a wide range of linguistic phenomena, and that none of
several state-of-the-art language models reaches accuracy above 1% on this
novel benchmark. We thus propose LAMBADA as a challenging test set, meant to
encourage the development of new models capable of genuine understanding of
broad context in natural language text.
</summary>
    <author>
      <name>Denis Paperno</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIMeC - Center for Mind/Brain Sciences, University of Trento</arxiv:affiliation>
    </author>
    <author>
      <name>Germán Kruszewski</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIMeC - Center for Mind/Brain Sciences, University of Trento</arxiv:affiliation>
    </author>
    <author>
      <name>Angeliki Lazaridou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIMeC - Center for Mind/Brain Sciences, University of Trento</arxiv:affiliation>
    </author>
    <author>
      <name>Quan Ngoc Pham</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIMeC - Center for Mind/Brain Sciences, University of Trento</arxiv:affiliation>
    </author>
    <author>
      <name>Raffaella Bernardi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIMeC - Center for Mind/Brain Sciences, University of Trento</arxiv:affiliation>
    </author>
    <author>
      <name>Sandro Pezzelle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIMeC - Center for Mind/Brain Sciences, University of Trento</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Baroni</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIMeC - Center for Mind/Brain Sciences, University of Trento</arxiv:affiliation>
    </author>
    <author>
      <name>Gemma Boleda</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CIMeC - Center for Mind/Brain Sciences, University of Trento</arxiv:affiliation>
    </author>
    <author>
      <name>Raquel Fernández</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute for Logic, Language &amp; Computation, University of Amsterdam</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, Accepted as a long paper for ACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05767v2</id>
    <updated>2016-07-24T13:19:23Z</updated>
    <published>2016-06-18T15:33:04Z</published>
    <title>On Reward Function for Survival</title>
    <summary>  Obtaining a survival strategy (policy) is one of the fundamental problems of
biological agents. In this paper, we generalize the formulation of previous
research related to the survival of an agent and we formulate the survival
problem as a maximization of the multi-step survival probability in future time
steps. We introduce a method for converting the maximization of multi-step
survival probability into a classical reinforcement learning problem. Using
this conversion, the reward function (negative temporal cost function) is
expressed as the log of the temporal survival probability. And we show that the
objective function of the reinforcement learning in this sense is proportional
to the variational lower bound of the original problem. Finally, We empirically
demonstrate that the agent learns survival behavior by using the reward
function introduced in this paper.
</summary>
    <author>
      <name>Naoto Yoshida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Joint 8th International Conference on Soft Computing and Intelligent
  Systems and 17th International Symposium on Advanced Intelligent Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05767v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05767v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05735v1</id>
    <updated>2016-06-18T10:06:44Z</updated>
    <published>2016-06-18T10:06:44Z</published>
    <title>Student performance prediction using classification data mining
  techniques</title>
    <summary>  Students opting engineering as their disciple is increasing rapidly. But due
to various factors and inappropriate primary education in India dropout rates
are high. Students are unable to excel in core engineering subjects which are
complex and mathematical, hence mostly get drop / keep term (kt) in that
subject. With the help of data mining techniques we can predict the performance
of students in terms of grades and dropout for a subject. This paper compares
various techniques such as na\"ive Bayes, LibSVM, J48, random forest, and JRip
and try to choose one of them as per our needs and their accuracy. Based on the
rules obtained from this technique(s), we derive the key factors influencing
student performance.
</summary>
    <author>
      <name>Muhammed Salman Shamsi</name>
    </author>
    <author>
      <name>Jhansi Lakshmi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 tables, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05725v1</id>
    <updated>2016-06-18T07:49:13Z</updated>
    <published>2016-06-18T07:49:13Z</published>
    <title>An Efficient Large-scale Semi-supervised Multi-label Classifier Capable
  of Handling Missing labels</title>
    <summary>  Multi-label classification has received considerable interest in recent
years. Multi-label classifiers have to address many problems including:
handling large-scale datasets with many instances and a large set of labels,
compensating missing label assignments in the training set, considering
correlations between labels, as well as exploiting unlabeled data to improve
prediction performance. To tackle datasets with a large set of labels,
embedding-based methods have been proposed which seek to represent the label
assignments in a low-dimensional space. Many state-of-the-art embedding-based
methods use a linear dimensionality reduction to represent the label
assignments in a low-dimensional space. However, by doing so, these methods
actually neglect the tail labels - labels that are infrequently assigned to
instances. We propose an embedding-based method that non-linearly embeds the
label vectors using an stochastic approach, thereby predicting the tail labels
more accurately. Moreover, the proposed method have excellent mechanisms for
handling missing labels, dealing with large-scale datasets, as well as
exploiting unlabeled data. With the best of our knowledge, our proposed method
is the first multi-label classifier that simultaneously addresses all of the
mentioned challenges. Experiments on real-world datasets show that our method
outperforms stateof-the-art multi-label classifiers by a large margin, in terms
of prediction performance, as well as training time.
</summary>
    <author>
      <name>Amirhossein Akbarnejad</name>
    </author>
    <author>
      <name>Mahdieh Soleymani Baghshah</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05611v2</id>
    <updated>2016-06-21T20:48:05Z</updated>
    <published>2016-06-17T17:52:31Z</published>
    <title>Data-driven HR - Résumé Analysis Based on Natural Language
  Processing and Machine Learning</title>
    <summary>  Recruiters usually spend less than a minute looking at each r\'esum\'e when
deciding whether it's worth continuing the recruitment process with the
candidate. Recruiters focus on keywords, and it's almost impossible to
guarantee a fair process of candidate selection. The main scope of this paper
is to tackle this issue by introducing a data-driven approach that shows how to
process r\'esum\'es automatically and give recruiters more time to only examine
promising candidates. Furthermore, we show how to leverage Machine Learning and
Natural Language Processing in order to extract all required information from
the r\'esum\'es. Once the information is extracted, a ranking score is
calculated. The score describes how well the candidates fit based on their
education, work experience and skills. Later this paper illustrates a prototype
application that shows how this novel approach can increase the productivity of
recruiters. The application enables them to filter and rank candidates based on
predefined job descriptions. Guided by the ranking, recruiters can get deeper
insights from candidate profiles and validate why and how the application
ranked them. This application shows how to improve the hiring process by giving
an unbiased hiring decision support.
</summary>
    <author>
      <name>Tim Zimmermann</name>
    </author>
    <author>
      <name>Leo Kotschenreuther</name>
    </author>
    <author>
      <name>Karsten Schmidt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Research Prototype, Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05611v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05611v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05597v1</id>
    <updated>2016-06-17T17:32:11Z</updated>
    <published>2016-06-17T17:32:11Z</published>
    <title>Adding Context to Concept Trees</title>
    <summary>  Concept Trees are a type of database that can organise arbitrary textual
information using a very simple rule. Each tree tries to represent a single
cohesive concept and the trees can link with each other for navigation and
semantic purposes. The trees are therefore a type of semantic network and would
benefit from having a consistent level of context for each of the nodes. The
Concept Tree nodes have a mathematical basis allowing for a consistent build
process. These would represent nouns or verbs in a text sentence, for example.
New to the design can then be lists of descriptive elements for each of the
nodes. The descriptors can also be weighted, but do not have to follow the
strict counting rule of the tree nodes. With the new descriptive layers, a much
richer type of knowledge can be achieved and still reasoned over automatically.
The linking structure of the licas network is very relevant to building the
concept trees now and forms the basis for their construction. The concept tree
- symbolic neural network relation is also extended further.
</summary>
    <author>
      <name>Kieran Greer</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05593v1</id>
    <updated>2016-06-17T17:24:36Z</updated>
    <published>2016-06-17T17:24:36Z</published>
    <title>Introspective Agents: Confidence Measures for General Value Functions</title>
    <summary>  Agents of general intelligence deployed in real-world scenarios must adapt to
ever-changing environmental conditions. While such adaptive agents may leverage
engineered knowledge, they will require the capacity to construct and evaluate
knowledge themselves from their own experience in a bottom-up, constructivist
fashion. This position paper builds on the idea of encoding knowledge as
temporally extended predictions through the use of general value functions.
Prior work has focused on learning predictions about externally derived signals
about a task or environment (e.g. battery level, joint position). Here we
advocate that the agent should also predict internally generated signals
regarding its own learning process - for example, an agent's confidence in its
learned predictions. Finally, we suggest how such information would be
beneficial in creating an introspective agent that is able to learn to make
good decisions in a complex, changing world.
</summary>
    <author>
      <name>Craig Sherstan</name>
    </author>
    <author>
      <name>Adam White</name>
    </author>
    <author>
      <name>Marlos C. Machado</name>
    </author>
    <author>
      <name>Patrick M. Pilarski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at the Ninth Conference on Artificial
  General Intelligence (AGI 2016), 4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05506v1</id>
    <updated>2016-06-17T12:51:23Z</updated>
    <published>2016-06-17T12:51:23Z</published>
    <title>Learning Abstract Classes using Deep Learning</title>
    <summary>  Humans are generally good at learning abstract concepts about objects and
scenes (e.g.\ spatial orientation, relative sizes, etc.). Over the last years
convolutional neural networks have achieved almost human performance in
recognizing concrete classes (i.e.\ specific object categories). This paper
tests the performance of a current CNN (GoogLeNet) on the task of
differentiating between abstract classes which are trivially differentiable for
humans. We trained and tested the CNN on the two abstract classes of horizontal
and vertical orientation and determined how well the network is able to
transfer the learned classes to other, previously unseen objects.
</summary>
    <author>
      <name>Sebastian Stabinger</name>
    </author>
    <author>
      <name>Antonio Rodriguez-Sanchez</name>
    </author>
    <author>
      <name>Justus Piater</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4108/eai.3-12-2015.2262468</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4108/eai.3-12-2015.2262468" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the proceedings of the International Conference on
  Bio-inspired Information and Communications Technologies 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05468v1</id>
    <updated>2016-06-17T10:25:24Z</updated>
    <published>2016-06-17T10:25:24Z</published>
    <title>Most central or least central? How much modeling decisions influence a
  node's centrality ranking in multiplex networks</title>
    <summary>  To understand a node's centrality in a multiplex network, its centrality
values in all the layers of the network can be aggregated. This requires a
normalization of the values, to allow their meaningful comparison and
aggregation over networks with different sizes and orders. The concrete choices
of such preprocessing steps like normalization and aggregation are almost never
discussed in network analytic papers. In this paper, we show that even sticking
to the most simple centrality index (the degree) but using different, classic
choices of normalization and aggregation strategies, can turn a node from being
among the most central to being among the least central. We present our results
by using an aggregation operator which scales between different, classic
aggregation strategies based on three multiplex networks. We also introduce a
new visualization and characterization of a node's sensitivity to the choice of
a normalization and aggregation strategy in multiplex networks. The observed
high sensitivity of single nodes to the specific choice of aggregation and
normalization strategies is of strong importance, especially for all kinds of
intelligence-analytic software as it questions the interpretations of the
findings.
</summary>
    <author>
      <name>Sude Tavassoli</name>
    </author>
    <author>
      <name>Katharina Anna Zweig</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05446v1</id>
    <updated>2016-06-17T08:30:28Z</updated>
    <published>2016-06-17T08:30:28Z</published>
    <title>Abducing Compliance of Incomplete Event Logs</title>
    <summary>  The capability to store data about business processes execution in so-called
Event Logs has brought to the diffusion of tools for the analysis of process
executions and for the assessment of the goodness of a process model.
Nonetheless, these tools are often very rigid in dealing with with Event Logs
that include incomplete information about the process execution. Thus, while
the ability of handling incomplete event data is one of the challenges
mentioned in the process mining manifesto, the evaluation of compliance of an
execution trace still requires an end-to-end complete trace to be performed.
  This paper exploits the power of abduction to provide a flexible, yet
computationally effective, framework to deal with different forms of
incompleteness in an Event Log. Moreover it proposes a refinement of the
classical notion of compliance into strong and conditional compliance to take
into account incomplete logs. Finally, performances evaluation in an
experimental setting shows the feasibility of the presented approach.
</summary>
    <author>
      <name>Federico Chesani</name>
    </author>
    <author>
      <name>Riccardo De Masellis</name>
    </author>
    <author>
      <name>Chiara Di Francescomarino</name>
    </author>
    <author>
      <name>Chiara Ghidini</name>
    </author>
    <author>
      <name>Paola Mello</name>
    </author>
    <author>
      <name>Marco Montali</name>
    </author>
    <author>
      <name>Sergio Tessaris</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05427v1</id>
    <updated>2016-06-17T06:52:32Z</updated>
    <published>2016-06-17T06:52:32Z</published>
    <title>Proceedings First International Workshop on Hammers for Type Theories</title>
    <summary>  This volume of EPTCS contains the proceedings of the First Workshop on
Hammers for Type Theories (HaTT 2016), held on 1 July 2016 as part of the
International Joint Conference on Automated Reasoning (IJCAR 2016) in Coimbra,
Portugal. The proceedings contain four regular papers, as well as abstracts of
the two invited talks by Pierre Corbineau (Verimag, France) and Aleksy Schubert
(University of Warsaw, Poland).
</summary>
    <author>
      <name>Jasmin Christian Blanchette</name>
    </author>
    <author>
      <name>Cezary Kaliszyk</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.210</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.210" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 210, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.05427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05336v3</id>
    <updated>2016-08-17T22:21:25Z</updated>
    <published>2016-06-16T19:55:29Z</published>
    <title>On the expressive power of deep neural networks</title>
    <summary>  We study the effects of the depth and width of a neural network on its
expressive power. Precise theoretical and experimental results are derived in
the generic setting of neural networks after random initialization. We find
that three different measures of functional expressivity: number of transitions
(a measure of non-linearity/complexity), network activation patterns (a new
definition with an intrinsic link to hyperplane arrangements in input space)
and number of dichotomies, show an exponential dependence on depth but not
width. These three measures are related to each other, and, are also directly
proportional to a fourth quantity, trajectory length. Most crucially, we show,
both theoretically and experimentally, that trajectory length grows
exponentially with depth, which is why all three measures display an
exponential dependence on depth.
  These results also suggest that parameters earlier in the network have
greater influence over the expressive power of the network. So for any layer,
its influence on expressivity is determined by the remaining depth of the
network after that layer, which is supported by experiments on fully connected
and convolutional networks on MNIST and CIFAR-10.
</summary>
    <author>
      <name>Maithra Raghu</name>
    </author>
    <author>
      <name>Ben Poole</name>
    </author>
    <author>
      <name>Jon Kleinberg</name>
    </author>
    <author>
      <name>Surya Ganguli</name>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05336v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05336v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05313v1</id>
    <updated>2016-06-16T18:48:51Z</updated>
    <published>2016-06-16T18:48:51Z</published>
    <title>Unsupervised Risk Estimation Using Only Conditional Independence
  Structure</title>
    <summary>  We show how to estimate a model's test error from unlabeled data, on
distributions very different from the training distribution, while assuming
only that certain conditional independencies are preserved between train and
test. We do not need to assume that the optimal predictor is the same between
train and test, or that the true distribution lies in any parametric family. We
can also efficiently differentiate the error estimate to perform unsupervised
discriminative learning. Our technical tool is the method of moments, which
allows us to exploit conditional independencies in the absence of a
fully-specified model. Our framework encompasses a large family of losses
including the log and exponential loss, and extends to structured output
settings such as hidden Markov models.
</summary>
    <author>
      <name>Jacob Steinhardt</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05312v1</id>
    <updated>2016-06-16T18:45:32Z</updated>
    <published>2016-06-16T18:45:32Z</published>
    <title>Successor Features for Transfer in Reinforcement Learning</title>
    <summary>  Transfer in reinforcement learning refers to the notion that generalization
should occur not only within a task but also across tasks. Our focus is on
transfer where the reward functions vary across tasks while the environment's
dynamics remain the same. The method we propose rests on two key ideas:
"successor features," a value function representation that decouples the
dynamics of the environment from the rewards, and "generalized policy
improvement," a generalization of dynamic programming's policy improvement step
that considers a set of policies rather than a single one. Put together, the
two ideas lead to an approach that integrates seamlessly within the
reinforcement learning framework and allows transfer to take place between
tasks without any restriction. The proposed method also provides performance
guarantees for the transferred policy even before any learning has taken place.
We derive two theorems that set our approach in firm theoretical ground and
present experiments that show that it successfully promotes transfer in
practice.
</summary>
    <author>
      <name>André Barreto</name>
    </author>
    <author>
      <name>Rémi Munos</name>
    </author>
    <author>
      <name>Tom Schaul</name>
    </author>
    <author>
      <name>David Silver</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05174v1</id>
    <updated>2016-06-16T13:09:16Z</updated>
    <published>2016-06-16T13:09:16Z</published>
    <title>Deep Reinforcement Learning Discovers Internal Models</title>
    <summary>  Deep Reinforcement Learning (DRL) is a trending field of research, showing
great promise in challenging problems such as playing Atari, solving Go and
controlling robots. While DRL agents perform well in practice we are still
lacking the tools to analayze their performance. In this work we present the
Semi-Aggregated MDP (SAMDP) model. A model best suited to describe policies
exhibiting both spatial and temporal hierarchies. We describe its advantages
for analyzing trained policies over other modeling approaches, and show that
under the right state representation, like that of DQN agents, SAMDP can help
to identify skills. We detail the automatic process of creating it from
recorded trajectories, up to presenting it on t-SNE maps. We explain how to
evaluate its fitness and show surprising results indicating high compatibility
with the policy at hand. We conclude by showing how using the SAMDP model, an
extra performance gain can be squeezed from the agent.
</summary>
    <author>
      <name>Nir Baram</name>
    </author>
    <author>
      <name>Tom Zahavy</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05124v1</id>
    <updated>2016-06-16T10:22:04Z</updated>
    <published>2016-06-16T10:22:04Z</published>
    <title>Robust Active Perception via Data-association aware Belief Space
  planning</title>
    <summary>  We develop a belief space planning (BSP) approach that advances the state of
the art by incorporating reasoning about data association (DA) within planning,
while considering additional sources of uncertainty. Existing BSP approaches
typically assume data association is given and perfect, an assumption that can
be harder to justify while operating, in the presence of localization
uncertainty, in ambiguous and perceptually aliased environments. In contrast,
our data association aware belief space planning (DA-BSP) approach explicitly
reasons about DA within belief evolution, and as such can better accommodate
these challenging real world scenarios. In particular, we show that due to
perceptual aliasing, the posterior belief becomes a mixture of probability
distribution functions, and design cost functions that measure the expected
level of ambiguity and posterior uncertainty. Using these and standard costs
(e.g.~control penalty, distance to goal) within the objective function, yields
a general framework that reliably represents action impact, and in particular,
capable of active disambiguation. Our approach is thus applicable to robust
active perception and autonomous navigation in perceptually aliased
environments. We demonstrate key aspects in basic and realistic simulations.
</summary>
    <author>
      <name>Shashank Pathak</name>
    </author>
    <author>
      <name>Antony Thomas</name>
    </author>
    <author>
      <name>Asaf Feniger</name>
    </author>
    <author>
      <name>Vadim Indelman</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9; I.2.10; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04956v1</id>
    <updated>2016-06-15T20:00:32Z</updated>
    <published>2016-06-15T20:00:32Z</published>
    <title>Assessing Human Error Against a Benchmark of Perfection</title>
    <summary>  An increasing number of domains are providing us with detailed trace data on
human decisions in settings where we can evaluate the quality of these
decisions via an algorithm. Motivated by this development, an emerging line of
work has begun to consider whether we can characterize and predict the kinds of
decisions where people are likely to make errors.
  To investigate what a general framework for human error prediction might look
like, we focus on a model system with a rich history in the behavioral
sciences: the decisions made by chess players as they select moves in a game.
We carry out our analysis at a large scale, employing datasets with several
million recorded games, and using chess tablebases to acquire a form of ground
truth for a subset of chess positions that have been completely solved by
computers but remain challenging even for the best players in the world.
  We organize our analysis around three categories of features that we argue
are present in most settings where the analysis of human error is applicable:
the skill of the decision-maker, the time available to make the decision, and
the inherent difficulty of the decision. We identify rich structure in all
three of these categories of features, and find strong evidence that in our
domain, features describing the inherent difficulty of an instance are
significantly more powerful than features based on skill or time.
</summary>
    <author>
      <name>Ashton Anderson</name>
    </author>
    <author>
      <name>Jon Kleinberg</name>
    </author>
    <author>
      <name>Sendhil Mullainathan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2939672.2939803</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2939672.2939803" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD 2016; 10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04753v1</id>
    <updated>2016-06-15T13:18:30Z</updated>
    <published>2016-06-15T13:18:30Z</published>
    <title>Safe Exploration in Finite Markov Decision Processes with Gaussian
  Processes</title>
    <summary>  In classical reinforcement learning, when exploring an environment, agents
accept arbitrary short term loss for long term gain. This is infeasible for
safety critical applications, such as robotics, where even a single unsafe
action may cause system failure. In this paper, we address the problem of
safely exploring finite Markov decision processes (MDP). We define safety in
terms of an, a priori unknown, safety constraint that depends on states and
actions. We aim to explore the MDP under this constraint, assuming that the
unknown function satisfies regularity conditions expressed via a Gaussian
process prior. We develop a novel algorithm for this task and prove that it is
able to completely explore the safely reachable part of the MDP without
violating the safety constraint. To achieve this, it cautiously explores safe
states and actions in order to gain statistical confidence about the safety of
unvisited state-action pairs from noisy observations collected while navigating
the environment. Moreover, the algorithm explicitly considers reachability when
exploring the MDP, ensuring that it does not get stuck in any state with no
safe way out. We demonstrate our method on digital terrain models for the task
of exploring an unknown map with a rover.
</summary>
    <author>
      <name>Matteo Turchetta</name>
    </author>
    <author>
      <name>Felix Berkenkamp</name>
    </author>
    <author>
      <name>Andreas Krause</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, extended version with proofs</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04695v1</id>
    <updated>2016-06-15T09:28:52Z</updated>
    <published>2016-06-15T09:28:52Z</published>
    <title>Strategic Attentive Writer for Learning Macro-Actions</title>
    <summary>  We present a novel deep recurrent neural network architecture that learns to
build implicit plans in an end-to-end manner by purely interacting with an
environment in reinforcement learning setting. The network builds an internal
plan, which is continuously updated upon observation of the next input from the
environment. It can also partition this internal representation into contiguous
sub- sequences by learning for how long the plan can be committed to - i.e.
followed without re-planing. Combining these properties, the proposed model,
dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally
abstracted macro- actions of varying lengths that are solely learnt from data
without any prior information. These macro-actions enable both structured
exploration and economic computation. We experimentally demonstrate that STRAW
delivers strong improvements on several ATARI games by employing temporally
extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same
time a general algorithm that can be applied on any sequence data. To that end,
we also show that when trained on text prediction task, STRAW naturally
predicts frequent n-grams (instead of macro-actions), demonstrating the
generality of the approach.
</summary>
    <author>
      <name> Alexander</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sasha</arxiv:affiliation>
    </author>
    <author>
      <name> Vezhnevets</name>
    </author>
    <author>
      <name>Volodymyr Mnih</name>
    </author>
    <author>
      <name>John Agapiou</name>
    </author>
    <author>
      <name>Simon Osindero</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04686v1</id>
    <updated>2016-06-15T09:05:56Z</updated>
    <published>2016-06-15T09:05:56Z</published>
    <title>Natural Language Generation as Planning under Uncertainty Using
  Reinforcement Learning</title>
    <summary>  We present and evaluate a new model for Natural Language Generation (NLG) in
Spoken Dialogue Systems, based on statistical planning, given noisy feedback
from the current generation context (e.g. a user and a surface realiser). We
study its use in a standard NLG problem: how to present information (in this
case a set of search results) to users, given the complex trade- offs between
utterance length, amount of information conveyed, and cognitive load. We set
these trade-offs by analysing existing MATCH data. We then train a NLG pol- icy
using Reinforcement Learning (RL), which adapts its behaviour to noisy feed-
back from the current generation context. This policy is compared to several
base- lines derived from previous work in this area. The learned policy
significantly out- performs all the prior approaches.
</summary>
    <author>
      <name>Verena Rieser</name>
    </author>
    <author>
      <name>Oliver Lemon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published EACL 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04615v1</id>
    <updated>2016-06-15T01:57:40Z</updated>
    <published>2016-06-15T01:57:40Z</published>
    <title>Deep Reinforcement Learning With Macro-Actions</title>
    <summary>  Deep reinforcement learning has been shown to be a powerful framework for
learning policies from complex high-dimensional sensory inputs to actions in
complex tasks, such as the Atari domain. In this paper, we explore output
representation modeling in the form of temporal abstraction to improve
convergence and reliability of deep reinforcement learning approaches. We
concentrate on macro-actions, and evaluate these on different Atari 2600 games,
where we show that they yield significant improvements in learning speed.
Additionally, we show that they can even achieve better scores than DQN. We
offer analysis and explanation for both convergence and final results,
revealing a problem deep RL approaches have with sparse reward signals.
</summary>
    <author>
      <name>Ishan P. Durugkar</name>
    </author>
    <author>
      <name>Clemens Rosenbaum</name>
    </author>
    <author>
      <name>Stefan Dernbach</name>
    </author>
    <author>
      <name>Sridhar Mahadevan</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04589v1</id>
    <updated>2016-06-14T23:05:39Z</updated>
    <published>2016-06-14T23:05:39Z</published>
    <title>Impossibility in Belief Merging</title>
    <summary>  With the aim of studying social properties of belief merging and having a
better understanding of impossibility, we extend in three ways the framework of
logic-based merging introduced by Konieczny and Pino P\'erez. First, at the
level of representation of the information, we pass from belief bases to
complex epistemic states. Second, the profiles are represented as functions of
finite societies to the set of epistemic states (a sort of vectors) and not as
multisets of epistemic states. Third, we extend the set of rational postulates
in order to consider the epistemic versions of the classical postulates of
Social Choice Theory: Standard Domain, Pareto Property, Independence of
Irrelevant Alternatives and Absence of Dictator. These epistemic versions of
social postulates are given, essentially, in terms of the finite propositional
logic. We state some representation theorems for these operators. These
extensions and representation theorems allow us to establish an epistemic and
very general version of Arrow's Impossibility Theorem. One of the interesting
features of our result, is that it holds for different representations of
epistemic states; for instance conditionals, Ordinal Conditional functions and,
of course, total preorders.
</summary>
    <author>
      <name>Amílcar Mata Díaz</name>
    </author>
    <author>
      <name>Ramón Pino Pérez</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T27, 68T30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04512v1</id>
    <updated>2016-06-14T19:13:30Z</updated>
    <published>2016-06-14T19:13:30Z</published>
    <title>Why is Compiling Lifted Inference into a Low-Level Language so
  Effective?</title>
    <summary>  First-order knowledge compilation techniques have proven efficient for lifted
inference. They compile a relational probability model into a target circuit on
which many inference queries can be answered efficiently. Early methods used
data structures as their target circuit. In our KR-2016 paper, we showed that
compiling to a low-level program instead of a data structure offers orders of
magnitude speedup, resulting in the state-of-the-art lifted inference
technique. In this paper, we conduct experiments to address two questions
regarding our KR-2016 results: 1- does the speedup come from more efficient
compilation or more efficient reasoning with the target circuit?, and 2- why
are low-level programs more efficient target circuits than data structures?
</summary>
    <author>
      <name>Seyed Mehran Kazemi</name>
    </author>
    <author>
      <name>David Poole</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, accepted at IJCAI-16 Statistical Relational AI
  (StaRAI) workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04486v1</id>
    <updated>2016-06-14T18:18:58Z</updated>
    <published>2016-06-14T18:18:58Z</published>
    <title>Lifted Convex Quadratic Programming</title>
    <summary>  Symmetry is the essential element of lifted inference that has recently
demon- strated the possibility to perform very efficient inference in
highly-connected, but symmetric probabilistic models models. This raises the
question, whether this holds for optimisation problems in general. Here we show
that for a large class of optimisation methods this is actually the case. More
precisely, we introduce the concept of fractional symmetries of convex
quadratic programs (QPs), which lie at the heart of many machine learning
approaches, and exploit it to lift, i.e., to compress QPs. These lifted QPs can
then be tackled with the usual optimization toolbox (off-the-shelf solvers,
cutting plane algorithms, stochastic gradients etc.). If the original QP
exhibits symmetry, then the lifted one will generally be more compact, and
hence their optimization is likely to be more efficient.
</summary>
    <author>
      <name>Martin Mladenov</name>
    </author>
    <author>
      <name>Leonard Kleinhans</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04442v1</id>
    <updated>2016-06-14T16:27:41Z</updated>
    <published>2016-06-14T16:27:41Z</published>
    <title>DeepMath - Deep Sequence Models for Premise Selection</title>
    <summary>  We study the effectiveness of neural sequence models for premise selection in
automated theorem proving, one of the main bottlenecks in the formalization of
mathematics. We propose a two stage approach for this task that yields good
results for the premise selection task on the Mizar corpus while avoiding the
hand-engineered features of existing state-of-the-art models. To our knowledge,
this is the first time deep learning has been applied to theorem proving.
</summary>
    <author>
      <name>Alex A. Alemi</name>
    </author>
    <author>
      <name>Francois Chollet</name>
    </author>
    <author>
      <name>Geoffrey Irving</name>
    </author>
    <author>
      <name>Christian Szegedy</name>
    </author>
    <author>
      <name>Josef Urban</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04422v2</id>
    <updated>2016-07-07T12:28:57Z</updated>
    <published>2016-06-14T15:25:28Z</published>
    <title>Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and
  Knowledge</title>
    <summary>  We propose Logic Tensor Networks: a uniform framework for integrating
automatic learning and reasoning. A logic formalism called Real Logic is
defined on a first-order language whereby formulas have truth-value in the
interval [0,1] and semantics defined concretely on the domain of real numbers.
Logical constants are interpreted as feature vectors of real numbers. Real
Logic promotes a well-founded integration of deductive reasoning on a
knowledge-base and efficient data-driven relational machine learning. We show
how Real Logic can be implemented in deep Tensor Neural Networks with the use
of Google's tensorflow primitives. The paper concludes with experiments
applying Logic Tensor Networks on a simple but representative example of
knowledge completion.
</summary>
    <author>
      <name>Luciano Serafini</name>
    </author>
    <author>
      <name>Artur d'Avila Garcez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figs, 1 table, 27 references</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04422v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04422v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04414v1</id>
    <updated>2016-06-14T15:12:01Z</updated>
    <published>2016-06-14T15:12:01Z</published>
    <title>The Parallel Knowledge Gradient Method for Batch Bayesian Optimization</title>
    <summary>  In many applications of black-box optimization, one can evaluate multiple
points simultaneously, e.g. when evaluating the performances of several
different neural network architectures in a parallel computing environment. In
this paper, we develop a novel batch Bayesian optimization algorithm --- the
parallel knowledge gradient method. By construction, this method provides the
one-step Bayes optimal batch of points to sample. We provide an efficient
strategy for computing this Bayes-optimal batch of points, and we demonstrate
that the parallel knowledge gradient method finds global optima significantly
faster than previous batch Bayesian optimization algorithms on both synthetic
test functions and when tuning hyperparameters of practical machine learning
algorithms, especially when function evaluations are noisy.
</summary>
    <author>
      <name>Jian Wu</name>
    </author>
    <author>
      <name>Peter I. Frazier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04397v1</id>
    <updated>2016-06-14T14:41:24Z</updated>
    <published>2016-06-14T14:41:24Z</published>
    <title>Relating Strong Spatial Cognition to Symbolic Problem Solving --- An
  Example</title>
    <summary>  In this note, we discuss and analyse a shortest path finding approach using
strong spatial cognition. It is compared with a symbolic graph-based algorithm
and it is shown that both approaches are similar with respect to structure and
complexity. Nevertheless, the strong spatial cognition solution is easy to
understand and even pops up immediately when one has to solve the problem.
</summary>
    <author>
      <name>Ulrich Furbach</name>
    </author>
    <author>
      <name>Florian Furbach</name>
    </author>
    <author>
      <name>Christian Freksa</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04345v1</id>
    <updated>2016-06-14T13:29:13Z</updated>
    <published>2016-06-14T13:29:13Z</published>
    <title>Digits that are not: Generating new types through deep neural nets</title>
    <summary>  For an artificial creative agent, an essential driver of the search for
novelty is a value function which is often provided by the system designer or
users. We argue that an important barrier for progress in creativity research
is the inability of these systems to develop their own notion of value for
novelty. We propose a notion of knowledge-driven creativity that circumvent the
need for an externally imposed value function, allowing the system to explore
based on what it has learned from a set of referential objects. The concept is
illustrated by a specific knowledge model provided by a deep generative
autoencoder. Using the described system, we train a knowledge model on a set of
digit images and we use the same model to build coherent sets of new digits
that do not belong to known digit types.
</summary>
    <author>
      <name>Akın Kazakçıand Mehdi Cherti</name>
    </author>
    <author>
      <name>Balázs Kégl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint ICCC'16, International Conference on Computational
  Creativity</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04327v1</id>
    <updated>2016-06-14T12:38:26Z</updated>
    <published>2016-06-14T12:38:26Z</published>
    <title>Entropy/IP: Uncovering Structure in IPv6 Addresses</title>
    <summary>  In this paper, we introduce Entropy/IP: a system that discovers Internet
address structure based on analyses of a subset of IPv6 addresses known to be
active, i.e., training data, gleaned by readily available passive and active
means. The system is completely automated and employs a combination of
information-theoretic and machine learning techniques to probabilistically
model IPv6 addresses. We present results showing that our system is effective
in exposing structural characteristics of portions of the IPv6 Internet address
space populated by active client, service, and router addresses.
  In addition to visualizing the address structure for exploration, the system
uses its models to generate candidate target addresses for scanning. For each
of 15 evaluated datasets, we train on 1K addresses and generate 1M candidates
for scanning. We achieve some success in 14 datasets, finding up to 40% of the
generated addresses to be active. In 11 of these datasets, we find active
network identifiers (e.g., /64 prefixes or `subnets') not seen in training.
Thus, we provide the first evidence that it is practical to discover subnets
and hosts by scanning probabilistically selected areas of the IPv6 address
space not known to contain active hosts a priori.
</summary>
    <author>
      <name>Pawel Foremski</name>
    </author>
    <author>
      <name>David Plonka</name>
    </author>
    <author>
      <name>Arthur Berger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is a pre-print version, submitted for peer-review. A Live
  Demo site is available at www.entropy-ip.com</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04250v2</id>
    <updated>2016-08-26T16:37:37Z</updated>
    <published>2016-06-14T08:38:18Z</published>
    <title>Experimental and causal view on information integration in autonomous
  agents</title>
    <summary>  The amount of digitally available but heterogeneous information about the
world is remarkable, and new technologies such as self-driving cars, smart
homes, or the internet of things may further increase it. In this paper we
examine certain aspects of the problem of how such heterogeneous information
can be harnessed by autonomous agents. After discussing potentials and
limitations of some existing approaches, we investigate how \emph{experiments}
can help to obtain a better understanding of the problem. Specifically, we
present a simple agent that integrates video data from a different agent, and
implement and evaluate a version of it on the novel experimentation platform
\emph{Malmo}. The focus of a second investigation is on how information about
the hardware of different agents, the agents' sensory data, and \emph{causal}
information can be utilized for knowledge transfer between agents and
subsequently more data-efficient decision making. Finally, we discuss potential
future steps w.r.t.\ theory and experimentation, and formulate open questions.
</summary>
    <author>
      <name>Philipp Geiger</name>
    </author>
    <author>
      <name>Katja Hofmann</name>
    </author>
    <author>
      <name>Bernhard Schölkopf</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04250v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04250v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04216v1</id>
    <updated>2016-06-14T07:01:00Z</updated>
    <published>2016-06-14T07:01:00Z</published>
    <title>Spreadsheet Probabilistic Programming</title>
    <summary>  Spreadsheet workbook contents are simple programs. Because of this,
probabilistic programming techniques can be used to perform Bayesian inversion
of spreadsheet computations. What is more, existing execution engines in
spreadsheet applications such as Microsoft Excel can be made to do this using
only built-in functionality. We demonstrate this by developing a native Excel
implementation of both a particle Markov Chain Monte Carlo variant and
black-box variational inference for spreadsheet probabilistic programming. The
resulting engine performs probabilistically coherent inference over spreadsheet
computations, notably including spreadsheets that include user-defined
black-box functions. Spreadsheet engines that choose to integrate the
functionality we describe in this paper will give their users the ability to
both easily develop probabilistic models and maintain them over time by
including actuals via a simple user-interface mechanism. For spreadsheet
end-users this would mean having access to efficient and probabilistically
coherent probabilistic modeling and inference for use in all kinds of decision
making under uncertainty.
</summary>
    <author>
      <name>Mike Wu</name>
    </author>
    <author>
      <name>Yura Perov</name>
    </author>
    <author>
      <name>Frank Wood</name>
    </author>
    <author>
      <name>Hongseok Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04190v1</id>
    <updated>2016-06-14T01:44:16Z</updated>
    <published>2016-06-14T01:44:16Z</published>
    <title>Micro-interventions in urban transport from pattern discovery on the
  flow of passengers and on the bus network</title>
    <summary>  In this paper, we describe a case study in a big metropolis, in which from
data collected by digital sensors, we tried to understand mobility patterns of
persons using buses and how this can generate knowledge to suggest
interventions that are applied incrementally into the transportation network in
use. We have first estimated an Origin-Destination matrix of buses users from
datasets about the ticket validation and GPS positioning of buses. Then we
represent the supply of buses with their routes through bus stops as a complex
network, which allowed us to understand the bottlenecks of the current scenario
and, in particular, applying community discovery techniques, to identify
clusters that the service supply infrastructure has. Finally, from the
superimposing of the flow of people represented in the OriginDestination matrix
in the supply network, we exemplify how micro-interventions can be prospected
by means of an example of the introduction of express routes.
</summary>
    <author>
      <name>Carlos Caminha</name>
    </author>
    <author>
      <name>Vasco Furtado</name>
    </author>
    <author>
      <name>Vládia Pinheiro e Caio Ponte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1606.03737</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04087v6</id>
    <updated>2016-08-28T18:00:52Z</updated>
    <published>2016-06-13T19:57:48Z</published>
    <title>Networked Intelligence: Towards Autonomous Cyber Physical Systems</title>
    <summary>  Developing intelligent systems requires combining results from both industry
and academia. In this report you find an overview of relevant research fields
and industrially applicable technologies for building very large scale cyber
physical systems. A concept architecture is used to illustrate how existing
pieces may fit together, and the maturity of the subsystems is estimated.
  The goal is to structure the developments and the challenge of machine
intelligence for Consumer and Industrial Internet technologists, cyber physical
systems researchers and people interested in the convergence of data &amp; Internet
of Things. It can be used for planning developments of intelligent systems.
</summary>
    <author>
      <name>Andre Karpistsenko</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04087v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04087v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04055v1</id>
    <updated>2016-06-13T18:18:07Z</updated>
    <published>2016-06-13T18:18:07Z</published>
    <title>Bacteria Foraging Algorithm with Genetic Operators for the Solution of
  QAP and mQAP</title>
    <summary>  The Bacterial Foraging Optimization (BFO) is one of the metaheuristics
algorithms that most widely used to solve optimization problems. The BFO is
imitated from the behavior of the foraging bacteria group such as Ecoli. The
main aim of algorithm is to eliminate those bacteria that have weak foraging
methods and maintaining those bacteria that have strong foraging methods. In
this extent, each bacterium communicates with other bacteria by sending signals
such that bacterium change the position in the next step if prior factors have
been satisfied. In fact, the process of algorithm allows bacteria to follow up
nutrients toward the optimal. In this paper, the BFO is used for the solutions
of Quadratic Assignment Problem (QAP), and multi- objective QAP (mQAP) by using
updating mechanisms including mutation, crossover, and a local search.
</summary>
    <author>
      <name>Saeid Parvandeh</name>
    </author>
    <author>
      <name>Ahmet Unveren</name>
    </author>
    <author>
      <name>Bill C. White</name>
    </author>
    <author>
      <name>Mohammadreza Boroumand</name>
    </author>
    <author>
      <name>Parya Soltani</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04000v1</id>
    <updated>2016-06-13T15:45:00Z</updated>
    <published>2016-06-13T15:45:00Z</published>
    <title>Using a Distributional Semantic Vector Space with a Knowledge Base for
  Reasoning in Uncertain Conditions</title>
    <summary>  The inherent inflexibility and incompleteness of commonsense knowledge bases
(KB) has limited their usefulness. We describe a system called Displacer for
performing KB queries extended with the analogical capabilities of the word2vec
distributional semantic vector space (DSVS). This allows the system to answer
queries with information which was not contained in the original KB in any
form. By performing analogous queries on semantically related terms and mapping
their answers back into the context of the original query using displacement
vectors, we are able to give approximate answers to many questions which, if
posed to the KB alone, would return no results.
  We also show how the hand-curated knowledge in a KB can be used to increase
the accuracy of a DSVS in solving analogy problems. In these ways, a KB and a
DSVS can make up for each other's weaknesses.
</summary>
    <author>
      <name>Douglas Summers-Stay</name>
    </author>
    <author>
      <name>Clare Voss</name>
    </author>
    <author>
      <name>Taylor Cassidy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Biologically Inspired Cognitive Architectures (2016), pp. 34-44</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.04000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03976v2</id>
    <updated>2016-06-24T13:13:05Z</updated>
    <published>2016-06-13T14:40:57Z</published>
    <title>Bounding and Minimizing Counterfactual Error</title>
    <summary>  There is intense interest in applying machine learning methods to problems of
causal inference which arise in applications such as healthcare, economic
policy, and education. In this paper we use the counterfactual inference
approach to causal inference, and propose new theoretical results and new
algorithms for performing counterfactual inference. Building on an idea
recently proposed by Johansson et al., our results and methods rely on learning
so-called "balanced" representations: representations that are similar between
the factual and counterfactual distributions. We give a novel, simple and
intuitive bound, showing that the expected counterfactual error of a
representation is bounded by a sum of the factual error of that representation
and the distance between the factual and counterfactual distributions induced
by the representation. We use Integral Probability Metrics to measure distances
between distributions, and focus on two special cases: the Wasserstein distance
and the Maximum Mean Discrepancy (MMD) distance. Our bound leads directly to
new algorithms, which are simpler and easier to employ compared to those
suggested in Johansson et al.. Experiments on real and simulated data show the
new algorithms match or outperform state-of-the-art methods.
</summary>
    <author>
      <name>Uri Shalit</name>
    </author>
    <author>
      <name>Fredrik Johansson</name>
    </author>
    <author>
      <name>David Sontag</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">added missing definition of PEHE_nn</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03976v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03976v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03968v1</id>
    <updated>2016-06-13T14:22:10Z</updated>
    <published>2016-06-13T14:22:10Z</published>
    <title>Visual-Inertial Scene Representations</title>
    <summary>  We describe a representation of a scene that captures geometric and semantic
attributes of objects within, along with their uncertainty. Objects are assumed
persistent in the scene, and their likelihood computed from intermittent visual
data using a convolutional architecture, integrated within a Bayesian filtering
framework with inertials and a context model. Our method yields a posterior
estimate of geometry (attributed point cloud and associated uncertainty),
semantics (identities and co-occurrence), and a point-estimate of topology for
a variable number of objects within the scene, implemented causally and in
real-time on commodity hardware.
</summary>
    <author>
      <name>Stefano Soatto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preliminary version, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03935v1</id>
    <updated>2016-06-13T13:15:41Z</updated>
    <published>2016-06-13T13:15:41Z</published>
    <title>A framework for redescription set construction</title>
    <summary>  Redescription mining is a field of knowledge discovery that aims at finding
different descriptions of similar subsets of instances in the data. These
instances are characterized with descriptive attributes from one or more
disjoint sets of attributes called views. By exploring different
characterizations it is possible to find non trivial and interesting
connections between different subsets of attributes. In this work, we explore
the process of creating possibly large and heterogeneous redescription set in
which redescriptions are iteratively improved by a conjunctive refinement
procedure aimed at increasing redescription accuracy. This set is used by our
redescription set construction procedure to create multiple redescription sets
of user defined size. Set construction is based on redescription selection by
using multi-objective optimization incorporating user defined importance levels
towards one or more redescription quality criteria. These properties
distinguish our approach from current state of the art approaches that create
one, mostly smaller set that contains redescriptions satisfying a pre-defined
set of constraints. We introduce a new redescription quality criterion that
assesses the variability of redescription accuracy when missing values are
present in the data. Finally, we compare the performance of our framework with
three state of the art redescription mining algorithms.
</summary>
    <author>
      <name>Matej Mihelčić</name>
    </author>
    <author>
      <name>Sašo Džeroski</name>
    </author>
    <author>
      <name>Nada Lavrač</name>
    </author>
    <author>
      <name>Tomislav Šmuc</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03894v1</id>
    <updated>2016-06-13T11:03:26Z</updated>
    <published>2016-06-13T11:03:26Z</published>
    <title>A Probabilistic-Based Model for Binary CSP</title>
    <summary>  This work introduces a probabilistic-based model for binary CSP that provides
a fine grained analysis of its internal structure. Assuming that a domain
modification could occur in the CSP, it shows how to express, in a predictive
way, the probability that a domain value becomes inconsistent, then it express
the expectation of the number of arc-inconsistent values in each domain of the
constraint network. Thus, it express the expectation of the number of
arc-inconsistent values for the whole constraint network. Next, it provides
bounds for each of these three probabilistic indicators. Finally, a polytime
algorithm, which propagates the probabilistic information, is presented.
</summary>
    <author>
      <name>Amine Balafrej</name>
    </author>
    <author>
      <name>Xavier Lorca</name>
    </author>
    <author>
      <name>Charlotte Truchet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03864v2</id>
    <updated>2016-06-14T07:59:18Z</updated>
    <published>2016-06-13T09:08:04Z</published>
    <title>Neural Associative Memory for Dual-Sequence Modeling</title>
    <summary>  Many important NLP problems can be posed as dual-sequence or
sequence-to-sequence modeling tasks. Recent advances in building end-to-end
neural architectures have been highly successful in solving such tasks. In this
work we propose a new architecture for dual-sequence modeling that is based on
associative memory. We derive AM-RNNs, a recurrent associative memory (AM)
which augments generic recurrent neural networks (RNN). This architecture is
extended to the Dual AM-RNN which operates on two AMs at once. Our models
achieve very competitive results on textual entailment. A qualitative analysis
demonstrates that long range dependencies between source and target-sequence
can be bridged effectively using Dual AM-RNNs. However, an initial experiment
on auto-encoding reveals that these benefits are not exploited by the system
when learning to solve sequence-to-sequence tasks which indicates that
additional supervision or regularization is needed.
</summary>
    <author>
      <name>Dirk Weissenborn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in RepL4NLP at ACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03864v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03864v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03860v1</id>
    <updated>2016-06-13T08:56:35Z</updated>
    <published>2016-06-13T08:56:35Z</published>
    <title>Reweighted Data for Robust Probabilistic Models</title>
    <summary>  Probabilistic models analyze data by relying on a set of assumptions. When a
model performs poorly, we challenge its assumptions. This approach has led to
myriad hand-crafted robust models; they offer protection against small
deviations from their assumptions. We propose a simple way to systematically
mitigate mismatch of a large class of probabilistic models. The idea is to
raise the likelihood of each observation to a weight. Inferring these weights
allows a model to identify observations that match its assumptions;
down-weighting others enables robust inference and improved predictive
accuracy. We study four different forms of model mismatch, ranging from missing
latent groups to structure misspecification. A Poisson factorization analysis
of the Movielens dataset shows the benefits of reweighting in a real data
scenario.
</summary>
    <author>
      <name>Yixin Wang</name>
    </author>
    <author>
      <name>Alp Kucukelbir</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03832v1</id>
    <updated>2016-06-13T06:58:34Z</updated>
    <published>2016-06-13T06:58:34Z</published>
    <title>Evidential Label Propagation Algorithm for Graphs</title>
    <summary>  Community detection has attracted considerable attention crossing many areas
as it can be used for discovering the structure and features of complex
networks. With the increasing size of social networks in real world, community
detection approaches should be fast and accurate. The Label Propagation
Algorithm (LPA) is known to be one of the near-linear solutions and benefits of
easy implementation, thus it forms a good basis for efficient community
detection methods. In this paper, we extend the update rule and propagation
criterion of LPA in the framework of belief functions. A new community
detection approach, called Evidential Label Propagation (ELP), is proposed as
an enhanced version of conventional LPA. The node influence is first defined to
guide the propagation process. The plausibility is used to determine the domain
label of each node. The update order of nodes is discussed to improve the
robustness of the method. ELP algorithm will converge after the domain labels
of all the nodes become unchanged. The mass assignments are calculated finally
as memberships of nodes. The overlapping nodes and outliers can be detected
simultaneously through the proposed method. The experimental results
demonstrate the effectiveness of ELP.
</summary>
    <author>
      <name>Kuang Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DRUID</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DRUID</arxiv:affiliation>
    </author>
    <author>
      <name>Quan Pan</name>
    </author>
    <author>
      <name>Zhun-Ga Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19th International Conference on Information Fusion, Jul 2016,
  Heidelber, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03784v1</id>
    <updated>2016-06-13T00:12:49Z</updated>
    <published>2016-06-13T00:12:49Z</published>
    <title>MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection</title>
    <summary>  We describe MITRE's submission to the SemEval-2016 Task 6, Detecting Stance
in Tweets. This effort achieved the top score in Task A on supervised stance
detection, producing an average F1 score of 67.8 when assessing whether a tweet
author was in favor or against a topic. We employed a recurrent neural network
initialized with features learned via distant supervision on two large
unlabeled datasets. We trained embeddings of words and phrases with the
word2vec skip-gram method, then used those features to learn sentence
representations via a hashtag prediction auxiliary task. These sentence vectors
were then fine-tuned for stance detection on several hundred labeled examples.
The result was a high performing system that used transfer learning to maximize
the value of the available training data.
</summary>
    <author>
      <name>Guido Zarrella</name>
    </author>
    <author>
      <name>Amy Marsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on Semantic Evaluation 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03777v1</id>
    <updated>2016-06-12T22:59:14Z</updated>
    <published>2016-06-12T22:59:14Z</published>
    <title>Neural Belief Tracker: Data-Driven Dialogue State Tracking</title>
    <summary>  Belief tracking is a core component of modern spoken dialogue system
pipelines. However, most current approaches would have difficulty scaling to
larger, more complex dialogue domains. This is due to their dependency on
either: a) Spoken Language Understanding models that require large amounts of
annotated training data; or b) hand-crafted semantic lexicons that capture the
lexical variation in users' language. We propose a novel Neural Belief Tracking
(NBT) framework which aims to overcome these problems by building on recent
advances in semantic representation learning. The NBT models reason over
continuous distributed representations of words, utterances and dialogue
context. Our evaluation on two datasets shows that this approach overcomes both
limitations, matching the performance of state-of-the-art models that have
greater resource requirements.
</summary>
    <author>
      <name>Nikola Mrkšić</name>
    </author>
    <author>
      <name>Diarmuid Ó Séaghdha</name>
    </author>
    <author>
      <name>Tsung-Hsien Wen</name>
    </author>
    <author>
      <name>Blaise Thomson</name>
    </author>
    <author>
      <name>Steve Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submission under review for EMNLP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03737v1</id>
    <updated>2016-06-12T16:17:57Z</updated>
    <published>2016-06-12T16:17:57Z</published>
    <title>Mining the Networks of Supply and Demand of Public Transport for
  Overload and Waste of Resources</title>
    <summary>  We propose here a methodology to help to understand the shortcomings of
public transportation in a city via the mining of complex networks representing
the supply and demand of public transport. We show how to build these networks
based upon data on smart card use in buses via the application of algorithms
that estimate an OD and reconstruct the complete itinerary of the passengers.
The overlapping of the two networks sheds light in potential overload and waste
in the offer of resources that can be mitigated with strategies for balancing
supply and demand.
</summary>
    <author>
      <name>Carlos Caminha</name>
    </author>
    <author>
      <name>Vasco Furtado</name>
    </author>
    <author>
      <name>Vládia Pinheiro e Caio Ponte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages e 10 fugures. Submited to Asonam 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03667v3</id>
    <updated>2016-09-08T06:38:20Z</updated>
    <published>2016-06-12T05:38:20Z</published>
    <title>Deep Reinforcement Learning with a Combinatorial Action Space for
  Predicting Popular Reddit Threads</title>
    <summary>  We introduce an online popularity prediction and tracking task as a benchmark
task for reinforcement learning with a combinatorial, natural language action
space. A specified number of discussion threads predicted to be popular are
recommended, chosen from a fixed window of recent comments to track. Novel deep
reinforcement learning architectures are studied for effective modeling of the
value function associated with actions comprised of interdependent sub-actions.
The proposed model, which represents dependence between sub-actions through a
bi-directional LSTM, gives the best performance across different experimental
configurations and domains, and it also generalizes well with varying numbers
of recommendation requests.
</summary>
    <author>
      <name>Ji He</name>
    </author>
    <author>
      <name>Mari Ostendorf</name>
    </author>
    <author>
      <name>Xiaodong He</name>
    </author>
    <author>
      <name>Jianshu Chen</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Lihong Li</name>
    </author>
    <author>
      <name>Li Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in EMNLP 2016, 11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03667v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03667v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03662v1</id>
    <updated>2016-06-12T03:42:10Z</updated>
    <published>2016-06-12T03:42:10Z</published>
    <title>Store Location Selection via Mining Search Query Logs of Baidu Maps</title>
    <summary>  Choosing a good location when opening a new store is crucial for the future
success of a business. Traditional methods include offline manual survey, which
is very time consuming, and analytic models based on census data, which are un-
able to adapt to the dynamic market. The rapid increase of the availability of
big data from various types of mobile devices, such as online query data and
offline positioning data, provides us with the possibility to develop automatic
and accurate data-driven prediction models for business store placement. In
this paper, we propose a Demand Distribution Driven Store Placement (D3SP)
framework for business store placement by mining search query data from Baidu
Maps. D3SP first detects the spatial-temporal distributions of customer demands
on different business services via query data from Baidu Maps, the largest
online map search engine in China, and detects the gaps between demand and sup-
ply. Then we determine candidate locations via clustering such gaps. In the
final stage, we solve the location optimization problem by predicting and
ranking the number of customers. We not only deploy supervised regression
models to predict the number of customers, but also learn to rank models to
directly rank the locations. We evaluate our framework on various types of
businesses in real-world cases, and the experiments results demonstrate the
effectiveness of our methods. D3SP as the core function for store placement has
already been implemented as a core component of our business analytics platform
and could be potentially used by chain store merchants on Baidu Nuomi.
</summary>
    <author>
      <name>Mengwen Xu</name>
    </author>
    <author>
      <name>Tianyi Wang</name>
    </author>
    <author>
      <name>Zhengwei Wu</name>
    </author>
    <author>
      <name>Jingbo Zhou</name>
    </author>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Haishan Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03634v1</id>
    <updated>2016-06-11T21:49:24Z</updated>
    <published>2016-06-11T21:49:24Z</published>
    <title>The Opacity of Backbones</title>
    <summary>  A backbone of a boolean formula $F$ is a collection $S$ of its variables for
which there is a unique partial assignment $a_S$ such that $F[a_S]$ is
satisfiable [MZK+99,WGS03]. This paper studies the nontransparency of
backbones. We show that, under the widely believed assumption that integer
factoring is hard, there exist sets of boolean formulas that have obvious,
nontrivial backbones yet finding the values, $a_S$, of those backbones is
intractable. We also show that, under the same assumption, there exist sets of
boolean formulas that obviously have large backbones yet producing such a
backbone $S$ is intractable. Further, we show that if integer factoring is not
merely worst-case hard but is frequently hard, as is widely believed, then the
frequency of hardness in our two results is not too much less than that
frequency.
</summary>
    <author>
      <name>Lane A. Hemaspaandra</name>
    </author>
    <author>
      <name>David E. Narváez</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03634v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03634v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1, F.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03568v1</id>
    <updated>2016-06-11T08:12:02Z</updated>
    <published>2016-06-11T08:12:02Z</published>
    <title>Word Sense Disambiguation using a Bidirectional LSTM</title>
    <summary>  In this paper we present a model that leverages a bidirectional long
short-term memory network to learn word sense disambiguation directly from
data. The approach is end-to-end trainable and makes effective use of word
order. Further, to improve the robustness of the model we introduce dropword, a
regularization technique that randomly removes words from the text. The model
is evaluated on two standard datasets and achieves state-of-the-art results on
both datasets, using identical hyperparameter settings.
</summary>
    <author>
      <name>Mikael Kågebäck</name>
    </author>
    <author>
      <name>Hans Salomonsson</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03490v2</id>
    <updated>2016-06-16T21:21:04Z</updated>
    <published>2016-06-10T21:28:47Z</published>
    <title>The Mythos of Model Interpretability</title>
    <summary>  Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.
</summary>
    <author>
      <name>Zachary C. Lipton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">presented at 2016 ICML Workshop on Human Interpretability in Machine
  Learning (WHI 2016), New York, NY</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03490v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03490v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03476v1</id>
    <updated>2016-06-10T20:51:29Z</updated>
    <published>2016-06-10T20:51:29Z</published>
    <title>Generative Adversarial Imitation Learning</title>
    <summary>  Consider learning a policy from example expert behavior, without interaction
with the expert or access to reinforcement signal. One approach is to recover
the expert's cost function with inverse reinforcement learning, then extract a
policy from that cost function with reinforcement learning. This approach is
indirect and can be slow. We propose a new general framework for directly
extracting a policy from data, as if it were obtained by reinforcement learning
following inverse reinforcement learning. We show that a certain instantiation
of our framework draws an analogy between imitation learning and generative
adversarial networks, from which we derive a model-free imitation learning
algorithm that obtains significant performance gains over existing model-free
methods in imitating complex behaviors in large, high-dimensional environments.
</summary>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03475v1</id>
    <updated>2016-06-10T20:45:30Z</updated>
    <published>2016-06-10T20:45:30Z</published>
    <title>De-identification of Patient Notes with Recurrent Neural Networks</title>
    <summary>  Objective: Patient notes in electronic health records (EHRs) may contain
critical information for medical investigations. However, the vast majority of
medical investigators can only access de-identified notes, in order to protect
the confidentiality of patients. In the United States, the Health Insurance
Portability and Accountability Act (HIPAA) defines 18 types of protected health
information (PHI) that needs to be removed to de-identify patient notes. Manual
de-identification is impractical given the size of EHR databases, the limited
number of researchers with access to the non-de-identified notes, and the
frequent mistakes of human annotators. A reliable automated de-identification
system would consequently be of high value.
  Materials and Methods: We introduce the first de-identification system based
on artificial neural networks (ANNs), which requires no handcrafted features or
rules, unlike existing systems. We compare the performance of the system with
state-of-the-art systems on two datasets: the i2b2 2014 de-identification
challenge dataset, which is the largest publicly available de-identification
dataset, and the MIMIC de-identification dataset, which we assembled and is
twice as large as the i2b2 2014 dataset.
  Results: Our ANN model outperforms the state-of-the-art systems. It yields an
F1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision
of 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with
a recall 99.25 and a precision of 99.06.
  Conclusion: Our findings support the use of ANNs for de-identification of
patient notes, as they show better performance than previously published
systems while requiring no feature engineering.
</summary>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <author>
      <name>Ji Young Lee</name>
    </author>
    <author>
      <name>Ozlem Uzuner</name>
    </author>
    <author>
      <name>Peter Szolovits</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03432v1</id>
    <updated>2016-06-10T19:24:10Z</updated>
    <published>2016-06-10T19:24:10Z</published>
    <title>Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on
  How Much</title>
    <summary>  Gibbs sampling is a Markov Chain Monte Carlo sampling technique that
iteratively samples variables from their conditional distributions. There are
two common scan orders for the variables: random scan and systematic scan. Due
to the benefits of locality in hardware, systematic scan is commonly used, even
though most statistical guarantees are only for random scan. While it has been
conjectured that the mixing times of random scan and systematic scan do not
differ by more than a logarithmic factor, we show by counterexample that this
is not the case, and we prove that that the mixing times do not differ by more
than a polynomial factor under mild conditions. To prove these relative bounds,
we introduce a method of augmenting the state space to study systematic scan
using conductance.
</summary>
    <author>
      <name>Bryan He</name>
    </author>
    <author>
      <name>Christopher De Sa</name>
    </author>
    <author>
      <name>Ioannis Mitliagkas</name>
    </author>
    <author>
      <name>Christopher Ré</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03402v1</id>
    <updated>2016-06-10T17:30:46Z</updated>
    <published>2016-06-10T17:30:46Z</published>
    <title>Length bias in Encoder Decoder Models and a Case for Global Conditioning</title>
    <summary>  Encoder-decoder networks are popular for probabilistic modeling sequences in
many applications. These models use the power of the Long Short-Term Memory
(LSTM) architecture to capture the full dependence among variables and are not
subject to label bias of locally conditioned models that assume partial
conditional independence. However in practice they exhibit a bias towards short
sequences even when using a beam search to find the optimal sequence.
Surprisingly, sometimes there is even a decline in accuracy with increasing the
beam size.
  In this paper we show that such phenomena are due to a discrepancy between
the full sequence margin and the per-element margin enforced by the locally
conditioned training objective of a encoder-decoder model. The discrepancy more
adversely impacts long sequences, explaining the bias towards predicting short
sequences.
  For the case where the predicted sequences come from a closed set, we show
that a globally conditioned model alleviates the above problems of
encoder-decoder models. From a practical point of view, our proposed model also
eliminates the need for a beam-search during inference, which reduces to an
efficient dot-product based search in a vector-space.
</summary>
    <author>
      <name>Pavel Sountsov</name>
    </author>
    <author>
      <name>Sunita Sarawagi</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03335v1</id>
    <updated>2016-06-10T14:12:47Z</updated>
    <published>2016-06-10T14:12:47Z</published>
    <title>WordNet2Vec: Corpora Agnostic Word Vectorization Method</title>
    <summary>  A complex nature of big data resources demands new methods for structuring
especially for textual content. WordNet is a good knowledge source for
comprehensive abstraction of natural language as its good implementations exist
for many languages. Since WordNet embeds natural language in the form of a
complex network, a transformation mechanism WordNet2Vec is proposed in the
paper. It creates vectors for each word from WordNet. These vectors encapsulate
general position - role of a given word towards all other words in the natural
language. Any list or set of such vectors contains knowledge about the context
of its component within the whole language. Such word representation can be
easily applied to many analytic tasks like classification or clustering. The
usefulness of the WordNet2Vec method was demonstrated in sentiment analysis,
i.e. classification with transfer learning for the real Amazon opinion textual
dataset.
</summary>
    <author>
      <name>Roman Bartusiak</name>
    </author>
    <author>
      <name>Łukasz Augustyniak</name>
    </author>
    <author>
      <name>Tomasz Kajdanowicz</name>
    </author>
    <author>
      <name>Przemysław Kazienko</name>
    </author>
    <author>
      <name>Maciej Piasecki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 16 figures, submitted to journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03329v2</id>
    <updated>2016-07-03T21:04:10Z</updated>
    <published>2016-06-10T14:02:12Z</published>
    <title>Community Structure in Industrial SAT Instances</title>
    <summary>  Modern SAT solvers have experienced a remarkable progress on solving
industrial instances. Most of the techniques have been developed after an
intensive experimental process. It is believed that these techniques exploit
the underlying structure of industrial instances. However, there are few works
trying to exactly characterize the main features of this structure.
  The research community on complex networks has developed techniques of
analysis and algorithms to study real-world graphs that can be used by the SAT
community. Recently, there have been some attempts to analyze the structure of
industrial SAT instances in terms of complex networks, with the aim of
explaining the success of SAT solving techniques, and possibly improving them.
  In this paper, inspired by the results on complex networks, we study the
community structure, or modularity, of industrial SAT instances. In a graph
with clear community structure, or high modularity, we can find a partition of
its nodes into communities such that most edges connect variables of the same
community. In our analysis, we represent SAT instances as graphs, and we show
that most application benchmarks are characterized by a high modularity. On the
contrary, random SAT instances are closer to the classical Erd\"os-R\'enyi
random graph model, where no structure can be observed. We also analyze how
this structure evolves by the effects of the execution of the SAT solver. We
detect that new clauses learnt by the solver during the search contribute to
destroy the original community structure of the formula. This partially
explains the distinct performance of SAT solvers on random and industrial SAT
instances.
</summary>
    <author>
      <name>Carlos Ansótegui</name>
    </author>
    <author>
      <name>Maria Luisa Bonet</name>
    </author>
    <author>
      <name>Jesús Giráldez-Cru</name>
    </author>
    <author>
      <name>Jordi Levy</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03298v1</id>
    <updated>2016-06-10T12:53:01Z</updated>
    <published>2016-06-10T12:53:01Z</published>
    <title>Structured Factored Inference: A Framework for Automated Reasoning in
  Probabilistic Programming Languages</title>
    <summary>  Reasoning on large and complex real-world models is a computationally
difficult task, yet one that is required for effective use of many AI
applications. A plethora of inference algorithms have been developed that work
well on specific models or only on parts of general models. Consequently, a
system that can intelligently apply these inference algorithms to different
parts of a model for fast reasoning is highly desirable. We introduce a new
framework called structured factored inference (SFI) that provides the
foundation for such a system. Using models encoded in a probabilistic
programming language, SFI provides a sound means to decompose a model into
sub-models, apply an inference algorithm to each sub-model, and combine the
resulting information to answer a query. Our results show that SFI is nearly as
accurate as exact inference yet retains the benefits of approximate inference
methods.
</summary>
    <author>
      <name>Avi Pfeffer</name>
    </author>
    <author>
      <name>Brian Ruttenberg</name>
    </author>
    <author>
      <name>William Kretschmer</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03289v1</id>
    <updated>2016-06-10T12:24:35Z</updated>
    <published>2016-06-10T12:24:35Z</published>
    <title>Tunable Online MUS/MSS Enumeration</title>
    <summary>  In various areas of computer science, the problem of dealing with a set of
constraints arises. If the set of constraints is unsatisfiable, one may ask for
a minimal description of the reason for this unsatisifi- ability. Minimal
unsatisifable subsets (MUSes) and maximal satisifiable subsets (MSSes) are two
kinds of such minimal descriptions. The goal of this work is the enumeration of
MUSes and MSSes for a given constraint system. As such full enumeration may be
intractable in general, we focus on building an online algorithm, which
produces MUSes/MSSes in an on-the-fly manner as soon as they are discovered.
The problem has been studied before even in its online version. However, our
algorithm uses a novel approach that is able to outperform current state-of-the
art algorithms for online MUS/MSS enumeration. Moreover, the performance of our
algorithm can be adjusted using tunable parameters. We evaluate the algorithm
on a set of benchmarks.
</summary>
    <author>
      <name>Jaroslav Bendik</name>
    </author>
    <author>
      <name>Nikola Benes</name>
    </author>
    <author>
      <name>Ivana Cerna</name>
    </author>
    <author>
      <name>Jiri Barnat</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03254v2</id>
    <updated>2016-08-15T10:11:49Z</updated>
    <published>2016-06-10T10:12:13Z</published>
    <title>Natural Language Generation enhances human decision-making with
  uncertain information</title>
    <summary>  Decision-making is often dependent on uncertain data, e.g. data associated
with confidence scores or probabilities. We present a comparison of different
information presentations for uncertain data and, for the first time, measure
their effects on human decision-making. We show that the use of Natural
Language Generation (NLG) improves decision-making under uncertainty, compared
to state-of-the-art graphical-based representation methods. In a task-based
study with 442 adults, we found that presentations using NLG lead to 24% better
decision-making on average than the graphical presentations, and to 44% better
decision-making when NLG is combined with graphics. We also show that women
achieve significantly better results when presented with NLG output (an 87%
increase on average compared to graphical presentations).
</summary>
    <author>
      <name>Dimitra Gkatzia</name>
    </author>
    <author>
      <name>Oliver Lemon</name>
    </author>
    <author>
      <name>Verena Rieser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">54th annual meeting of the Association for Computational Linguistics
  (ACL), Berlin 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03254v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03254v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03244v2</id>
    <updated>2016-06-14T15:49:20Z</updated>
    <published>2016-06-10T09:31:26Z</published>
    <title>Simple epistemic planning: generalised gossiping</title>
    <summary>  The gossip problem, in which information (known as secrets) must be shared
among a certain number of agents using the minimum number of calls, is of
interest in the conception of communication networks and protocols. We extend
the gossip problem to arbitrary epistemic depths. For example, we may require
not only that all agents know all secrets but also that all agents know that
all agents know all secrets. We give optimal protocols for various versions of
the generalised gossip problem, depending on the graph of communication links,
in the case of two-way communications, one-way communications and parallel
communication. We also study different variants which allow us to impose
negative goals such as that certain agents must not know certain secrets. We
show that in the presence of negative goals testing the existence of a
successful protocol is NP-complete whereas this is always polynomial-time in
the case of purely positive goals.
</summary>
    <author>
      <name>Martin C. Cooper</name>
    </author>
    <author>
      <name>Andreas Herzig</name>
    </author>
    <author>
      <name>Faustine Maffre</name>
    </author>
    <author>
      <name>Frédéric Maris</name>
    </author>
    <author>
      <name>Pierre Régnier</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03244v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03244v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; C.2.2; F.2.2; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03229v1</id>
    <updated>2016-06-10T08:39:22Z</updated>
    <published>2016-06-10T08:39:22Z</published>
    <title>Towards Anthropo-inspired Computational Systems: the $P^3$ Model</title>
    <summary>  This paper proposes a model which aim is providing a more coherent framework
for agents design. We identify three closely related anthropo-centered domains
working on separate functional levels. Abstracting from human physiology,
psychology, and philosophy we create the $P^3$ model to be used as a multi-tier
approach to deal with complex class of problems. The three layers identified in
this model have been named PhysioComputing, MindComputing, and MetaComputing.
Several instantiations of this model are finally presented related to different
IT areas such as artificial intelligence, distributed computing, software and
service engineering.
</summary>
    <author>
      <name>Michael W. Bridges</name>
    </author>
    <author>
      <name>Salvatore Distefano</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>Marat Minlebaev</name>
    </author>
    <author>
      <name>Max Talanov</name>
    </author>
    <author>
      <name>Jordi Vallverdú</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of the 9th International KES Conference on AGENTS
  AND MULTI-AGENT SYSTEMS: TECHNOLOGIES AND APPLICATIONS, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.03229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03191v1</id>
    <updated>2016-06-10T05:55:56Z</updated>
    <published>2016-06-10T05:55:56Z</published>
    <title>Fuzzy-Klassen Model for Development Disparities Analysis based on Gross
  Regional Domestic Product Sector of a Region</title>
    <summary>  Analysis of regional development imbalances quadrant has a very important
meaning in order to see the extent of achievement of the development of certain
areas as well as the difference. Factors that could be used as a tool to
measure the inequality of development is to look at the average growth and
development contribution of each sector of Gross Regional Domestic Product
(GRDP) based on the analyzed region and the reference region. This study
discusses the development of a model to determine the regional development
imbalances using fuzzy approach system, and the rules of typology Klassen. The
model is then called fuzzy-Klassen. Implications Product Mamdani fuzzy system
is used in the model as an inference engine to generate output after
defuzzyfication process. Application of MATLAB is used as a tool of analysis in
this study. The test a result of Kota Cilegon is shows that there are
significant differences between traditional Klassen typology analyses with the
results of the model developed. Fuzzy model-Klassen shows GRDP sector
inequality Cilegon City is dominated by Quadrant I (K4), where status is the
sector forward and grows exponentially. While the traditional Klassen typology,
half of GRDP sector is dominated by Quadrant IV (K4) with a sector that is
lagging relative status.
</summary>
    <author>
      <name>Tb. Ai Munandar</name>
    </author>
    <author>
      <name>Retantyo Wardoyo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/ijca2015905389</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/ijca2015905389" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 1 Figures, 5 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03152v4</id>
    <updated>2016-09-12T16:23:42Z</updated>
    <published>2016-06-10T01:02:19Z</published>
    <title>Policy Networks with Two-Stage Training for Dialogue Systems</title>
    <summary>  In this paper, we propose to use deep policy networks which are trained with
an advantage actor-critic method for statistically optimised dialogue systems.
First, we show that, on summary state and action spaces, deep Reinforcement
Learning (RL) outperforms Gaussian Processes methods. Summary state and action
spaces lead to good performance but require pre-engineering effort, RL
knowledge, and domain expertise. In order to remove the need to define such
summary spaces, we show that deep RL can also be trained efficiently on the
original state and action spaces. Dialogue systems based on partially
observable Markov decision processes are known to require many dialogues to
train, which makes them unappealing for practical deployment. We show that a
deep RL method based on an actor-critic architecture can exploit a small amount
of data very efficiently. Indeed, with only a few hundred dialogues collected
with a handcrafted policy, the actor-critic deep learner is considerably
bootstrapped from a combination of supervised and batch RL. In addition,
convergence to an optimal policy is significantly sped up compared to other
deep RL methods initialized on the data with batch RL. All experiments are
performed on a restaurant domain derived from the Dialogue State Tracking
Challenge 2 (DSTC2) dataset.
</summary>
    <author>
      <name>Mehdi Fatemi</name>
    </author>
    <author>
      <name>Layla El Asri</name>
    </author>
    <author>
      <name>Hannes Schulz</name>
    </author>
    <author>
      <name>Jing He</name>
    </author>
    <author>
      <name>Kaheer Suleman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SIGDial 2016 (Submitted: May 2016; Accepted: Jun 30, 2016)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the SIGDIAL 2016 Conference, pages 101--110, Los
  Angeles, USA, 13-15 September 2016. Association for Computational Linguistics</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.03152v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03152v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03137v2</id>
    <updated>2016-07-05T18:25:07Z</updated>
    <published>2016-06-09T22:39:54Z</published>
    <title>Cooperative Inverse Reinforcement Learning</title>
    <summary>  For an autonomous system to be helpful to humans and to pose no unwarranted
risks, it needs to align its values with those of the humans in its environment
in such a way that its actions contribute to the maximization of value for the
humans. We propose a formal definition of the value alignment problem as
cooperative inverse reinforcement learning (CIRL). A CIRL problem is a
cooperative, partial-information game with two agents, human and robot; both
are rewarded according to the human's reward function, but the robot does not
initially know what this is. In contrast to classical IRL, where the human is
assumed to act optimally in isolation, optimal CIRL solutions produce behaviors
such as active teaching, active learning, and communicative actions that are
more effective in achieving value alignment. We show that computing optimal
joint policies in CIRL games can be reduced to solving a POMDP, prove that
optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL
algorithm.
</summary>
    <author>
      <name>Dylan Hadfield-Menell</name>
    </author>
    <author>
      <name>Anca Dragan</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <author>
      <name>Stuart Russell</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03137v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03137v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03044v1</id>
    <updated>2016-06-09T18:10:31Z</updated>
    <published>2016-06-09T18:10:31Z</published>
    <title>The "Horse'' Inside: Seeking Causes Behind the Behaviours of Music
  Content Analysis Systems</title>
    <summary>  Building systems that possess the sensitivity and intelligence to identify
and describe high-level attributes in music audio signals continues to be an
elusive goal, but one that surely has broad and deep implications for a wide
variety of applications. Hundreds of papers have so far been published toward
this goal, and great progress appears to have been made. Some systems produce
remarkable accuracies at recognising high-level semantic concepts, such as
music style, genre and mood. However, it might be that these numbers do not
mean what they seem. In this paper, we take a state-of-the-art music content
analysis system and investigate what causes it to achieve exceptionally high
performance in a benchmark music audio dataset. We dissect the system to
understand its operation, determine its sensitivities and limitations, and
predict the kinds of knowledge it could and could not possess about music. We
perform a series of experiments to illuminate what the system has actually
learned to do, and to what extent it is performing the intended music listening
task. Our results demonstrate how the initial manifestation of music
intelligence in this state-of-the-art can be deceptive. Our work provides
constructive directions toward developing music content analysis systems that
can address the music information and creation needs of real-world users.
</summary>
    <author>
      <name>Bob L. Sturm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 17 figures, this work was accepted for publication in a
  journal special issue in Apr. 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; I.2.6; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03002v1</id>
    <updated>2016-06-09T15:41:17Z</updated>
    <published>2016-06-09T15:41:17Z</published>
    <title>MuFuRU: The Multi-Function Recurrent Unit</title>
    <summary>  Recurrent neural networks such as the GRU and LSTM found wide adoption in
natural language processing and achieve state-of-the-art results for many
tasks. These models are characterized by a memory state that can be written to
and read from by applying gated composition operations to the current input and
the previous state. However, they only cover a small subset of potentially
useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that
allow for arbitrary differentiable functions as composition operations.
Furthermore, MuFuRUs allow for an input- and state-dependent choice of these
composition operations that is learned. Our experiments demonstrate that the
additional functionality helps in different sequence modeling tasks, including
the evaluation of propositional logic formulae, language modeling and sentiment
analysis.
</summary>
    <author>
      <name>Dirk Weissenborn</name>
    </author>
    <author>
      <name>Tim Rocktäschel</name>
    </author>
    <link href="http://arxiv.org/abs/1606.03002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02979v2</id>
    <updated>2016-08-08T14:49:07Z</updated>
    <published>2016-06-09T14:45:39Z</published>
    <title>Generative Topic Embedding: a Continuous Representation of Documents
  (Extended Version with Proofs)</title>
    <summary>  Word embedding maps words into a low-dimensional continuous embedding space
by exploiting the local word collocation patterns in a small context window. On
the other hand, topic modeling maps documents onto a low-dimensional topic
space, by utilizing the global word collocation patterns in the same document.
These two types of patterns are complementary. In this paper, we propose a
generative topic embedding model to combine the two types of patterns. In our
model, topics are represented by embedding vectors, and are shared across
documents. The probability of each word is influenced by both its local context
and its topic. A variational inference method yields the topic embeddings as
well as the topic mixing proportions for each document. Jointly they represent
the document in a low-dimensional continuous space. In two document
classification tasks, our method performs better than eight existing methods,
with fewer features. In addition, we illustrate with an example that our method
can generate coherent topics even based on only one document.
</summary>
    <author>
      <name>Shaohua Li</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <author>
      <name>Jun Zhu</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages. The original version has been accepted in ACL 2016 as a
  long paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02979v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02979v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02899v1</id>
    <updated>2016-06-09T10:43:21Z</updated>
    <published>2016-06-09T10:43:21Z</published>
    <title>A Cognitive Architecture for the Implementation of Emotions in Computing
  Systems</title>
    <summary>  In this paper we present a new neurobiologically-inspired affective cognitive
architecture: NEUCOGAR (NEUromodulating COGnitive ARchitecture). The objective
of NEUCOGAR is the identification of a mapping from the influence of serotonin,
dopamine and noradrenaline to the computing processes based on Von Neuman's
architecture, in order to implement affective phenomena which can operate on
the Turing's machine model. As basis of the modeling we use and extend the
L\"ovheim Cube of Emotion with parameters of the Von Neumann architecture.
Validation is conducted via simulation on a computing system of dopamine
neuromodulation and its effects on the Cortex. In the experimental phase of the
project, the increase of computing power and storage redistribution due to
emotion stimulus modulated by the dopamine system, confirmed the soundness of
the model.
</summary>
    <author>
      <name>Jordi Vallverdú</name>
    </author>
    <author>
      <name>Max Talanov</name>
    </author>
    <author>
      <name>Salvatore Distefano</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>Alexander Tchitchigin</name>
    </author>
    <author>
      <name>Ildar Nurgaliev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.bica.2015.11.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.bica.2015.11.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">BICA, Volume 15, January 2016, Pages 34-40</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.02899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02877v1</id>
    <updated>2016-06-09T09:02:16Z</updated>
    <published>2016-06-09T09:02:16Z</published>
    <title>Understanding User Instructions by Utilizing Open Knowledge for Service
  Robots</title>
    <summary>  Understanding user instructions in natural language is an active research
topic in AI and robotics. Typically, natural user instructions are high-level
and can be reduced into low-level tasks expressed in common verbs (e.g.,
`take', `get', `put'). For robots understanding such instructions, one of the
key challenges is to process high-level user instructions and achieve the
specified tasks with robots' primitive actions. To address this, we propose
novel algorithms by utilizing semantic roles of common verbs defined in
semantic dictionaries and integrating multiple open knowledge to generate task
plans. Specifically, we present a new method for matching and recovering
semantics of user instructions and a novel task planner that exploits
functional knowledge of robot's action model. To verify and evaluate our
approach, we implemented a prototype system using knowledge from several open
resources. Experiments on our system confirmed the correctness and efficiency
of our algorithms. Notably, our system has been deployed in the KeJia robot,
which participated the annual RoboCup@Home competitions in the past three years
and achieved encouragingly high scores in the benchmark tests.
</summary>
    <author>
      <name>Dongcai Lu</name>
    </author>
    <author>
      <name>Feng Wu</name>
    </author>
    <author>
      <name>Xiaoping Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02858v2</id>
    <updated>2016-08-08T21:21:19Z</updated>
    <published>2016-06-09T08:19:16Z</published>
    <title>A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</title>
    <summary>  Enabling a computer to understand a document so that it can answer
comprehension questions is a central, yet unsolved goal of NLP. A key factor
impeding its solution by machine learned systems is the limited availability of
human-annotated data. Hermann et al. (2015) seek to solve this problem by
creating over a million training examples by pairing CNN and Daily Mail news
articles with their summarized bullet points, and show that a neural network
can then be trained to give good performance on this task. In this paper, we
conduct a thorough examination of this new reading comprehension task. Our
primary aim is to understand what depth of language understanding is required
to do well on this task. We approach this from one side by doing a careful
hand-analysis of a small subset of the problems and from the other by showing
that simple, carefully designed systems can obtain accuracies of 73.6% and
76.6% on these two datasets, exceeding current state-of-the-art results by
7-10% and approaching what we believe is the ceiling for performance on this
task.
</summary>
    <author>
      <name>Danqi Chen</name>
    </author>
    <author>
      <name>Jason Bolton</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACL 2016, updated results</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02858v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02858v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02854v1</id>
    <updated>2016-06-09T08:06:00Z</updated>
    <published>2016-06-09T08:06:00Z</published>
    <title>e-Commerce product classification: our participation at cDiscount 2015
  challenge</title>
    <summary>  This report describes our participation in the cDiscount 2015 challenge where
the goal was to classify product items in a predefined taxonomy of products.
Our best submission yielded an accuracy score of 64.20\% in the private part of
the leaderboard and we were ranked 10th out of 175 participating teams. We
followed a text classification approach employing mainly linear models. The
final solution was a weighted voting system which combined a variety of trained
models.
</summary>
    <author>
      <name>Ioannis Partalas</name>
    </author>
    <author>
      <name>Georgios Balikas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02825v2</id>
    <updated>2016-06-10T13:48:30Z</updated>
    <published>2016-06-09T04:59:50Z</published>
    <title>Arbitrage-Free Combinatorial Market Making via Integer Programming</title>
    <summary>  We present a new combinatorial market maker that operates arbitrage-free
combinatorial prediction markets specified by integer programs. Although the
problem of arbitrage-free pricing, while maintaining a bound on the subsidy
provided by the market maker, is #P-hard in the worst case, we posit that the
typical case might be amenable to modern integer programming (IP) solvers. At
the crux of our method is the Frank-Wolfe (conditional gradient) algorithm
which is used to implement a Bregman projection aligned with the market maker's
cost function, using an IP solver as an oracle. We demonstrate the tractability
and improved accuracy of our approach on real-world prediction market data from
combinatorial bets placed on the 2010 NCAA Men's Division I Basketball
Tournament, where the outcome space is of size 2^63. To our knowledge, this is
the first implementation and empirical evaluation of an arbitrage-free
combinatorial prediction market on this scale.
</summary>
    <author>
      <name>Christian Kroer</name>
    </author>
    <author>
      <name>Miroslav Dudík</name>
    </author>
    <author>
      <name>Sébastien Lahaie</name>
    </author>
    <author>
      <name>Sivaraman Balakrishnan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2940716.2940767</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2940716.2940767" rel="related"/>
    <link href="http://arxiv.org/abs/1606.02825v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02825v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02807v1</id>
    <updated>2016-06-09T03:06:46Z</updated>
    <published>2016-06-09T03:06:46Z</published>
    <title>Face valuing: Training user interfaces with facial expressions and
  reinforcement learning</title>
    <summary>  An important application of interactive machine learning is extending or
amplifying the cognitive and physical capabilities of a human. To accomplish
this, machines need to learn about their human users' intentions and adapt to
their preferences. In most current research, a user has conveyed preferences to
a machine using explicit corrective or instructive feedback; explicit feedback
imposes a cognitive load on the user and is expensive in terms of human effort.
The primary objective of the current work is to demonstrate that a learning
agent can reduce the amount of explicit feedback required for adapting to the
user's preferences pertaining to a task by learning to perceive a value of its
behavior from the human user, particularly from the user's facial
expressions---we call this face valuing. We empirically evaluate face valuing
on a grip selection task. Our preliminary results suggest that an agent can
quickly adapt to a user's changing preferences with minimal explicit feedback
by learning a value function that maps facial features extracted from a camera
image to expected future reward. We believe that an agent learning to perceive
a value from the body language of its human user is complementary to existing
interactive machine learning approaches and will help in creating successful
human-machine interactive applications.
</summary>
    <author>
      <name>Vivek Veeriah</name>
    </author>
    <author>
      <name>Patrick M. Pilarski</name>
    </author>
    <author>
      <name>Richard S. Sutton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, IJCAI 2016 - Interactive Machine Learning
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02767v2</id>
    <updated>2016-06-23T13:27:01Z</updated>
    <published>2016-06-08T21:46:20Z</published>
    <title>Theoretical Robopsychology: Samu Has Learned Turing Machines</title>
    <summary>  From the point of view of a programmer, the robopsychology is a synonym for
the activity is done by developers to implement their machine learning
applications. This robopsychological approach raises some fundamental
theoretical questions of machine learning. Our discussion of these questions is
constrained to Turing machines. Alan Turing had given an algorithm (aka the
Turing Machine) to describe algorithms. If it has been applied to describe
itself then this brings us to Turing's notion of the universal machine. In the
present paper, we investigate algorithms to write algorithms. From a pedagogy
point of view, this way of writing programs can be considered as a combination
of learning by listening and learning by doing due to it is based on applying
agent technology and machine learning. As the main result we introduce the
problem of learning and then we show that it cannot easily be handled in
reality therefore it is reasonable to use machine learning algorithm for
learning Turing machines.
</summary>
    <author>
      <name>Norbert Bátfai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, added a missing cc* value and the appearance of Table 1 is
  improved</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02767v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02767v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02647v1</id>
    <updated>2016-06-08T17:34:13Z</updated>
    <published>2016-06-08T17:34:13Z</published>
    <title>Safe and Efficient Off-Policy Reinforcement Learning</title>
    <summary>  In this work, we take a fresh look at some old and new algorithms for
off-policy, return-based reinforcement learning. Expressing these in a common
form, we derive a novel algorithm, Retrace($\lambda$), with three desired
properties: (1) low variance; (2) safety, as it safely uses samples collected
from any behaviour policy, whatever its degree of "off-policyness"; and (3)
efficiency, as it makes the best use of samples collected from near on-policy
behaviour policies. We analyse the contractive nature of the related operator
under both off-policy policy evaluation and control settings and derive online
sample-based algorithms. To our knowledge, this is the first return-based
off-policy control algorithm converging a.s. to $Q^*$ without the GLIE
assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we
prove the convergence of Watkins' Q($\lambda$), which was still an open
problem. We illustrate the benefits of Retrace($\lambda$) on a standard suite
of Atari 2600 games.
</summary>
    <author>
      <name>Rémi Munos</name>
    </author>
    <author>
      <name>Tom Stepleton</name>
    </author>
    <author>
      <name>Anna Harutyunyan</name>
    </author>
    <author>
      <name>Marc G. Bellemare</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02645v2</id>
    <updated>2016-07-15T14:02:55Z</updated>
    <published>2016-06-08T17:29:17Z</published>
    <title>Simplified Boardgames</title>
    <summary>  We formalize Simplified Boardgames language, which describes a subclass of
arbitrary board games. The language structure is based on the regular
expressions, which makes the rules easily machine-processable while keeping the
rules concise and fairly human-readable.
</summary>
    <author>
      <name>Jakub Kowalski</name>
    </author>
    <author>
      <name>Jakub Sutowicz</name>
    </author>
    <author>
      <name>Marek Szykuła</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02645v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02645v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02583v1</id>
    <updated>2016-06-08T14:47:35Z</updated>
    <published>2016-06-08T14:47:35Z</published>
    <title>The Dark Side of Ethical Robots</title>
    <summary>  Concerns over the risks associated with advances in Artificial Intelligence
have prompted calls for greater efforts toward robust and beneficial AI,
including machine ethics. Recently, roboticists have responded by initiating
the development of so-called ethical robots. These robots would, ideally,
evaluate the consequences of their actions and morally justify their choices.
This emerging field promises to develop extensively over the next years.
However, in this paper, we point out an inherent limitation of the emerging
field of ethical robots. We show that building ethical robots also necessarily
facilitates the construction of unethical robots. In three experiments, we show
that it is remarkably easy to modify an ethical robot so that it behaves
competitively, or even aggressively. The reason for this is that the specific
AI, required to make an ethical robot, can always be exploited to make
unethical robots. Hence, the development of ethical robots will not guarantee
the responsible deployment of AI. While advocating for ethical robots, we
conclude that preventing the misuse of robots is beyond the scope of
engineering, and requires instead governance frameworks underpinned by
legislation. Without this, the development of ethical robots will serve to
increase the risks of robotic malpractice instead of diminishing it.
</summary>
    <author>
      <name>Dieter Vanderelst</name>
    </author>
    <author>
      <name>Alan Winfield</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02562v1</id>
    <updated>2016-06-08T14:08:21Z</updated>
    <published>2016-06-08T14:08:21Z</published>
    <title>DialPort: Connecting the Spoken Dialog Research Community to Real User
  Data</title>
    <summary>  This paper describes a new spoken dialog portal that connects systems
produced by the spoken dialog academic research community and gives them access
to real users. We introduce a distributed, multi-modal, multi-agent prototype
dialog framework that affords easy integration with various remote resources,
ranging from end-to-end dialog systems to external knowledge APIs. To date, the
DialPort portal has successfully connected to the multi-domain spoken dialog
system at Cambridge University, the NOAA (National Oceanic and Atmospheric
Administration) weather API and the Yelp API.
</summary>
    <author>
      <name>Tiancheng Zhao</name>
    </author>
    <author>
      <name>Kyusong Lee</name>
    </author>
    <author>
      <name>Maxine Eskenazi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Peer Review of SigDial 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02560v1</id>
    <updated>2016-06-08T14:03:25Z</updated>
    <published>2016-06-08T14:03:25Z</published>
    <title>Towards End-to-End Learning for Dialog State Tracking and Management
  using Deep Reinforcement Learning</title>
    <summary>  This paper presents an end-to-end framework for task-oriented dialog systems
using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to
interface with a relational database and jointly learn policies for both
language understanding and dialog strategy. Moreover, we propose a hybrid
algorithm that combines the strength of reinforcement learning and supervised
learning to achieve faster learning speed. We evaluated the proposed model on a
20 Question Game conversational game simulator. Results show that the proposed
method outperforms the modular-based baseline and learns a distributed
representation of the latent dialog state.
</summary>
    <author>
      <name>Tiancheng Zhao</name>
    </author>
    <author>
      <name>Maxine Eskenazi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Under peer review of to SigDial 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02556v4</id>
    <updated>2016-08-24T15:19:45Z</updated>
    <published>2016-06-08T13:57:44Z</published>
    <title>DISCO Nets: DISsimilarity COefficient Networks</title>
    <summary>  We present a new type of probabilistic model which we call DISsimilarity
COefficient Networks (DISCO Nets). DISCO Nets allow us to efficiently sample
from a posterior distribution parametrised by a neural network. During
training, DISCO Nets are learned by minimising the dissimilarity coefficient
between the true distribution and the estimated distribution. This allows us to
tailor the training to the loss related to the task at hand. We empirically
show that (i) by modeling uncertainty on the output value, DISCO Nets
outperform equivalent non-probabilistic predictive networks and (ii) DISCO Nets
accurately model the uncertainty of the output, outperforming existing
probabilistic models based on deep neural networks.
</summary>
    <author>
      <name>Diane Bouchacourt</name>
    </author>
    <author>
      <name>M. Pawan Kumar</name>
    </author>
    <author>
      <name>Sebastian Nowozin</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02556v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02556v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02542v1</id>
    <updated>2016-06-08T13:19:01Z</updated>
    <published>2016-06-08T13:19:01Z</published>
    <title>Symbolic Music Data Version 1.0</title>
    <summary>  In this document, we introduce a new dataset designed for training machine
learning models of symbolic music data. Five datasets are provided, one of
which is from a newly collected corpus of 20K midi files. We describe our
preprocessing and cleaning pipeline, which includes the exclusion of a number
of files based on scores from a previously developed probabilistic machine
learning model. We also define training, testing and validation splits for the
new dataset, based on a clustering scheme which we also describe. Some simple
histograms are included.
</summary>
    <author>
      <name>Christian Walder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1606.01368</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02710v1</id>
    <updated>2016-06-08T12:00:28Z</updated>
    <published>2016-06-08T12:00:28Z</published>
    <title>A Modified Vortex Search Algorithm for Numerical Function Optimization</title>
    <summary>  The Vortex Search (VS) algorithm is one of the recently proposed
metaheuristic algorithms which was inspired from the vortical flow of the
stirred fluids. Although the VS algorithm is shown to be a good candidate for
the solution of certain optimization problems, it also has some drawbacks. In
the VS algorithm, candidate solutions are generated around the current best
solution by using a Gaussian distribution at each iteration pass. This provides
simplicity to the algorithm but it also leads to some problems along.
Especially, for the functions those have a number of local minimum points, to
select a single point to generate candidate solutions leads the algorithm to
being trapped into a local minimum point. Due to the adaptive step-size
adjustment scheme used in the VS algorithm, the locality of the created
candidate solutions is increased at each iteration pass. Therefore, if the
algorithm cannot escape a local point as quickly as possible, it becomes much
more difficult for the algorithm to escape from that point in the latter
iterations. In this study, a modified Vortex Search algorithm (MVS) is proposed
to overcome above mentioned drawback of the existing VS algorithm. In the MVS
algorithm, the candidate solutions are generated around a number of points at
each iteration pass. Computational results showed that with the help of this
modification the global search ability of the existing VS algorithm is improved
and the MVS algorithm outperformed the existing VS algorithm, PSO2011 and ABC
algorithms for the benchmark numerical function set.
</summary>
    <author>
      <name>Berat Doğan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijaia.2016.7304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijaia.2016.7304" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of Artificial Intelligence &amp; Applications
  (IJAIA), Volume 7, Number 3, May 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.02710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02485v1</id>
    <updated>2016-06-08T10:07:01Z</updated>
    <published>2016-06-08T10:07:01Z</published>
    <title>Exploring Implicit Human Responses to Robot Mistakes in a Learning from
  Demonstration Task</title>
    <summary>  As robots enter human environments, they will be expected to accomplish a
tremendous range of tasks. It is not feasible for robot designers to
pre-program these behaviors or know them in advance, so one way to address this
is through end-user programming, such as via learning from demonstration (LfD).
While significant work has been done on the mechanics of enabling robot
learning from human teachers, one unexplored aspect is enabling mutual feedback
between both the human teacher and robot during the learning process, i.e.,
implicit learning. In this paper, we explore one aspect of this mutual
understanding, grounding sequences, where both a human and robot provide
non-verbal feedback to signify their mutual understanding during interaction.
We conducted a study where people taught an autonomous humanoid robot a dance,
and performed gesture analysis to measure people's responses to the robot
during correct and incorrect demonstrations.
</summary>
    <author>
      <name>Cory J. Hayes</name>
    </author>
    <author>
      <name>Maryam Moosaei</name>
    </author>
    <author>
      <name>Laurel D. Riek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, IEEE RO-MAN 2016, IEEE International Symposium on
  Robot and Human Interactive Communication (RO-MAN 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02447v1</id>
    <updated>2016-06-08T08:27:09Z</updated>
    <published>2016-06-08T08:27:09Z</published>
    <title>Learning Language Games through Interaction</title>
    <summary>  We introduce a new language learning setting relevant to building adaptive
natural language interfaces. It is inspired by Wittgenstein's language games: a
human wishes to accomplish some task (e.g., achieving a certain configuration
of blocks), but can only communicate with a computer, who performs the actual
actions (e.g., removing all red blocks). The computer initially knows nothing
about language and therefore must learn it from scratch through interaction,
while the human adapts to the computer's capabilities. We created a game in a
blocks world and collected interactions from 100 people playing it. First, we
analyze the humans' strategies, showing that using compositionality and
avoiding synonyms correlates positively with task performance. Second, we
compare computer strategies, showing how to quickly learn a semantic parsing
model from scratch, and that modeling pragmatics further accelerates learning
for successful players.
</summary>
    <author>
      <name>Sida I. Wang</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <author>
      <name>Christopher D. Manning</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, ACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02407v1</id>
    <updated>2016-06-08T05:31:43Z</updated>
    <published>2016-06-08T05:31:43Z</published>
    <title>Structured Convolution Matrices for Energy-efficient Deep learning</title>
    <summary>  We derive a relationship between network representation in energy-efficient
neuromorphic architectures and block Toplitz convolutional matrices. Inspired
by this connection, we develop deep convolutional networks using a family of
structured convolutional matrices and achieve state-of-the-art trade-off
between energy efficiency and classification accuracy for well-known image
recognition tasks. We also put forward a novel method to train binary
convolutional networks by utilising an existing connection between
noisy-rectified linear units and binary activations.
</summary>
    <author>
      <name>Rathinakumar Appuswamy</name>
    </author>
    <author>
      <name>Tapan Nayak</name>
    </author>
    <author>
      <name>John Arthur</name>
    </author>
    <author>
      <name>Steven Esser</name>
    </author>
    <author>
      <name>Paul Merolla</name>
    </author>
    <author>
      <name>Jeffrey Mckinstry</name>
    </author>
    <author>
      <name>Timothy Melano</name>
    </author>
    <author>
      <name>Myron Flickner</name>
    </author>
    <author>
      <name>Dharmendra Modha</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02396v1</id>
    <updated>2016-06-08T04:48:49Z</updated>
    <published>2016-06-08T04:48:49Z</published>
    <title>Deep Successor Reinforcement Learning</title>
    <summary>  Learning robust value functions given raw observations and rewards is now
possible with model-free and model-based deep reinforcement learning
algorithms. There is a third alternative, called Successor Representations
(SR), which decomposes the value function into two components -- a reward
predictor and a successor map. The successor map represents the expected future
state occupancy from any given state and the reward predictor maps states to
scalar rewards. The value function of a state can be computed as the inner
product between the successor map and the reward weights. In this paper, we
present DSR, which generalizes SR within an end-to-end deep reinforcement
learning framework. DSR has several appealing properties including: increased
sensitivity to distal reward changes due to factorization of reward and world
dynamics, and the ability to extract bottleneck states (subgoals) given
successor maps trained under a random policy. We show the efficacy of our
approach on two diverse environments given raw pixel observations -- simple
grid-world domains (MazeBase) and the Doom game engine.
</summary>
    <author>
      <name>Tejas D. Kulkarni</name>
    </author>
    <author>
      <name>Ardavan Saeedi</name>
    </author>
    <author>
      <name>Simanta Gautam</name>
    </author>
    <author>
      <name>Samuel J. Gershman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02382v1</id>
    <updated>2016-06-08T02:57:00Z</updated>
    <published>2016-06-08T02:57:00Z</published>
    <title>Deep Learning Convolutional Networks for Multiphoton Microscopy
  Vasculature Segmentation</title>
    <summary>  Recently there has been an increasing trend to use deep learning frameworks
for both 2D consumer images and for 3D medical images. However, there has been
little effort to use deep frameworks for volumetric vascular segmentation. We
wanted to address this by providing a freely available dataset of 12 annotated
two-photon vasculature microscopy stacks. We demonstrated the use of deep
learning framework consisting both 2D and 3D convolutional filters (ConvNet).
Our hybrid 2D-3D architecture produced promising segmentation result. We
derived the architectures from Lee et al. who used the ZNN framework initially
designed for electron microscope image segmentation. We hope that by sharing
our volumetric vasculature datasets, we will inspire other researchers to
experiment with vasculature dataset and improve the used network architectures.
</summary>
    <author>
      <name>Petteri Teikari</name>
    </author>
    <author>
      <name>Marc Santos</name>
    </author>
    <author>
      <name>Charissa Poon</name>
    </author>
    <author>
      <name>Kullervo Hynynen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1; I.5.4; I.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02378v2</id>
    <updated>2016-06-20T17:32:22Z</updated>
    <published>2016-06-08T02:36:11Z</published>
    <title>SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks</title>
    <summary>  We introduce SE3-Nets, which are deep networks designed to model rigid body
motion from raw point cloud data. Based only on pairs of depth images along
with an action vector and point wise data associations, SE3-Nets learn to
segment effected object parts and predict their motion resulting from the
applied force. Rather than learning point wise flow vectors, SE3-Nets predict
SE3 transformations for different parts of the scene. Using simulated depth
data of a table top scene and a robot manipulator, we show that the structure
underlying SE3-Nets enables them to generate a far more consistent prediction
of object motion than traditional flow based networks.
</summary>
    <author>
      <name>Arunkumar Byravan</name>
    </author>
    <author>
      <name>Dieter Fox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Under Review. V1 Update: Fixed reference mistake, Added new
  mask comparison figure, Fixed few typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02378v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02378v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02355v1</id>
    <updated>2016-06-07T23:43:42Z</updated>
    <published>2016-06-07T23:43:42Z</published>
    <title>Active Long Term Memory Networks</title>
    <summary>  Continual Learning in artificial neural networks suffers from interference
and forgetting when different tasks are learned sequentially. This paper
introduces the Active Long Term Memory Networks (A-LTM), a model of sequential
multi-task deep learning that is able to maintain previously learned
association between sensory input and behavioral output while acquiring knew
knowledge. A-LTM exploits the non-convex nature of deep neural networks and
actively maintains knowledge of previously learned, inactive tasks using a
distillation loss. Distortions of the learned input-output map are penalized
but hidden layers are free to transverse towards new local optima that are more
favorable for the multi-task objective. We re-frame the McClelland's seminal
Hippocampal theory with respect to Catastrophic Inference (CI) behavior
exhibited by modern deep architectures trained with back-propagation and
inhomogeneous sampling of latent factors across epochs. We present empirical
results of non-trivial CI during continual learning in Deep Linear Networks
trained on the same task, in Convolutional Neural Networks when the task shifts
from predicting semantic to graphical factors and during domain adaptation from
simple to complex environments. We present results of the A-LTM model's ability
to maintain viewpoint recognition learned in the highly controlled iLab-20M
dataset with 10 object categories and 88 camera viewpoints, while adapting to
the unstructured domain of Imagenet with 1,000 object categories.
</summary>
    <author>
      <name>Tommaso Furlanello</name>
    </author>
    <author>
      <name>Jiaping Zhao</name>
    </author>
    <author>
      <name>Andrew M. Saxe</name>
    </author>
    <author>
      <name>Laurent Itti</name>
    </author>
    <author>
      <name>Bosco S. Tjan</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02231v1</id>
    <updated>2016-06-07T17:44:44Z</updated>
    <published>2016-06-07T17:44:44Z</published>
    <title>Emotional Intensity analysis in Bipolar subjects</title>
    <summary>  The massive availability of digital repositories of human thought opens
radical novel way of studying the human mind. Natural language processing tools
and computational models have evolved such that many mental conditions are
predicted by analysing speech. Transcription of interviews and discourses are
analyzed using syntactic, grammatical or sentiment analysis to infer the mental
state. Here we set to investigate if classification of Bipolar and control
subjects is possible. We develop the Emotion Intensity Index based on the
Dictionary of Affect, and find that subjects categories are distinguishable.
Using classical classification techniques we get more than 75\% of labeling
performance. These results sumed to previous studies show that current
automated speech analysis is capable of identifying altered mental states
towards a quantitative psychiatry.
</summary>
    <author>
      <name>Facundo Carrillo</name>
    </author>
    <author>
      <name>Natalia Mota</name>
    </author>
    <author>
      <name>Mauro Copelli</name>
    </author>
    <author>
      <name>Sidarta Ribeiro</name>
    </author>
    <author>
      <name>Mariano Sigman</name>
    </author>
    <author>
      <name>Guillermo Cecchi</name>
    </author>
    <author>
      <name>Diego Fernandez Slezak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at MLINI-2015 workshop, 2015 (arXiv:cs/0101200)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02221v1</id>
    <updated>2016-06-07T17:10:16Z</updated>
    <published>2016-06-07T17:10:16Z</published>
    <title>Multi-resource defensive strategies for patrolling games with alarm
  systems</title>
    <summary>  Security Games employ game theoretical tools to derive resource allocation
strategies in security domains. Recent works considered the presence of alarm
systems, even suffering various forms of uncertainty, and showed that
disregarding alarm signals may lead to arbitrarily bad strategies. The central
problem with an alarm system, unexplored in other Security Games, is finding
the best strategy to respond to alarm signals for each mobile defensive
resource. The literature provides results for the basic single-resource case,
showing that even in that case the problem is computationally hard. In this
paper, we focus on the challenging problem of designing algorithms scaling with
multiple resources. First, we focus on finding the minimum number of resources
assuring non-null protection to every target. Then, we deal with the
computation of multi-resource strategies with different degrees of coordination
among resources. For each considered problem, we provide a computational
analysis and propose algorithmic methods.
</summary>
    <author>
      <name>Nicola Basilico</name>
    </author>
    <author>
      <name>Giuseppe De Nittis</name>
    </author>
    <author>
      <name>Nicola Gatti</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02096v1</id>
    <updated>2016-06-07T11:07:56Z</updated>
    <published>2016-06-07T11:07:56Z</published>
    <title>Towards Playlist Generation Algorithms Using RNNs Trained on
  Within-Track Transitions</title>
    <summary>  We introduce a novel playlist generation algorithm that focuses on the
quality of transitions using a recurrent neural network (RNN). The proposed
model assumes that optimal transitions between tracks can be modelled and
predicted by internal transitions within music tracks. We introduce modelling
sequences of high-level music descriptors using RNNs and discuss an experiment
involving different similarity functions, where the sequences are provided by a
musical structural analysis algorithm. Qualitative observations show that the
proposed approach can effectively model transitions of music tracks in
playlists.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, accepted to Workshop on Surprise, Opposition, and
  Obstruction in Adaptive and Personalized Systems (SOAP) 2016, Halifax, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02041v1</id>
    <updated>2016-06-07T06:55:42Z</updated>
    <published>2016-06-07T06:55:42Z</published>
    <title>Sorting out symptoms: design and evaluation of the 'babylon check'
  automated triage system</title>
    <summary>  Prior to seeking professional medical care it is increasingly common for
patients to use online resources such as automated symptom checkers. Many such
systems attempt to provide a differential diagnosis based on the symptoms
elucidated from the user, which may lead to anxiety if life or limb-threatening
conditions are part of the list, a phenomenon termed 'cyberchondria' [1].
Systems that provide advice on where to seek help, rather than a diagnosis, are
equally popular, and in our view provide the most useful information. In this
technical report we describe how such a triage system can be modelled
computationally, how medical insights can be translated into triage flows, and
how such systems can be validated and tested. We present babylon check, our
commercially deployed automated triage system, as a case study, and illustrate
its performance in a large, semi-naturalistic deployment study.
</summary>
    <author>
      <name>Katherine Middleton</name>
    </author>
    <author>
      <name>Mobasher Butt</name>
    </author>
    <author>
      <name>Nils Hammerla</name>
    </author>
    <author>
      <name>Steven Hamblin</name>
    </author>
    <author>
      <name>Karan Mehta</name>
    </author>
    <author>
      <name>Ali Parsa</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02032v1</id>
    <updated>2016-06-07T05:13:37Z</updated>
    <published>2016-06-07T05:13:37Z</published>
    <title>Human vs. Computer Go: Review and Prospect</title>
    <summary>  The Google DeepMind challenge match in March 2016 was a historic achievement
for computer Go development. This article discusses the development of
computational intelligence (CI) and its relative strength in comparison with
human intelligence for the game of Go. We first summarize the milestones
achieved for computer Go from 1998 to 2016. Then, the computer Go programs that
have participated in previous IEEE CIS competitions as well as methods and
techniques used in AlphaGo are briefly introduced. Commentaries from three
high-level professional Go players on the five AlphaGo versus Lee Sedol games
are also included. We conclude that AlphaGo beating Lee Sedol is a huge
achievement in artificial intelligence (AI) based largely on CI methods. In the
future, powerful computer Go programs such as AlphaGo are expected to be
instrumental in promoting Go education and AI real-world applications.
</summary>
    <author>
      <name>Chang-Shing Lee</name>
    </author>
    <author>
      <name>Mei-Hui Wang</name>
    </author>
    <author>
      <name>Shi-Jim Yen</name>
    </author>
    <author>
      <name>Ting-Han Wei</name>
    </author>
    <author>
      <name>I-Chen Wu</name>
    </author>
    <author>
      <name>Ping-Chiang Chou</name>
    </author>
    <author>
      <name>Chun-Hsun Chou</name>
    </author>
    <author>
      <name>Ming-Wan Wang</name>
    </author>
    <author>
      <name>Tai-Hsiung Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is with 6 pages and 3 figures. And, it is accepted and
  will be published in IEEE Computational Intelligence Magazine in August, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01949v1</id>
    <updated>2016-06-06T21:36:02Z</updated>
    <published>2016-06-06T21:36:02Z</published>
    <title>Assisted Energy Management in Smart Microgrids</title>
    <summary>  Demand response provides utilities with a mechanism to share with end users
the stochasticity resulting from the use of renewable sources. Pricing is
accordingly used to reflect energy availability, to allocate such a limited
resource to those loads that value it most. However, the strictly competitive
mechanism can result in service interruption in presence of competing demand.
To solve this issue we investigate on the use of forward contracts, i.e.,
service level agreements priced to reflect the expectation of future supply and
demand curves. Given the limited resources of microgrids, service interruption
is an opposite objective to the one of service availability. We firstly design
policy-based brokers and identify then a learning broker based on artificial
neural networks. We show the latter being progressively minimizing the
reimbursement costs and maximizing the overall profit.
</summary>
    <author>
      <name>Andrea Monacchi</name>
    </author>
    <author>
      <name>Wilfried Elmenreich</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01930v1</id>
    <updated>2016-06-06T20:26:42Z</updated>
    <published>2016-06-06T20:26:42Z</published>
    <title>Consistency and Trust in Peer Data Exchange Systems</title>
    <summary>  We propose and investigate a semantics for "peer data exchange systems" where
different peers are related by data exchange constraints and trust
relationships. These two elements plus the data at the peers' sites and their
local integrity constraints are made compatible via a semantics that
characterizes sets of "solution instances" for the peers. They are the intended
-possibly virtual- instances for a peer that are obtained through a data repair
semantics that we introduce and investigate. The semantically correct answers
from a peer to a query, the so-called "peer consistent answers", are defined as
those answers that are invariant under all its different solution instances. We
show that solution instances can be specified as the models of logic programs
with a stable model semantics. The repair semantics is based on null values as
used in SQL databases, and is also of independent interest for repairs of
single databases with respect to integrity constraints.
</summary>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <author>
      <name>Loreto Bravo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/S147106841600017X</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/S147106841600017X" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Theory and Practice of Logic Programming (TPLP). It
  includes appendix that will be published only in electronic format</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01885v1</id>
    <updated>2016-06-06T19:50:47Z</updated>
    <published>2016-06-06T19:50:47Z</published>
    <title>Learning to Optimize</title>
    <summary>  Algorithm design is a laborious process and often requires many iterations of
ideation and validation. In this paper, we explore automating algorithm design
and present a method to learn an optimization algorithm, which we believe to be
the first method that can automatically discover a better algorithm. We
approach this problem from a reinforcement learning perspective and represent
any particular optimization algorithm as a policy. We learn an optimization
algorithm using guided policy search and demonstrate that the resulting
algorithm outperforms existing hand-engineered algorithms in terms of
convergence speed and/or the final objective value.
</summary>
    <author>
      <name>Ke Li</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01868v1</id>
    <updated>2016-06-06T19:21:32Z</updated>
    <published>2016-06-06T19:21:32Z</published>
    <title>Unifying Count-Based Exploration and Intrinsic Motivation</title>
    <summary>  We consider an agent's uncertainty about its environment and the problem of
generalizing this uncertainty across observations. Specifically, we focus on
the problem of exploration in non-tabular reinforcement learning. Drawing
inspiration from the intrinsic motivation literature, we use sequential density
models to measure uncertainty, and propose a novel algorithm for deriving a
pseudo-count from an arbitrary sequential density model. This technique enables
us to generalize count-based exploration algorithms to the non-tabular case. We
apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw
pixels. We transform these pseudo-counts into intrinsic rewards and obtain
significantly improved exploration in a number of hard games, including the
infamously difficult Montezuma's Revenge.
</summary>
    <author>
      <name>Marc G. Bellemare</name>
    </author>
    <author>
      <name>Sriram Srinivasan</name>
    </author>
    <author>
      <name>Georg Ostrovski</name>
    </author>
    <author>
      <name>Tom Schaul</name>
    </author>
    <author>
      <name>David Saxton</name>
    </author>
    <author>
      <name>Remi Munos</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01855v1</id>
    <updated>2016-06-06T18:34:56Z</updated>
    <published>2016-06-06T18:34:56Z</published>
    <title>Bayesian Poisson Tucker Decomposition for Learning the Structure of
  International Relations</title>
    <summary>  We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling
country--country interaction event data. These data consist of interaction
events of the form "country $i$ took action $a$ toward country $j$ at time
$t$." BPTD discovers overlapping country--community memberships, including the
number of latent communities. In addition, it discovers directed
community--community interaction networks that are specific to "topics" of
action types and temporal "regimes." We show that BPTD yields an efficient MCMC
inference algorithm and achieves better predictive performance than related
models. We also demonstrate that it discovers interpretable latent structure
that agrees with our knowledge of international relations.
</summary>
    <author>
      <name>Aaron Schein</name>
    </author>
    <author>
      <name>Mingyuan Zhou</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <author>
      <name>Hanna Wallach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 33rd International Conference on
  Machine Learning (ICML 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01847v2</id>
    <updated>2016-06-23T19:52:41Z</updated>
    <published>2016-06-06T17:59:56Z</published>
    <title>Multimodal Compact Bilinear Pooling for Visual Question Answering and
  Visual Grounding</title>
    <summary>  Modeling textual or visual information with vector representations trained
from large language or visual datasets has been successfully explored in recent
years. However, tasks such as visual question answering require combining these
vector representations with each other. Approaches to multimodal pooling
include element-wise multiplication or addition, as well as concatenation of
the visual and textual representations. We hypothesize that these methods are
not as expressive as an outer product of the visual and textual vectors. As the
outer product is typically infeasible due to its high dimensionality, we
instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to
efficiently and expressively combine multimodal features. We extensively
evaluate MCB on the visual question answering and grounding tasks. We
consistently show the benefit of MCB over ablations without MCB. For visual
question answering, we present an architecture which uses MCB twice, once for
predicting attention over spatial features and again to combine the attended
representation with the question representation. This model outperforms the
state-of-the-art on the Visual7W dataset and the VQA challenge.
</summary>
    <author>
      <name>Akira Fukui</name>
    </author>
    <author>
      <name>Dong Huk Park</name>
    </author>
    <author>
      <name>Daylen Yang</name>
    </author>
    <author>
      <name>Anna Rohrbach</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added qualitative and quantitative results</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01847v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01847v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01623v2</id>
    <updated>2016-06-10T22:39:37Z</updated>
    <published>2016-06-06T06:16:36Z</published>
    <title>Position-Indexed Formulations for Kidney Exchange</title>
    <summary>  A kidney exchange is an organized barter market where patients in need of a
kidney swap willing but incompatible donors. Determining an optimal set of
exchanges is theoretically and empirically hard. Traditionally, exchanges took
place in cycles, with each participating patient-donor pair both giving and
receiving a kidney. The recent introduction of chains, where a donor without a
paired patient triggers a sequence of donations without requiring a kidney in
return, increased the efficacy of fielded kidney exchanges---while also
dramatically raising the empirical computational hardness of clearing the
market in practice. While chains can be quite long, unbounded-length chains are
not desirable: planned donations can fail before transplant for a variety of
reasons, and the failure of a single donation causes the rest of that chain to
fail, so parallel shorter chains are better in practice.
  In this paper, we address the tractable clearing of kidney exchanges with
short cycles and chains that are long but bounded. This corresponds to the
practice at most modern fielded kidney exchanges. We introduce three new
integer programming formulations, two of which are compact. Furthermore, one of
these models has a linear programming relaxation that is exactly as tight as
the previous tightest formulation (which was not compact) for instances in
which each donor has a paired patient. On real data from the UNOS nationwide
exchange in the United States and the NLDKSS nationwide exchange in the United
Kingdom, as well as on generated realistic large-scale data, we show that our
new models are competitive with all existing solvers---in many cases
outperforming all other solvers by orders of magnitude.
</summary>
    <author>
      <name>John P. Dickerson</name>
    </author>
    <author>
      <name>David F. Manlove</name>
    </author>
    <author>
      <name>Benjamin Plaut</name>
    </author>
    <author>
      <name>Tuomas Sandholm</name>
    </author>
    <author>
      <name>James Trimble</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared at the ACM Conference on Economics and Computation (EC-16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01623v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01623v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01540v1</id>
    <updated>2016-06-05T17:54:48Z</updated>
    <published>2016-06-05T17:54:48Z</published>
    <title>OpenAI Gym</title>
    <summary>  OpenAI Gym is a toolkit for reinforcement learning research. It includes a
growing collection of benchmark problems that expose a common interface, and a
website where people can share their results and compare the performance of
algorithms. This whitepaper discusses the components of OpenAI Gym and the
design decisions that went into the software.
</summary>
    <author>
      <name>Greg Brockman</name>
    </author>
    <author>
      <name>Vicki Cheung</name>
    </author>
    <author>
      <name>Ludwig Pettersson</name>
    </author>
    <author>
      <name>Jonas Schneider</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01515v2</id>
    <updated>2016-08-04T00:41:00Z</updated>
    <published>2016-06-05T14:26:56Z</published>
    <title>Coordination in Categorical Compositional Distributional Semantics</title>
    <summary>  An open problem with categorical compositional distributional semantics is
the representation of words that are considered semantically vacuous from a
distributional perspective, such as determiners, prepositions, relative
pronouns or coordinators. This paper deals with the topic of coordination
between identical syntactic types, which accounts for the majority of
coordination cases in language. By exploiting the compact closed structure of
the underlying category and Frobenius operators canonically induced over the
fixed basis of finite-dimensional vector spaces, we provide a morphism as
representation of a coordinator tensor, and we show how it lifts from atomic
types to compound types. Linguistic intuitions are provided, and the importance
of the Frobenius operators as an addition to the compact closed setting with
regard to language is discussed.
</summary>
    <author>
      <name>Dimitri Kartsaklis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Queen Mary University of London</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.221.4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.221.4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SLPCS 2016, arXiv:1608.01018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 221, 2016, pp. 29-38</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.01515v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01515v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01412v1</id>
    <updated>2016-06-04T20:06:03Z</updated>
    <published>2016-06-04T20:06:03Z</published>
    <title>Distance Metric Ensemble Learning and the Andrews-Curtis Conjecture</title>
    <summary>  Motivated by the search for a counterexample to the Poincar\'e conjecture in
three and four dimensions, the Andrews-Curtis conjecture was proposed in 1965.
It is now generally suspected that the Andrews-Curtis conjecture is false, but
small potential counterexamples are not so numerous, and previous work has
attempted to eliminate some via combinatorial search. Progress has however been
limited, with the most successful approach (breadth-first-search using
secondary storage) being neither scalable nor heuristically-informed. A
previous empirical analysis of problem structure examined several heuristic
measures of search progress and determined that none of them provided any
useful guidance for search. In this article, we induce new quality measures
directly from the problem structure and combine them to produce a more
effective search driver via ensemble machine learning. By this means, we
eliminate 19 potential counterexamples, the status of which had been unknown
for some years.
</summary>
    <author>
      <name>Krzysztof Krawiec</name>
    </author>
    <author>
      <name>Jerry Swan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01404v1</id>
    <updated>2016-06-04T18:34:51Z</updated>
    <published>2016-06-04T18:34:51Z</published>
    <title>Generating Natural Language Inference Chains</title>
    <summary>  The ability to reason with natural language is a fundamental prerequisite for
many NLP tasks such as information extraction, machine translation and question
answering. To quantify this ability, systems are commonly tested whether they
can recognize textual entailment, i.e., whether one sentence can be inferred
from another one. However, in most NLP applications only single source
sentences instead of sentence pairs are available. Hence, we propose a new task
that measures how well a model can generate an entailed sentence from a source
sentence. We take entailment-pairs of the Stanford Natural Language Inference
corpus and train an LSTM with attention. On a manually annotated test set we
found that 82% of generated sentences are correct, an improvement of 10.3% over
an LSTM baseline. A qualitative analysis shows that this model is not only
capable of shortening input sentences, but also inferring new statements via
paraphrasing and phrase entailment. We then apply this model recursively to
input-output pairs, thereby generating natural language inference chains that
can be used to automatically construct an entailment graph from source
sentences. Finally, by swapping source and target sentences we can also train a
model that given an input sentence invents additional information to generate a
new sentence.
</summary>
    <author>
      <name>Vladyslav Kolesnyk</name>
    </author>
    <author>
      <name>Tim Rocktäschel</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01380v1</id>
    <updated>2016-06-04T14:12:32Z</updated>
    <published>2016-06-04T14:12:32Z</published>
    <title>Effective Multi-Robot Spatial Task Allocation using Model Approximations</title>
    <summary>  Real-world multi-agent planning problems cannot be solved using
decision-theoretic planning methods due to the exponential complexity. We
approximate firefighting in rescue simulation as a spatially distributed task
and model with multi-agent Markov decision process. We use recent approximation
methods for spatial task problems to reduce the model complexity. Our
approximations are single-agent, static task, shortest path pruning, dynamic
planning horizon, and task clustering. We create scenarios from RoboCup Rescue
Simulation maps and evaluate our methods on these graph worlds. The results
show that our approach is faster and better than comparable methods and has
negligible performance loss compared to the optimal policy. We also show that
our method has a similar performance as DCOP methods on example RCRS scenarios.
</summary>
    <author>
      <name>Okan Aşık</name>
    </author>
    <author>
      <name>H. Levent Akın</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RoboCup 2016 Symposium</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01245v1</id>
    <updated>2016-06-04T03:28:41Z</updated>
    <published>2016-06-04T03:28:41Z</published>
    <title>Scalable Algorithms for Tractable Schatten Quasi-Norm Minimization</title>
    <summary>  The Schatten-p quasi-norm $(0&lt;p&lt;1)$ is usually used to replace the standard
nuclear norm in order to approximate the rank function more accurately.
However, existing Schatten-p quasi-norm minimization algorithms involve
singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each
iteration, and thus may become very slow and impractical for large-scale
problems. In this paper, we first define two tractable Schatten quasi-norms,
i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove
that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively,
which lead to the design of very efficient algorithms that only need to update
two much smaller factor matrices. We also design two efficient proximal
alternating linearized minimization algorithms for solving representative
matrix completion problems. Finally, we provide the global convergence and
performance guarantees for our algorithms, which have better convergence
properties than existing algorithms. Experimental results on synthetic and
real-world data show that our algorithms are more accurate than the
state-of-the-art methods, and are orders of magnitude faster.
</summary>
    <author>
      <name>Fanhua Shang</name>
    </author>
    <author>
      <name>Yuanyuan Liu</name>
    </author>
    <author>
      <name>James Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 5 figures, Appears in Proceedings of the 30th AAAI
  Conference on Artificial Intelligence (AAAI), Phoenix, Arizona, USA, pp.
  2016--2022, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01307v1</id>
    <updated>2016-06-03T23:49:02Z</updated>
    <published>2016-06-03T23:49:02Z</published>
    <title>Scene Grammars, Factor Graphs, and Belief Propagation</title>
    <summary>  We consider a class of probabilistic grammars for generating scenes with
multiple objects. Probabilistic scene grammars capture relationships between
objects using compositional rules that provide important contextual cues for
inference with ambiguous data. We show how to represent the distribution
defined by a probabilistic scene grammar using a factor graph. We also show how
to efficiently perform message passing in this factor graph. This leads to an
efficient approach for inference with a grammar model using belief propagation
as the underlying computational engine. Inference with belief propagation
naturally combines bottom-up and top-down contextual information and leads to a
robust algorithm for aggregating evidence. We show experiments on two different
applications to demonstrate the generality of the framework. The first
application involves detecting curves in noisy images, and we address this
problem using a grammar that generates a collection of curves using a
first-order Markov process. The second application involves localizing faces
and parts of faces in images. In this case, we use a grammar that captures
spatial relationships between the parts of a face. In both applications the
same framework leads to robust inference algorithms that can effectively
combine weak local information to reason about a scene.
</summary>
    <author>
      <name>Jeroen Chua</name>
    </author>
    <author>
      <name>Pedro F. Felzenszwalb</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01269v1</id>
    <updated>2016-06-03T20:32:52Z</updated>
    <published>2016-06-03T20:32:52Z</published>
    <title>End-to-end LSTM-based dialog control optimized with supervised and
  reinforcement learning</title>
    <summary>  This paper presents a model for end-to-end learning of task-oriented dialog
systems. The main component of the model is a recurrent neural network (an
LSTM), which maps from raw dialog history directly to a distribution over
system actions. The LSTM automatically infers a representation of dialog
history, which relieves the system developer of much of the manual feature
engineering of dialog state. In addition, the developer can provide software
that expresses business rules and provides access to programmatic APIs,
enabling the LSTM to take actions in the real world on behalf of the user. The
LSTM can be optimized using supervised learning (SL), where a domain expert
provides example dialogs which the LSTM should imitate; or using reinforcement
learning (RL), where the system improves by interacting directly with end
users. Experiments show that SL and RL are complementary: SL alone can derive a
reasonable initial policy from a small number of training dialogs; and starting
RL optimization with a policy trained with SL substantially accelerates the
learning rate of RL.
</summary>
    <author>
      <name>Jason D. Williams</name>
    </author>
    <author>
      <name>Geoffrey Zweig</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01116v1</id>
    <updated>2016-06-03T14:47:12Z</updated>
    <published>2016-06-03T14:47:12Z</published>
    <title>The belief noisy-or model applied to network reliability analysis</title>
    <summary>  One difficulty faced in knowledge engineering for Bayesian Network (BN) is
the quan-tification step where the Conditional Probability Tables (CPTs) are
determined. The number of parameters included in CPTs increases exponentially
with the number of parent variables. The most common solution is the
application of the so-called canonical gates. The Noisy-OR (NOR) gate, which
takes advantage of the independence of causal interactions, provides a
logarithmic reduction of the number of parameters required to specify a CPT. In
this paper, an extension of NOR model based on the theory of belief functions,
named Belief Noisy-OR (BNOR), is proposed. BNOR is capable of dealing with both
aleatory and epistemic uncertainty of the network. Compared with NOR, more rich
information which is of great value for making decisions can be got when the
available knowledge is uncertain. Specially, when there is no epistemic
uncertainty, BNOR degrades into NOR. Additionally, different structures of BNOR
are presented in this paper in order to meet various needs of engineers. The
application of BNOR model on the reliability evaluation problem of networked
systems demonstrates its effectiveness.
</summary>
    <author>
      <name>Kuang Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DRUID</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DRUID</arxiv:affiliation>
    </author>
    <author>
      <name>Quan Pan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Uncertainty, Fuzziness and
  Knowledge-Based Systems, World Scientific Publishing, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.01116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01113v1</id>
    <updated>2016-06-03T14:44:15Z</updated>
    <published>2016-06-03T14:44:15Z</published>
    <title>ECMdd: Evidential c-medoids clustering with multiple prototypes</title>
    <summary>  In this work, a new prototype-based clustering method named Evidential
C-Medoids (ECMdd), which belongs to the family of medoid-based clustering for
proximity data, is proposed as an extension of Fuzzy C-Medoids (FCMdd) on the
theoretical framework of belief functions. In the application of FCMdd and
original ECMdd, a single medoid (prototype), which is supposed to belong to the
object set, is utilized to represent one class. For the sake of clarity, this
kind of ECMdd using a single medoid is denoted by sECMdd. In real clustering
applications, using only one pattern to capture or interpret a class may not
adequately model different types of group structure and hence limits the
clustering performance. In order to address this problem, a variation of ECMdd
using multiple weighted medoids, denoted by wECMdd, is presented. Unlike
sECMdd, in wECMdd objects in each cluster carry various weights describing
their degree of representativeness for that class. This mechanism enables each
class to be represented by more than one object. Experimental results in
synthetic and real data sets clearly demonstrate the superiority of sECMdd and
wECMdd. Moreover, the clustering results by wECMdd can provide richer
information for the inner structure of the detected classes with the help of
prototype weights.
</summary>
    <author>
      <name>Kuang Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DRUID</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DRUID</arxiv:affiliation>
    </author>
    <author>
      <name>Quan Pan</name>
    </author>
    <author>
      <name>Zhun-Ga Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patcog.2016.05.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patcog.2016.05.005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition, Elsevier, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.01113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01015v1</id>
    <updated>2016-06-03T09:31:13Z</updated>
    <published>2016-06-03T09:31:13Z</published>
    <title>Selecting the Best Player Formation for Corner-Kick Situations Based on
  Bayes' Estimation</title>
    <summary>  In the domain of the Soccer simulation 2D league of the RoboCup project,
appropriate player positioning against a given opponent team is an important
factor of soccer team performance. This work proposes a model which decides the
strategy that should be applied regarding a particular opponent team. This task
can be realized by applying preliminary a learning phase where the model
determines the most effective strategies against clusters of opponent teams.
The model determines the best strategies by using sequential Bayes' estimators.
As a first trial of the system, the proposed model is used to determine the
association of player formations against opponent teams in the particular
situation of corner-kick. The implemented model shows satisfying abilities to
compare player formations that are similar to each other in terms of
performance and determines the right ranking even by running a decent number of
simulation games.
</summary>
    <author>
      <name>Jordan Henrio</name>
    </author>
    <author>
      <name>Thomas Henn</name>
    </author>
    <author>
      <name>Tomoharu Nakashima</name>
    </author>
    <author>
      <name>Hidehisa Akiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, RoboCup Symposium 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00979v1</id>
    <updated>2016-06-03T06:40:14Z</updated>
    <published>2016-06-03T06:40:14Z</published>
    <title>Question Answering over Knowledge Base with Neural Attention Combining
  Global Knowledge Information</title>
    <summary>  With the rapid growth of knowledge bases (KBs) on the web, how to take full
advantage of them becomes increasingly important. Knowledge base-based question
answering (KB-QA) is one of the most promising approaches to access the
substantial knowledge. Meantime, as the neural network-based (NN-based) methods
develop, NN-based KB-QA has already achieved impressive results. However,
previous work did not put emphasis on question representation, and the question
is converted into a fixed vector regardless of its candidate answers. This
simple representation strategy is unable to express the proper information of
the question. Hence, we present a neural attention-based model to represent the
questions dynamically according to the different focuses of various candidate
answer aspects. In addition, we leverage the global knowledge inside the
underlying KB, aiming at integrating the rich KB information into the
representation of the answers. And it also alleviates the out of vocabulary
(OOV) problem, which helps the attention model to represent the question more
precisely. The experimental results on WEBQUESTIONS demonstrate the
effectiveness of the proposed approach.
</summary>
    <author>
      <name>Yuanzhe Zhang</name>
    </author>
    <author>
      <name>Kang Liu</name>
    </author>
    <author>
      <name>Shizhu He</name>
    </author>
    <author>
      <name>Guoliang Ji</name>
    </author>
    <author>
      <name>Zhanyi Liu</name>
    </author>
    <author>
      <name>Hua Wu</name>
    </author>
    <author>
      <name>Jun Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00927v1</id>
    <updated>2016-06-02T22:47:57Z</updated>
    <published>2016-06-02T22:47:57Z</published>
    <title>An interactive fuzzy goal programming algorithm to solve decentralized
  bi-level multiobjective fractional programming problem</title>
    <summary>  This paper proposes a fuzzy goal programming based on Taylor series for
solving decentralized bi-level multiobjective fractional programming (DBLMOFP)
problem. In the proposed approach, all of the membership functions are
associated with the fuzzy goals of each objective at the both levels and also
the fractional membership functions are converted to linear functions using the
Taylor series approach. Then a fuzzy goal programming is proposed to reach the
highest degree of each of the membership goals by taking the most satisfactory
solution for all decision makers at the both levels. Finally, a numerical
example is presented to illustrate the effectiveness of the proposed approach.
</summary>
    <author>
      <name>Hasan Dalman</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q17, 68Q10, 68W30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00917v1</id>
    <updated>2016-06-02T22:01:50Z</updated>
    <published>2016-06-02T22:01:50Z</published>
    <title>Towards a Job Title Classification System</title>
    <summary>  Document classification for text, images and other applicable entities has
long been a focus of research in academia and also finds application in many
industrial settings. Amidst a plethora of approaches to solve such problems,
machine-learning techniques have found success in a variety of scenarios. In
this paper we discuss the design of a machine learning-based semi-supervised
job title classification system for the online job recruitment domain currently
in production at CareerBuilder.com and propose enhancements to it. The system
leverages a varied collection of classification as well clustering algorithms.
These algorithms are encompassed in an architecture that facilitates leveraging
existing off-the-shelf machine learning tools and techniques while keeping into
consideration the challenges of constructing a scalable classification system
for a large taxonomy of categories. As a continuously evolving system that is
still under development we first discuss the existing semi-supervised
classification system which is composed of both clustering and classification
components in a proximity-based classifier setup and results of which are
already used across numerous products at CareerBuilder. We then elucidate our
long-term goals for job title classification and propose enhancements to the
existing system in the form of a two-stage coarse and fine level classifier
augmentation to construct a cascade of hierarchical vertical classifiers.
Preliminary results are presented using experimental evaluation on real world
industrial data.
</summary>
    <author>
      <name>Faizan Javed</name>
    </author>
    <author>
      <name>Matt McNair</name>
    </author>
    <author>
      <name>Ferosh Jacob</name>
    </author>
    <author>
      <name>Meng Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01924v2</id>
    <updated>2016-06-23T00:38:21Z</updated>
    <published>2016-06-02T19:37:14Z</published>
    <title>Preliminaries of a Space Situational Awareness Ontology</title>
    <summary>  Space situational awareness (SSA) is vital for international safety and
security, and the future of space travel. By improving SSA data-sharing we
improve global SSA. Computational ontology may provide one means toward that
goal. This paper develops the ontology of the SSA domain and takes steps in the
creation of the space situational awareness ontology. Ontology objectives,
requirements and desiderata are outlined; and both the SSA domain and the
discipline of ontology are described. The purposes of the ontology include:
exploring the potential for ontology development and engineering to (i)
represent SSA data, general domain knowledge, objects and relationships (ii)
annotate and express the meaning of that data, and (iii) foster SSA
data-exchange and integration among SSA actors, orbital debris databases, space
object catalogs and other SSA data repositories. By improving SSA via data- and
knowledge-sharing, we can (iv) expand our scientific knowledge of the space
environment, (v) advance our capacity for planetary defense from near-Earth
objects, and (vi) ensure the future of safe space flight for generations to
come.
</summary>
    <author>
      <name>Robert John Rovetto</name>
    </author>
    <author>
      <name>T. S. Kelso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at 26th AIAA/AAS Space Flight Mechanics meeting,
  Napa, CA. USA, February 14-18, 2016. Forthcoming in Advances in the
  Astronautics, Univelt</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.01924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00787v1</id>
    <updated>2016-06-02T18:20:35Z</updated>
    <published>2016-06-02T18:20:35Z</published>
    <title>Prior Swapping for Data-Independent Inference</title>
    <summary>  While Bayesian methods are praised for their ability to incorporate useful
prior knowledge, in practice, priors that allow for computationally convenient
or tractable inference are more commonly used. In this paper, we investigate
the following question: for a given model, is it possible to use any convenient
prior to infer a false posterior, and afterwards, given some true prior of
interest, quickly transform this result into the true posterior?
  We present a procedure to carry out this task: given an inferred false
posterior and true prior, our algorithm generates samples from the true
posterior. This transformation procedure, which we call "prior swapping" works
for arbitrary priors. Notably, its cost is independent of data size. It
therefore allows us, in some cases, to apply significantly less-costly
inference procedures to more-sophisticated models than previously possible. It
also lets us quickly perform any additional inferences, such as with updated
priors or for many different hyperparameter settings, without touching the
data. We prove that our method can generate asymptotically exact samples, and
demonstrate it empirically on a number of models and priors.
</summary>
    <author>
      <name>Willie Neiswanger</name>
    </author>
    <author>
      <name>Eric Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00776v2</id>
    <updated>2016-06-14T02:01:16Z</updated>
    <published>2016-06-02T17:37:31Z</published>
    <title>Multiresolution Recurrent Neural Networks: An Application to Dialogue
  Response Generation</title>
    <summary>  We introduce the multiresolution recurrent neural network, which extends the
sequence-to-sequence framework to model natural language generation as two
parallel discrete stochastic processes: a sequence of high-level coarse tokens,
and a sequence of natural language tokens. There are many ways to estimate or
learn the high-level coarse tokens, but we argue that a simple extraction
procedure is sufficient to capture a wealth of high-level discourse semantics.
Such procedure allows training the multiresolution recurrent neural network by
maximizing the exact joint log-likelihood over both sequences. In contrast to
the standard log- likelihood objective w.r.t. natural language tokens (word
perplexity), optimizing the joint log-likelihood biases the model towards
modeling high-level abstractions. We apply the proposed model to the task of
dialogue response generation in two challenging domains: the Ubuntu technical
support domain, and Twitter conversations. On Ubuntu, the model outperforms
competing approaches by a substantial margin, achieving state-of-the-art
results according to both automatic evaluation metrics and a human evaluation
study. On Twitter, the model appears to generate more relevant and on-topic
responses according to automatic evaluation metrics. Finally, our experiments
demonstrate that the proposed model is more adept at overcoming the sparsity of
natural language and is better able to capture long-term structure.
</summary>
    <author>
      <name>Iulian Vlad Serban</name>
    </author>
    <author>
      <name>Tim Klinger</name>
    </author>
    <author>
      <name>Gerald Tesauro</name>
    </author>
    <author>
      <name>Kartik Talamadupula</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 2 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00776v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00776v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00652v1</id>
    <updated>2016-06-02T12:48:39Z</updated>
    <published>2016-06-02T12:48:39Z</published>
    <title>Death and Suicide in Universal Artificial Intelligence</title>
    <summary>  Reinforcement learning (RL) is a general paradigm for studying intelligent
behaviour, with applications ranging from artificial intelligence to psychology
and economics. AIXI is a universal solution to the RL problem; it can learn any
computable environment. A technical subtlety of AIXI is that it is defined
using a mixture over semimeasures that need not sum to 1, rather than over
proper probability measures. In this work we argue that the shortfall of a
semimeasure can naturally be interpreted as the agent's estimate of the
probability of its death. We formally define death for generally intelligent
agents like AIXI, and prove a number of related theorems about their behaviour.
Notable discoveries include that agent behaviour can change radically under
positive linear transformations of the reward signal (from suicidal to
dogmatically self-preserving), and that the agent's posterior belief that it
will survive increases over time.
</summary>
    <author>
      <name>Jarryd Martin</name>
    </author>
    <author>
      <name>Tom Everitt</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference: Artificial General Intelligence (AGI) 2016 13 pages, 2
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00626v1</id>
    <updated>2016-06-02T11:14:47Z</updated>
    <published>2016-06-02T11:14:47Z</published>
    <title>The Challenge of Non-Technical Loss Detection using Artificial
  Intelligence: A Survey</title>
    <summary>  Detection of non-technical losses (NTL) which include electricity theft,
faulty meters or billing errors has attracted increasing attention from
researchers in electrical engineering and computer science. NTLs cause
significant harm to the economy, as in some countries they may range up to 40%
of the total electricity distributed. The predominant research direction is
employing artificial intelligence (AI) to solve this problem. Promising
approaches have been reported falling into two categories: expert systems
incorporating hand-crafted expert knowledge or machine learning, also called
pattern recognition or data mining, which learns fraudulent consumption
patterns from examples without being explicitly programmed. This paper first
provides an overview about how NTLs are defined and their impact on economies.
Next, it covers the fundamental pillars of AI relevant to this domain. It then
surveys these research efforts in a comprehensive review of algorithms,
features and data sets used. It finally identifies the key scientific and
engineering challenges in NTL detection and suggests how they could be solved.
We believe that those challenges have not sufficiently been addressed in past
contributions and that covering those is necessary in order to advance NTL
detection.
</summary>
    <author>
      <name>Patrick Glauner</name>
    </author>
    <author>
      <name>Andre Boechat</name>
    </author>
    <author>
      <name>Lautaro Dolberg</name>
    </author>
    <author>
      <name>Jorge Meira</name>
    </author>
    <author>
      <name>Radu State</name>
    </author>
    <author>
      <name>Franck Bettinger</name>
    </author>
    <author>
      <name>Yves Rangoni</name>
    </author>
    <author>
      <name>Diogo Duarte</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00561v1</id>
    <updated>2016-06-02T07:08:01Z</updated>
    <published>2016-06-02T07:08:01Z</published>
    <title>Mining Software Components from Object-Oriented APIs</title>
    <summary>  Object-oriented Application Programing Interfaces (APIs) support software
reuse by providing pre-implemented functionalities. Due to the huge number of
included classes, reusing and understanding large APIs is a complex task.
Otherwise, software components are admitted to be more reusable and
understandable entities than object-oriented ones. Thus, in this paper, we
propose an approach for reengineering object-oriented APIs into component-based
ones. We mine components as a group of classes based on the frequency they are
used together and their ability to form a quality-centric component. To
validate our approach, we experimented on 100 Java applications that used
Android APIs.
</summary>
    <author>
      <name>Anas Shatnawi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MAREL</arxiv:affiliation>
    </author>
    <author>
      <name>Abdelhak Seriai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MAREL</arxiv:affiliation>
    </author>
    <author>
      <name>Houari Sahraoui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GEODES</arxiv:affiliation>
    </author>
    <author>
      <name>Zakarea Al-Shara</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MAREL</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-14130-5_23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-14130-5_23" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Software Reuse, Jan 2015, Miami, FL,
  United States. SpringerLink, Lecture Notes in Computer Science (8919),
  pp.330-347, 2014, Software Reuse for Dynamic Systems in the Cloud and Beyond</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.00561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00496v1</id>
    <updated>2016-06-01T23:12:53Z</updated>
    <published>2016-06-01T23:12:53Z</published>
    <title>On the equivalence between Kolmogorov-Smirnov and ROC curve metrics for
  binary classification</title>
    <summary>  Binary decisions are very common in artificial intelligence. Applying a
threshold on the continuous score gives the human decider the power to control
the operating point to separate the two classes. The classifier,s
discriminating power is measured along the continuous range of the score by the
Area Under the ROC curve (AUC_ROC) in most application fields. Only finances
uses the poor single point metric maximum Kolmogorov-Smirnov (KS) distance.
This paper proposes the Area Under the KS curve (AUC_KS) for performance
assessment and proves AUC_ROC = 0.5 + AUC_KS, as a simpler way to calculate the
AUC_ROC. That is even more important for ROC averaging in ensembles of
classifiers or n fold cross-validation. The proof is geometrically inspired on
rotating all KS curve to make it lie on the top of the ROC chance diagonal. On
the practical side, the independent variable on the abscissa on the KS curve
simplifies the calculation of the AUC_ROC. On the theoretical side, this
research gives insights on probabilistic interpretations of classifiers
assessment and integrates the existing body of knowledge of the information
theoretical ROC approach with the proposed statistical approach based on the
thoroughly known KS distribution.
</summary>
    <author>
      <name>Paulo J. L. Adeodato</name>
    </author>
    <author>
      <name>Sílvio B. Melo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00401v3</id>
    <updated>2016-06-21T12:18:24Z</updated>
    <published>2016-06-01T19:07:48Z</published>
    <title>How to advance general game playing artificial intelligence by player
  modelling</title>
    <summary>  General game playing artificial intelligence has recently seen important
advances due to the various techniques known as 'deep learning'. However the
advances conceal equally important limitations in their reliance on: massive
data sets; fortuitously constructed problems; and absence of any human-level
complexity, including other human opponents. On the other hand, deep learning
systems which do beat human champions, such as in Go, do not generalise well.
The power of deep learning simultaneously exposes its weakness. Given that deep
learning is mostly clever reconfigurations of well-established methods, moving
beyond the state of art calls for forward-thinking visionary solutions, not
just more of the same. I present the argument that general game playing
artificial intelligence will require a generalised player model. This is
because games are inherently human artefacts which therefore, as a class of
problems, contain cases which require a human-style problem solving approach. I
relate this argument to the performance of state of art general game playing
agents. I then describe a concept for a formal category theoretic basis to a
generalised player model. This formal model approach integrates my existing
'Behavlets' method for psychologically-derived player modelling:
  Cowley, B., Charles, D. (2016). Behavlets: a Method for Practical Player
Modelling using Psychology-Based Player Traits and Domain Specific Features.
User Modeling and User-Adapted Interaction, 26(2), 257-306.
</summary>
    <author>
      <name>Benjamin Ultan Cowley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00401v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00401v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00339v1</id>
    <updated>2016-06-01T16:04:47Z</updated>
    <published>2016-06-01T16:04:47Z</published>
    <title>A structured argumentation framework for detaching conditional
  obligations</title>
    <summary>  We present a general formal argumentation system for dealing with the
detachment of conditional obligations. Given a set of facts, constraints, and
conditional obligations, we answer the question whether an unconditional
obligation is detachable by considering reasons for and against its detachment.
For the evaluation of arguments in favor of detaching obligations we use a
Dung-style argumentation-theoretical semantics. We illustrate the modularity of
the general framework by considering some extensions, and we compare the
framework to some related approaches from the literature.
</summary>
    <author>
      <name>Mathieu Beirlaen</name>
    </author>
    <author>
      <name>Christian Straßer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is our submission to DEON 2016, including the technical appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T37" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4; F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00133v1</id>
    <updated>2016-06-01T06:46:51Z</updated>
    <published>2016-06-01T06:46:51Z</published>
    <title>A Survey of Qualitative Spatial and Temporal Calculi -- Algebraic and
  Computational Properties</title>
    <summary>  Qualitative Spatial and Temporal Reasoning (QSTR) is concerned with symbolic
knowledge representation, typically over infinite domains. The motivations for
employing QSTR techniques range from exploiting computational properties that
allow efficient reasoning to capture human cognitive concepts in a
computational framework. The notion of a qualitative calculus is one of the
most prominent QSTR formalisms. This article presents the first overview of all
qualitative calculi developed to date and their computational properties,
together with generalized definitions of the fundamental concepts and methods,
which now encompass all existing calculi. Moreover, we provide a classification
of calculi according to their algebraic properties.
</summary>
    <author>
      <name>Frank Dylla</name>
    </author>
    <author>
      <name>Jae Hee Lee</name>
    </author>
    <author>
      <name>Till Mossakowski</name>
    </author>
    <author>
      <name>Thomas Schneider</name>
    </author>
    <author>
      <name>André Van Delden</name>
    </author>
    <author>
      <name>Jasper Van De Ven</name>
    </author>
    <author>
      <name>Diedrich Wolter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM Computing Surveys</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00117v1</id>
    <updated>2016-06-01T04:49:08Z</updated>
    <published>2016-06-01T04:49:08Z</published>
    <title>Hardness of the Pricing Problem for Chains in Barter Exchanges</title>
    <summary>  Kidney exchange is a barter market where patients trade willing but medically
incompatible donors. These trades occur via cycles, where each patient-donor
pair both gives and receives a kidney, and via chains, which begin with an
altruistic donor who does not require a kidney in return. For logistical
reasons, the maximum length of a cycle is typically limited to a small
constant, while chains can be much longer. Given a compatibility graph of
patient-donor pairs, altruists, and feasible potential transplants between
them, finding even a maximum-cardinality set of vertex-disjoint cycles and
chains is NP-hard. There has been much work on developing provably optimal
solvers that are efficient in practice. One of the leading techniques has been
branch and price, where column generation is used to incrementally bring cycles
and chains into the optimization model on an as-needed basis. In particular,
only positive-price columns need to be brought into the model. We prove that
finding a positive-price chain is NP-complete. This shows incorrectness of two
leading branch-and-price solvers that suggested polynomial-time chain pricing
algorithms.
</summary>
    <author>
      <name>Benjamin Plaut</name>
    </author>
    <author>
      <name>John P. Dickerson</name>
    </author>
    <author>
      <name>Tuomas Sandholm</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00075v1</id>
    <updated>2016-05-31T23:48:55Z</updated>
    <published>2016-05-31T23:48:55Z</published>
    <title>Applications of Probabilistic Programming (Master's thesis, 2015)</title>
    <summary>  This thesis describes work on two applications of probabilistic programming:
the learning of probabilistic program code given specifications, in particular
program code of one-dimensional samplers; and the facilitation of sequential
Monte Carlo inference with help of data-driven proposals. The latter is
presented with experimental results on a linear Gaussian model and a
non-parametric dependent Dirichlet process mixture of objects model for object
recognition and tracking.
  In Chapter 1 we provide a brief introduction to probabilistic programming.
  In Chapter 3 we present an approach to automatic discovery of samplers in the
form of probabilistic programs. We formulate a Bayesian approach to this
problem by specifying a grammar-based prior over probabilistic program code. We
use an approximate Bayesian computation method to learn the programs, whose
executions generate samples that statistically match observed data or
analytical characteristics of distributions of interest. In our experiments we
leverage different probabilistic programming systems to perform Markov chain
Monte Carlo sampling over the space of programs. Experimental results have
demonstrated that, using the proposed methodology, we can learn approximate and
even some exact samplers. Finally, we show that our results are competitive
with regard to genetic programming methods.
  In Chapter 3, we describe a way to facilitate sequential Monte Carlo
inference in probabilistic programming using data-driven proposals. In
particular, we develop a distance-based proposal for the non-parametric
dependent Dirichlet process mixture of objects model. We implement this
approach in the probabilistic programming system Anglican, and show that for
that model data-driven proposals provide significant performance improvements.
We also explore the possibility of using neural networks to improve data-driven
proposals.
</summary>
    <author>
      <name>Yura N Perov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supervisor: Frank Wood. The thesis was prepared in the Department of
  Engineering Science at the University of Oxford</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00068v1</id>
    <updated>2016-05-31T22:37:43Z</updated>
    <published>2016-05-31T22:37:43Z</published>
    <title>Quantifying the probable approximation error of probabilistic inference
  programs</title>
    <summary>  This paper introduces a new technique for quantifying the approximation error
of a broad class of probabilistic inference programs, including ones based on
both variational and Monte Carlo approaches. The key idea is to derive a
subjective bound on the symmetrized KL divergence between the distribution
achieved by an approximate inference program and its true target distribution.
The bound's validity (and subjectivity) rests on the accuracy of two auxiliary
probabilistic programs: (i) a "reference" inference program that defines a gold
standard of accuracy and (ii) a "meta-inference" program that answers the
question "what internal random choices did the original approximate inference
program probably make given that it produced a particular result?" The paper
includes empirical results on inference problems drawn from linear regression,
Dirichlet process mixture modeling, HMMs, and Bayesian networks. The
experiments show that the technique is robust to the quality of the reference
inference program and that it can detect implementation bugs that are not
apparent from predictive performance.
</summary>
    <author>
      <name>Marco F Cusumano-Towner</name>
    </author>
    <author>
      <name>Vikash K Mansinghka</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00058v2</id>
    <updated>2016-06-06T10:53:16Z</updated>
    <published>2016-05-31T21:52:13Z</published>
    <title>How to avoid ethically relevant Machine Consciousness</title>
    <summary>  This paper discusses the root cause of systems perceiving the self experience
and how to exploit adaptive and learning features without introducing ethically
problematic system properties.
</summary>
    <author>
      <name>Aleksander Lodwich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00058v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00058v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00002v1</id>
    <updated>2016-05-31T21:32:48Z</updated>
    <published>2016-05-31T21:32:48Z</published>
    <title>Uncertain programming model for multi-item solid transportation problem</title>
    <summary>  In this paper, an uncertain Multi-objective Multi-item Solid Transportation
Problem (MMSTP) based on uncertainty theory is presented. In the model,
transportation costs, supplies, demands and conveyances parameters are taken to
be uncertain parameters. There are restrictions on some items and conveyances
of the model. Therefore, some particular items cannot be transported by some
exceptional conveyances. Using the advantage of uncertainty theory, the MMSTP
is first converted into an equivalent deterministic MMSTP. By applying convex
combination method and minimizing distance function method, the deterministic
MMSTP is reduced into single objective programming problems. Thus, both single
objective programming problems are solved using Maple 18.02 optimization
toolbox. Finally, a numerical example is given to illustrate the performance of
the models.
</summary>
    <author>
      <name>Hasan Dalman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13042-016-0538-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13042-016-0538-7" rel="related"/>
    <link href="http://arxiv.org/abs/1606.00002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T37, 62C86, 90C29, 90C70" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09782v3</id>
    <updated>2016-07-18T03:25:03Z</updated>
    <published>2016-05-31T19:37:29Z</published>
    <title>Adversarial Feature Learning</title>
    <summary>  The ability of the Generative Adversarial Networks (GANs) framework to learn
generative models mapping from simple latent distributions to arbitrarily
complex data distributions has been demonstrated empirically, with compelling
results showing generators learn to "linearize semantics" in the latent space
of such models. Intuitively, such latent spaces may serve as useful feature
representations for auxiliary problems where semantics are relevant. However,
in their existing form, GANs have no means of learning the inverse mapping --
projecting data back into the latent space. We propose Bidirectional Generative
Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and
demonstrate that the resulting learned feature representation is useful for
auxiliary supervised discrimination tasks, competitive with contemporary
approaches to unsupervised and self-supervised feature learning.
</summary>
    <author>
      <name>Jeff Donahue</name>
    </author>
    <author>
      <name>Philipp Krähenbühl</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09782v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09782v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09772v1</id>
    <updated>2016-05-31T19:12:41Z</updated>
    <published>2016-05-31T19:12:41Z</published>
    <title>Technical Report: Directed Controller Synthesis of Discrete Event
  Systems</title>
    <summary>  This paper presents a Directed Controller Synthesis (DCS) technique for
discrete event systems. The DCS method explores the solution space for reactive
controllers guided by a domain-independent heuristic. The heuristic is derived
from an efficient abstraction of the environment based on the componentized way
in which complex environments are described. Then by building the composition
of the components on-the-fly DCS obtains a solution by exploring a reduced
portion of the state space. This work focuses on untimed discrete event systems
with safety and co-safety (i.e. reachability) goals. An evaluation for the
technique is presented comparing it to other well-known approaches to
controller synthesis (based on symbolic representation and compositional
analyses).
</summary>
    <author>
      <name>Daniel Ciolek</name>
    </author>
    <author>
      <name>Victor Braberman</name>
    </author>
    <author>
      <name>Nicolás D'Ippolito</name>
    </author>
    <author>
      <name>Sebastián Uchitel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, submitted to the 55th IEEE Conference on Decision and
  Control</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09757v1</id>
    <updated>2016-05-31T18:35:44Z</updated>
    <published>2016-05-31T18:35:44Z</published>
    <title>Towards ontology driven learning of visual concept detectors</title>
    <summary>  The maturity of deep learning techniques has led in recent years to a
breakthrough in object recognition in visual media. While for some specific
benchmarks, neural techniques seem to match if not outperform human judgement,
challenges are still open for detecting arbitrary concepts in arbitrary videos.
In this paper, we propose a system that combines neural techniques, a large
scale visual concepts ontology, and an active learning loop, to provide on the
fly model learning of arbitrary concepts. We give an overview of the system as
a whole, and focus on the central role of the ontology for guiding and
bootstrapping the learning of new concepts, improving the recall of concept
detection, and, on the user end, providing semantic search on a library of
annotated videos.
</summary>
    <author>
      <name>Sanchit Arora</name>
    </author>
    <author>
      <name>Chuck Cho</name>
    </author>
    <author>
      <name>Paul Fitzpatrick</name>
    </author>
    <author>
      <name>Francois Scharffe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">unpublished</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09735v1</id>
    <updated>2016-05-31T17:30:54Z</updated>
    <published>2016-05-31T17:30:54Z</published>
    <title>Information Theoretically Aided Reinforcement Learning for Embodied
  Agents</title>
    <summary>  Reinforcement learning for embodied agents is a challenging problem. The
accumulated reward to be optimized is often a very rugged function, and
gradient methods are impaired by many local optimizers. We demonstrate, in an
experimental setting, that incorporating an intrinsic reward can smoothen the
optimization landscape while preserving the global optimizers of interest. We
show that policy gradient optimization for locomotion in a complex morphology
is significantly improved when supplementing the extrinsic reward by an
intrinsic reward defined in terms of the mutual information of time consecutive
sensor readings.
</summary>
    <author>
      <name>Guido Montufar</name>
    </author>
    <author>
      <name>Keyan Ghazi-Zahedi</name>
    </author>
    <author>
      <name>Nihat Ay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 8 pages appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09674v2</id>
    <updated>2016-08-17T18:25:42Z</updated>
    <published>2016-05-31T15:34:36Z</published>
    <title>Variational Information Maximizing Exploration</title>
    <summary>  Scalable and effective exploration remains a key challenge in reinforcement
learning (RL). While there are methods with optimality guarantees in the
setting of discrete state and action spaces, these methods cannot be applied in
high-dimensional deep RL scenarios. As such, most contemporary RL relies on
simple heuristics such as epsilon-greedy exploration or adding Gaussian noise
to the controls. This paper introduces Variational Information Maximizing
Exploration (VIME), an exploration strategy based on maximization of
information gain about the agent's belief of environment dynamics. We propose a
practical implementation, using variational inference in Bayesian neural
networks which efficiently handles continuous state and action spaces. VIME
modifies the MDP reward function, and can be applied with several different
underlying RL algorithms. We demonstrate that VIME achieves significantly
better performance compared to heuristic exploration methods across a variety
of continuous control tasks and algorithms, including tasks with very sparse
rewards.
</summary>
    <author>
      <name>Rein Houthooft</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Yan Duan</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Filip De Turck</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09674v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09674v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09593v1</id>
    <updated>2016-05-31T12:11:51Z</updated>
    <published>2016-05-31T12:11:51Z</published>
    <title>Controlling Exploration Improves Training for Deep Neural Networks</title>
    <summary>  Stochastic optimization methods are widely used for training of deep neural
networks. However, it is still a challenging research problem to achieve
effective training by using stochastic optimization methods. This is due to the
difficulties in finding good parameters on a loss function that have many
saddle points. In this paper, we propose a stochastic optimization method
called STDProp for effective training of deep neural networks. Its key idea is
to effectively explore parameters on a complex surface of a loss function. We
additionally develop momentum version of STDProp. While our approaches are easy
to implement with high memory efficiency, it is more effective than other
practical stochastic optimization methods for deep neural networks.
</summary>
    <author>
      <name>Yasutoshi Ida</name>
    </author>
    <author>
      <name>Yasuhiro Fujiwara</name>
    </author>
    <author>
      <name>Sotetsu Iwamura</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09564v1</id>
    <updated>2016-05-31T10:31:16Z</updated>
    <published>2016-05-31T10:31:16Z</published>
    <title>Determining the Characteristic Vocabulary for a Specialized Dictionary
  using Word2vec and a Directed Crawler</title>
    <summary>  Specialized dictionaries are used to understand concepts in specific domains,
especially where those concepts are not part of the general vocabulary, or
having meanings that differ from ordinary languages. The first step in creating
a specialized dictionary involves detecting the characteristic vocabulary of
the domain in question. Classical methods for detecting this vocabulary involve
gathering a domain corpus, calculating statistics on the terms found there, and
then comparing these statistics to a background or general language corpus.
Terms which are found significantly more often in the specialized corpus than
in the background corpus are candidates for the characteristic vocabulary of
the domain. Here we present two tools, a directed crawler, and a distributional
semantics package, that can be used together, circumventing the need of a
background corpus. Both tools are available on the web.
</summary>
    <author>
      <name>Gregory Grefenstette</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAO</arxiv:affiliation>
    </author>
    <author>
      <name>Lawrence Muchemi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAO</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">GLOBALEX 2016: Lexicographic Resources for Human Language
  Technology, May 2016, Portoroz, Slovenia. 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.09564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09505v1</id>
    <updated>2016-05-31T06:43:51Z</updated>
    <published>2016-05-31T06:43:51Z</published>
    <title>Psychologically based Virtual-Suspect for Interrogative Interview
  Training</title>
    <summary>  In this paper, we present a Virtual-Suspect system which can be used to train
inexperienced law enforcement personnel in interrogation strategies. The system
supports different scenario configurations based on historical data. The
responses presented by the Virtual-Suspect are selected based on the
psychological state of the suspect, which can be configured as well.
Furthermore, each interrogator's statement affects the Virtual-Suspect's
current psychological state, which may lead the interrogation in different
directions. In addition, the model takes into account the context in which the
statements are made. Experiments with 24 subjects demonstrate that the
Virtual-Suspect's behavior is similar to that of a human who plays the role of
the suspect.
</summary>
    <author>
      <name>Moshe Bitan</name>
    </author>
    <author>
      <name>Galit Nahari</name>
    </author>
    <author>
      <name>Zvi Nisin</name>
    </author>
    <author>
      <name>Ariel Roth</name>
    </author>
    <author>
      <name>Sarit Kraus</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09497v1</id>
    <updated>2016-05-31T04:54:46Z</updated>
    <published>2016-05-31T04:54:46Z</published>
    <title>Interdependent Scheduling Games</title>
    <summary>  We propose a model of interdependent scheduling games in which each player
controls a set of services that they schedule independently. A player is free
to schedule his own services at any time; however, each of these services only
begins to accrue reward for the player when all predecessor services, which may
or may not be controlled by the same player, have been activated. This model,
where players have interdependent services, is motivated by the problems faced
in planning and coordinating large-scale infrastructures, e.g., restoring
electricity and gas to residents after a natural disaster or providing medical
care in a crisis when different agencies are responsible for the delivery of
staff, equipment, and medicine. We undertake a game-theoretic analysis of this
setting and in particular consider the issues of welfare maximization,
computing best responses, Nash dynamics, and existence and computation of Nash
equilibria.
</summary>
    <author>
      <name>Andres Abeliuk</name>
    </author>
    <author>
      <name>Haris Aziz</name>
    </author>
    <author>
      <name>Gerardo Berbeglia</name>
    </author>
    <author>
      <name>Serge Gaspers</name>
    </author>
    <author>
      <name>Petr Kalina</name>
    </author>
    <author>
      <name>Nicholas Mattei</name>
    </author>
    <author>
      <name>Dominik Peters</name>
    </author>
    <author>
      <name>Paul Stursberg</name>
    </author>
    <author>
      <name>Pascal Van Hentenryck</name>
    </author>
    <author>
      <name>Toby Walsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A80, 90B35, 91B74" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; I.2; F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09370v1</id>
    <updated>2016-05-30T19:57:56Z</updated>
    <published>2016-05-30T19:57:56Z</published>
    <title>Unsupervised Discovery of El Nino Using Causal Feature Learning on
  Microlevel Climate Data</title>
    <summary>  We show that the climate phenomena of El Nino and La Nina arise naturally as
states of macro-variables when our recent causal feature learning framework
(Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind
(ZW) and sea surface temperatures (SST) taken over the equatorial band of the
Pacific Ocean. The method identifies these unusual climate states on the basis
of the relation between ZW and SST patterns without any input about past
occurrences of El Nino or La Nina. The simpler alternatives of (i) clustering
the SST fields while disregarding their relationship with ZW patterns, or (ii)
clustering the joint ZW-SST patterns, do not discover El Nino. We discuss the
degree to which our method supports a causal interpretation and use a
low-dimensional toy example to explain its success over other clustering
approaches. Finally, we propose a new robust and scalable alternative to our
original algorithm (Chalupka 2016), which circumvents the need for
high-dimensional density learning.
</summary>
    <author>
      <name>Krzysztof Chalupka</name>
    </author>
    <author>
      <name>Tobias Bischoff</name>
    </author>
    <author>
      <name>Pietro Perona</name>
    </author>
    <author>
      <name>Frederick Eberhardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for plenary presentation at UAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09304v3</id>
    <updated>2016-06-06T17:34:59Z</updated>
    <published>2016-05-30T16:22:54Z</published>
    <title>Synthesizing the preferred inputs for neurons in neural networks via
  deep generator networks</title>
    <summary>  Deep neural networks (DNNs) have demonstrated state-of-the-art results on
many pattern recognition tasks, especially vision classification problems.
Understanding the inner workings of such computational brains is both
fascinating basic science that is interesting in its own right - similar to why
we study the human brain - and will enable researchers to further improve DNNs.
One path to understanding how a neural network functions internally is to study
what each of its neurons has learned to detect. One such method is called
activation maximization (AM), which synthesizes an input (e.g. an image) that
highly activates a neuron. Here we dramatically improve the qualitative state
of the art of activation maximization by harnessing a powerful, learned prior:
a deep generator network (DGN). The algorithm (1) generates qualitatively
state-of-the-art synthetic images that look almost real, (2) reveals the
features learned by each neuron in an interpretable way, (3) generalizes well
to new datasets and somewhat well to different network architectures without
requiring the prior to be relearned, and (4) can be considered as a
high-quality generative method (in this case, by generating novel, creative,
interesting, recognizable images).
</summary>
    <author>
      <name>Anh Nguyen</name>
    </author>
    <author>
      <name>Alexey Dosovitskiy</name>
    </author>
    <author>
      <name>Jason Yosinski</name>
    </author>
    <author>
      <name>Thomas Brox</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 35 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09304v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09304v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09293v1</id>
    <updated>2016-05-30T16:01:51Z</updated>
    <published>2016-05-30T16:01:51Z</published>
    <title>Internal Guidance for Satallax</title>
    <summary>  We propose a new internal guidance method for automated theorem provers based
on the given-clause algorithm. Our method influences the choice of unprocessed
clauses using positive and negative examples from previous proofs. To this end,
we present an efficient scheme for Naive Bayesian classification by
generalising label occurrences to types with monoid structure. This makes it
possible to extend existing fast classifiers, which consider only positive
examples, with negative ones. We implement the method in the higher-order logic
prover Satallax, where we modify the delay with which propositions are
processed. We evaluated our method on a simply-typed higher-order logic version
of the Flyspeck project, where it solves 26% more problems than Satallax
without internal guidance.
</summary>
    <author>
      <name>Michael Färber</name>
    </author>
    <author>
      <name>Chad Brown</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09128v1</id>
    <updated>2016-05-30T07:40:13Z</updated>
    <published>2016-05-30T07:40:13Z</published>
    <title>Control of Memory, Active Perception, and Action in Minecraft</title>
    <summary>  In this paper, we introduce a new set of reinforcement learning (RL) tasks in
Minecraft (a flexible 3D world). We then use these tasks to systematically
compare and contrast existing deep reinforcement learning (DRL) architectures
with our new memory-based DRL architectures. These tasks are designed to
emphasize, in a controllable manner, issues that pose challenges for RL methods
including partial observability (due to first-person visual observations),
delayed rewards, high-dimensional visual observations, and the need to use
active perception in a correct manner so as to perform well in the tasks. While
these tasks are conceptually simple to describe, by virtue of having all of
these challenges simultaneously they are difficult for current DRL
architectures. Additionally, we evaluate the generalization performance of the
architectures on environments not used during training. The experimental
results show that our new architectures generalize to unseen environments
better than existing DRL architectures.
</summary>
    <author>
      <name>Junhyuk Oh</name>
    </author>
    <author>
      <name>Valliappa Chockalingam</name>
    </author>
    <author>
      <name>Satinder Singh</name>
    </author>
    <author>
      <name>Honglak Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09081v1</id>
    <updated>2016-05-30T00:50:39Z</updated>
    <published>2016-05-30T00:50:39Z</published>
    <title>Understanding Convolutional Neural Networks</title>
    <summary>  Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a
variety of machine learning tasks. However, their mathematical properties and
behavior are quite poorly understood. There is some work, in the form of a
framework, for analyzing the operations that they perform. The goal of this
project is to present key results from this theory, and provide intuition for
why CNNs work.
</summary>
    <author>
      <name>Jayanth Koushik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Machine Learning Course Project at Carnegie Mellon
  University</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09042v2</id>
    <updated>2016-09-02T05:48:45Z</updated>
    <published>2016-05-29T18:24:45Z</published>
    <title>MCMC assisted by Belief Propagaion</title>
    <summary>  Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most
popular algorithms for computational inference in Graphical Models (GM). In
principle, MCMC is an exact probabilistic method which, however, often suffers
from exponentially slow mixing. In contrast, BP is a deterministic method,
which is typically fast, empirically very successful, however in general
lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC
algorithms correcting the approximation error of BP, i.e., we provide a way to
compensate for BP errors via a consecutive BP-aware MCMC. Our framework is
based on the Loop Calculus (LC) approach which allows to express the BP error
as a sum of weighted generalized loops. Although the full series is
computationally intractable, it is known that a truncated series, summing up
all 2-regular loops, is computable in polynomial-time for planar pair-wise
binary GMs and it also provides a highly accurate approximation empirically.
Motivated by this, we first propose a polynomial-time approximation MCMC scheme
for the truncated series of general (non-planar) pair-wise binary models. Our
main idea here is to use the Worm algorithm, known to provide fast mixing in
other (related) problems, and then design an appropriate rejection scheme to
sample 2-regular loops. Furthermore, we also design an efficient rejection-free
MCMC scheme for approximating the full series. The main novelty underlying our
design is in utilizing the concept of cycle basis, which provides an efficient
decomposition of the generalized loops. In essence, the proposed MCMC schemes
run on transformed GM built upon the non-trivial BP solution, and our
experiments show that this synthesis of BP and MCMC outperforms both direct
MCMC and bare BP schemes.
</summary>
    <author>
      <name>Sungsoo Ahn</name>
    </author>
    <author>
      <name>Michael Chertkov</name>
    </author>
    <author>
      <name>Jinwoo Shin</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08878v1</id>
    <updated>2016-05-28T11:29:09Z</updated>
    <published>2016-05-28T11:29:09Z</published>
    <title>Computational Estimate Visualisation and Evaluation of Agent Classified
  Rules Learning System</title>
    <summary>  Student modelling and agent classified rules learning as applied in the
development of the intelligent Preassessment System has been presented in
[10],[11]. In this paper, we now demystify the theory behind the development of
the pre-assessment system followed by some computational experimentation and
graph visualisation of the agent classified rules learning algorithm in the
estimation and prediction of classified rules. In addition, we present some
preliminary results of the pre-assessment system evaluation. From the results,
it is gathered that the system has performed according to its design
specification.
</summary>
    <author>
      <name>Kennedy E. Ehimwenma</name>
    </author>
    <author>
      <name>Martin Beer</name>
    </author>
    <author>
      <name>Paul Crowther</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 15 figures, International Journal of Emerging Technologies
  in Learning iJET, 11(1), 2016, PhD research work</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Emerging Technologies in Learning (iJET),
  11(01), 38-47 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.08878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08803v1</id>
    <updated>2016-05-27T21:24:32Z</updated>
    <published>2016-05-27T21:24:32Z</published>
    <title>Density estimation using Real NVP</title>
    <summary>  Unsupervised learning of probabilistic models is a central yet challenging
problem in machine learning. Specifically, designing models with tractable
learning, sampling, inference and evaluation is crucial in solving this task.
We extend the space of such models using real-valued non-volume preserving
(real NVP) transformations, a set of powerful invertible and learnable
transformations, resulting in an unsupervised learning algorithm with exact
log-likelihood computation, exact sampling, exact inference of latent
variables, and an interpretable latent space. We demonstrate its ability to
model natural images on four datasets through sampling, log-likelihood
evaluation and latent variable manipulations.
</summary>
    <author>
      <name>Laurent Dinh</name>
    </author>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <author>
      <name>Samy Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages of main content, 3 pages of bibliography, 18 pages of
  appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08803v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08803v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08695v2</id>
    <updated>2016-05-31T19:46:10Z</updated>
    <published>2016-05-27T15:49:50Z</published>
    <title>TensorFlow: A system for large-scale machine learning</title>
    <summary>  TensorFlow is a machine learning system that operates at large scale and in
heterogeneous environments. TensorFlow uses dataflow graphs to represent
computation, shared state, and the operations that mutate that state. It maps
the nodes of a dataflow graph across many machines in a cluster, and within a
machine across multiple computational devices, including multicore CPUs,
general-purpose GPUs, and custom designed ASICs known as Tensor Processing
Units (TPUs). This architecture gives flexibility to the application developer:
whereas in previous "parameter server" designs the management of shared state
is built into the system, TensorFlow enables developers to experiment with
novel optimizations and training algorithms. TensorFlow supports a variety of
applications, with particularly strong support for training and inference on
deep neural networks. Several Google services use TensorFlow in production, we
have released it as an open-source project, and it has become widely used for
machine learning research. In this paper, we describe the TensorFlow dataflow
model in contrast to existing systems, and demonstrate the compelling
performance that TensorFlow achieves for several real-world applications.
</summary>
    <author>
      <name>Martín Abadi</name>
    </author>
    <author>
      <name>Paul Barham</name>
    </author>
    <author>
      <name>Jianmin Chen</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Andy Davis</name>
    </author>
    <author>
      <name>Jeffrey Dean</name>
    </author>
    <author>
      <name>Matthieu Devin</name>
    </author>
    <author>
      <name>Sanjay Ghemawat</name>
    </author>
    <author>
      <name>Geoffrey Irving</name>
    </author>
    <author>
      <name>Michael Isard</name>
    </author>
    <author>
      <name>Manjunath Kudlur</name>
    </author>
    <author>
      <name>Josh Levenberg</name>
    </author>
    <author>
      <name>Rajat Monga</name>
    </author>
    <author>
      <name>Sherry Moore</name>
    </author>
    <author>
      <name>Derek G. Murray</name>
    </author>
    <author>
      <name>Benoit Steiner</name>
    </author>
    <author>
      <name>Paul Tucker</name>
    </author>
    <author>
      <name>Vijay Vasudevan</name>
    </author>
    <author>
      <name>Pete Warden</name>
    </author>
    <author>
      <name>Martin Wicke</name>
    </author>
    <author>
      <name>Yuan Yu</name>
    </author>
    <author>
      <name>Xiaoqiang Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 figures; v2 has a spelling correction in the metadata</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08695v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08695v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08478v1</id>
    <updated>2016-05-26T23:43:32Z</updated>
    <published>2016-05-26T23:43:32Z</published>
    <title>Model-Free Imitation Learning with Policy Optimization</title>
    <summary>  In imitation learning, an agent learns how to behave in an environment with
an unknown cost function by mimicking expert demonstrations. Existing imitation
learning algorithms typically involve solving a sequence of planning or
reinforcement learning problems. Such algorithms are therefore not directly
applicable to large, high-dimensional environments, and their performance can
significantly degrade if the planning problems are not solved to optimality.
Under the apprenticeship learning formalism, we develop alternative model-free
algorithms for finding a parameterized stochastic policy that performs at least
as well as an expert policy on an unknown cost function, based on sample
trajectories from the expert. Our approach, based on policy gradients, scales
to large continuous environments with guaranteed convergence to local minima.
</summary>
    <author>
      <name>Jonathan Ho</name>
    </author>
    <author>
      <name>Jayesh K. Gupta</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 33rd International Conference on Machine
  Learning, 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JMLR W&amp;CP 48 (2016) 2760-2769</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.08478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08464v1</id>
    <updated>2016-05-26T22:34:37Z</updated>
    <published>2016-05-26T22:34:37Z</published>
    <title>Low-Cost Scene Modeling using a Density Function Improves Segmentation
  Performance</title>
    <summary>  We propose a low cost and effective way to combine a free simulation software
and free CAD models for modeling human-object interaction in order to improve
human &amp; object segmentation. It is intended for research scenarios related to
safe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial
domain. The task of human and object modeling has been used for detecting
activity, and for inferring and predicting actions, different from those works,
we do human and object modeling in order to learn interactions in RGB-D data
for improving segmentation. For this purpose, we define a novel density
function to model a three dimensional (3D) scene in a virtual environment
(VREP). This density function takes into account various possible
configurations of human-object and object-object relationships and interactions
governed by their affordances. Using this function, we synthesize a large,
realistic and highly varied synthetic RGB-D dataset that we use for training.
We train a random forest classifier, and the pixelwise predictions obtained is
integrated as a unary term in a pairwise conditional random fields (CRF). Our
evaluation shows that modeling these interactions improves segmentation
performance by ~7\% in mean average precision and recall over state-of-the-art
methods that ignore these interactions in real-world data. Our approach is
computationally efficient, robust and can run real-time on consumer hardware.
</summary>
    <author>
      <name>Vivek Sharma</name>
    </author>
    <author>
      <name>Sule Yildirim-Yayilgan</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication at 25th IEEE International Symposium on
  Robot and Human Interactive Communication (RO-MAN), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08412v1</id>
    <updated>2016-05-26T19:19:43Z</updated>
    <published>2016-05-26T19:19:43Z</published>
    <title>CITlab ARGUS for historical handwritten documents</title>
    <summary>  We describe CITlab's recognition system for the HTRtS competition attached to
the 13. International Conference on Document Analysis and Recognition, ICDAR
2015. The task comprises the recognition of historical handwritten documents.
The core algorithms of our system are based on multi-dimensional recurrent
neural networks (MDRNN) and connectionist temporal classification (CTC). The
software modules behind that as well as the basic utility technologies are
essentially powered by PLANET's ARGUS framework for intelligent text
recognition and image processing.
</summary>
    <author>
      <name>Gundram Leifert</name>
    </author>
    <author>
      <name>Tobias Strauß</name>
    </author>
    <author>
      <name>Tobias Grüning</name>
    </author>
    <author>
      <name>Roger Labahn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Description of CITlab's System for the HTRtS 2015 Task : Handwritten
  Text Recognition on the tranScriptorium Dataset</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08374v1</id>
    <updated>2016-05-26T17:33:31Z</updated>
    <published>2016-05-26T17:33:31Z</published>
    <title>Kronecker Determinantal Point Processes</title>
    <summary>  Determinantal Point Processes (DPPs) are probabilistic models over all
subsets a ground set of $N$ items. They have recently gained prominence in
several applications that rely on "diverse" subsets. However, their
applicability to large problems is still limited due to the $\mathcal O(N^3)$
complexity of core tasks such as sampling and learning. We enable efficient
sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel
matrix decomposes as a tensor product of multiple smaller kernel matrices. This
decomposition immediately enables fast exact sampling. But contrary to what one
may expect, leveraging the Kronecker product structure for speeding up DPP
learning turns out to be more difficult. We overcome this challenge, and derive
batch and stochastic optimization algorithms for efficiently learning the
parameters of a KronDPP.
</summary>
    <author>
      <name>Zelda Mariet</name>
    </author>
    <author>
      <name>Suvrit Sra</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08367v2</id>
    <updated>2016-05-27T02:29:20Z</updated>
    <published>2016-05-26T17:10:10Z</published>
    <title>Probabilistic Inference Modulo Theories</title>
    <summary>  We present SGDPLL(T), an algorithm that solves (among many other problems)
probabilistic inference modulo theories, that is, inference problems over
probabilistic models defined via a logic theory provided as a parameter
(currently, propositional, equalities on discrete sorts, and inequalities, more
specifically difference arithmetic, on bounded integers). While many solutions
to probabilistic inference over logic representations have been proposed,
SGDPLL(T) is simultaneously (1) lifted, (2) exact and (3) modulo theories, that
is, parameterized by a background logic theory. This offers a foundation for
extending it to rich logic languages such as data structures and relational
data. By lifted, we mean algorithms with constant complexity in the domain size
(the number of values that variables can take). We also detail a solver for
summations with difference arithmetic and show experimental results from a
scenario in which SGDPLL(T) is much faster than a state-of-the-art
probabilistic solver.
</summary>
    <author>
      <name>Rodrigo de Salvo Braz</name>
    </author>
    <author>
      <name>Ciaran O'Reilly</name>
    </author>
    <author>
      <name>Vibhav Gogate</name>
    </author>
    <author>
      <name>Rina Dechter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to StarAI-16 workshop as closely revised version of
  IJCAI-16 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08367v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08367v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08187v3</id>
    <updated>2016-06-14T18:29:14Z</updated>
    <published>2016-05-26T08:26:34Z</published>
    <title>The Symbolic Interior Point Method</title>
    <summary>  A recent trend in probabilistic inference emphasizes the codification of
models in a formal syntax, with suitable high-level features such as
individuals, relations, and connectives, enabling descriptive clarity,
succinctness and circumventing the need for the modeler to engineer a custom
solver. Unfortunately, bringing these linguistic and pragmatic benefits to
numerical optimization has proven surprisingly challenging. In this paper, we
turn to these challenges: we introduce a rich modeling language, for which an
interior-point method computes approximate solutions in a generic way. While
logical features easily complicates the underlying model, often yielding
intricate dependencies, we exploit and cache local structure using algebraic
decision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently
realizable in ADDs, but we argue and show that well-known optimization methods
are not ideal for ADDs. Our engine, therefore, invokes a sophisticated
matrix-free approach. We demonstrate the flexibility of the resulting
symbolic-numeric optimizer on decision making and compressed sensing tasks with
millions of non-zero entries.
</summary>
    <author>
      <name>Martin Mladenov</name>
    </author>
    <author>
      <name>Vaishak Belle</name>
    </author>
    <author>
      <name>Kristian Kersting</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08187v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08187v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08150v1</id>
    <updated>2016-05-26T05:49:25Z</updated>
    <published>2016-05-26T05:49:25Z</published>
    <title>Cognitive Dynamic Systems: A Technical Review of Cognitive Radar</title>
    <summary>  We start with the history of cognitive radar, where origins of the PAC,
Fuster research on cognition and principals of cognition are provided. Fuster
describes five cognitive functions: perception, memory, attention, language,
and intelligence. We describe the Perception-Action Cyclec as it applies to
cognitive radar, and then discuss long-term memory, memory storage, memory
retrieval and working memory. A comparison between memory in human cognition
and cognitive radar is given as well. Attention is another function described
by Fuster, and we have given the comparison of attention in human cognition and
cognitive radar. We talk about the four functional blocks from the PAC:
Bayesian filter, feedback information, dynamic programming and state-space
model for the radar environment. Then, to show that the PAC improves the
tracking accuracy of Cognitive Radar over Traditional Active Radar, we have
provided simulation results. In the simulation, three nonlinear filters:
Cubature Kalman Filter, Unscented Kalman Filter and Extended Kalman Filter are
compared. Based on the results, radars implemented with CKF perform better than
the radars implemented with UKF or radars implemented with EKF. Further, radar
with EKF has the worst accuracy and has the biggest computation load because of
derivation and evaluation of Jacobian matrices. We suggest using the concept of
risk management to better control parameters and improve performance in
cognitive radar. We believe, spectrum sensing can be seen as a potential
interest to be used in cognitive radar and we propose a new approach
Probabilistic ICA which will presumably reduce noise based on estimation error
in cognitive radar. Parallel computing is a concept based on divide and
conquers mechanism, and we suggest using the parallel computing approach in
cognitive radar by doing complicated calculations or tasks to reduce processing
time.
</summary>
    <author>
      <name>Krishanth Krishnan</name>
    </author>
    <author>
      <name>Taralyn Schwering</name>
    </author>
    <author>
      <name>Saman Sarraf</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08104v4</id>
    <updated>2016-08-31T16:06:03Z</updated>
    <published>2016-05-25T23:58:55Z</published>
    <title>Deep Predictive Coding Networks for Video Prediction and Unsupervised
  Learning</title>
    <summary>  While great strides have been made in using deep learning algorithms to solve
supervised learning tasks, the problem of unsupervised learning - leveraging
unlabeled examples to learn about the structure of a domain - remains a
difficult unsolved challenge. Here, we explore prediction of future frames in a
video sequence as an unsupervised learning rule for learning about the
structure of the visual world. We describe a predictive neural network
("PredNet") architecture that is inspired by the concept of "predictive coding"
from the neuroscience literature. These networks learn to predict future frames
in a video sequence, with each layer in the network making local predictions
and only forwarding deviations from those predictions to subsequent network
layers. We show that these networks are able to robustly learn to predict the
movement of synthetic (rendered) objects, and that in doing so, the networks
learn internal representations that are useful for decoding latent object
parameters (e.g. pose) that support object recognition with fewer training
views. We also show that these networks can scale to complex natural image
streams (car-mounted camera videos), capturing key aspects of both egocentric
movement and the movement of objects in the visual scene, and generalizing
across video datasets. These results suggest that prediction represents a
powerful framework for unsupervised learning, allowing for implicit learning of
object and scene structure.
</summary>
    <author>
      <name>William Lotter</name>
    </author>
    <author>
      <name>Gabriel Kreiman</name>
    </author>
    <author>
      <name>David Cox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code and example video clips can be found here:
  https://coxlab.github.io/prednet/</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08104v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08104v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08062v2</id>
    <updated>2016-06-01T18:23:04Z</updated>
    <published>2016-05-25T20:15:38Z</published>
    <title>A PAC RL Algorithm for Episodic POMDPs</title>
    <summary>  Many interesting real world domains involve reinforcement learning (RL) in
partially observable environments. Efficient learning in such domains is
important, but existing sample complexity bounds for partially observable RL
are at least exponential in the episode length. We give, to our knowledge, the
first partially observable RL algorithm with a polynomial bound on the number
of episodes on which the algorithm may not achieve near-optimal performance.
Our algorithm is suitable for an important class of episodic POMDPs. Our
approach builds on recent advances in method of moments for latent variable
model estimation.
</summary>
    <author>
      <name>Zhaohan Daniel Guo</name>
    </author>
    <author>
      <name>Shayan Doroudi</name>
    </author>
    <author>
      <name>Emma Brunskill</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 19th International Conference on Artificial
  Intelligence and Statistics, pp. 510-518, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.08062v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08062v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07999v1</id>
    <updated>2016-05-25T18:33:10Z</updated>
    <published>2016-05-25T18:33:10Z</published>
    <title>Toward a general, scaleable framework for Bayesian teaching with
  applications to topic models</title>
    <summary>  Machines, not humans, are the world's dominant knowledge accumulators but
humans remain the dominant decision makers. Interpreting and disseminating the
knowledge accumulated by machines requires expertise, time, and is prone to
failure. The problem of how best to convey accumulated knowledge from computers
to humans is a critical bottleneck in the broader application of machine
learning. We propose an approach based on human teaching where the problem is
formalized as selecting a small subset of the data that will, with high
probability, lead the human user to the correct inference. This approach,
though successful for modeling human learning in simple laboratory experiments,
has failed to achieve broader relevance due to challenges in formulating
general and scalable algorithms. We propose general-purpose teaching via
pseudo-marginal sampling and demonstrate the algorithm by teaching topic
models. Simulation results show our sampling-based approach: effectively
approximates the probability where ground-truth is possible via enumeration,
results in data that are markedly different from those expected by random
sampling, and speeds learning especially for small amounts of data. Application
to movie synopsis data illustrates differences between teaching and random
sampling for teaching distributions and specific topics, and demonstrates gains
in scalability and applicability to real-world problems.
</summary>
    <author>
      <name>Baxter S. Eaves Jr</name>
    </author>
    <author>
      <name>Patrick Shafto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, 5 Figures, submitted to IJCAI 2016 workshop on Interactive
  Machine Learning: Connecting Humans and Machines</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07989v2</id>
    <updated>2016-07-05T02:10:11Z</updated>
    <published>2016-05-25T18:10:48Z</published>
    <title>Compliant Conditions for Polynomial Time Approximation of Operator
  Counts</title>
    <summary>  In this paper, we develop a computationally simpler version of the operator
count heuristic for a particular class of domains. The contribution of this
abstract is threefold, we (1) propose an efficient closed form approximation to
the operator count heuristic using the Lagrangian dual; (2) leverage compressed
sensing techniques to obtain an integer approximation for operator counts in
polynomial time; and (3) discuss the relationship of the proposed formulation
to existing heuristics and investigate properties of domains where such
approaches appear to be useful.
</summary>
    <author>
      <name>Tathagata Chakraborti</name>
    </author>
    <author>
      <name>Sarath Sreedharan</name>
    </author>
    <author>
      <name>Sailik Sengupta</name>
    </author>
    <author>
      <name>T. K. Satish Kumar</name>
    </author>
    <author>
      <name>Subbarao Kambhampati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at the International Symposium on Combinatorial Search
  (SoCS), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07989v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07989v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07969v2</id>
    <updated>2016-05-26T15:49:33Z</updated>
    <published>2016-05-25T17:17:21Z</published>
    <title>Adaptive Neural Compilation</title>
    <summary>  This paper proposes an adaptive neural-compilation framework to address the
problem of efficient program learning. Traditional code optimisation strategies
used in compilers are based on applying pre-specified set of transformations
that make the code faster to execute without changing its semantics. In
contrast, our work involves adapting programs to make them more efficient while
considering correctness only on a target input distribution. Our approach is
inspired by the recent works on differentiable representations of programs. We
show that it is possible to compile programs written in a low-level language to
a differentiable representation. We also show how programs in this
representation can be optimised to make them efficient on a target distribution
of inputs. Experimental results demonstrate that our approach enables learning
specifically-tuned algorithms for given data distributions with a high success
rate.
</summary>
    <author>
      <name>Rudy Bunel</name>
    </author>
    <author>
      <name>Alban Desmaison</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>Philip H. S. Torr</name>
    </author>
    <author>
      <name>M. Pawan Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to NIPS 2016, code and supplementary materials will be
  available on author's page</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07969v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07969v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07918v1</id>
    <updated>2016-05-25T14:59:46Z</updated>
    <published>2016-05-25T14:59:46Z</published>
    <title>Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks
  with Feedback Negative Sampling</title>
    <summary>  Previous studies in Open Information Extraction (Open IE) are mainly based on
extraction patterns. They manually define patterns or automatically learn them
from a large corpus. However, these approaches are limited when grasping the
context of a sentence, and they fail to capture implicit relations. In this
paper, we address this problem with the following methods. First, we exploit
long short-term memory (LSTM) networks to extract higher-level features along
the shortest dependency paths, connecting headwords of relations and arguments.
The path-level features from LSTM networks provide useful clues regarding
contextual information and the validity of arguments. Second, we constructed
samples to train LSTM networks without the need for manual labeling. In
particular, feedback negative sampling picks highly negative samples among
non-positive samples through a model trained with positive samples. The
experimental results show that our approach produces more precise and abundant
extractions than state-of-the-art open IE systems. To the best of our
knowledge, this is the first work to apply deep learning to Open IE.
</summary>
    <author>
      <name>Byungsoo Kim</name>
    </author>
    <author>
      <name>Hwanjo Yu</name>
    </author>
    <author>
      <name>Gary Geunbae Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07918v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07918v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07895v1</id>
    <updated>2016-05-25T14:23:21Z</updated>
    <published>2016-05-25T14:23:21Z</published>
    <title>Automatic Extraction of Causal Relations from Natural Language Texts: A
  Comprehensive Survey</title>
    <summary>  Automatic extraction of cause-effect relationships from natural language
texts is a challenging open problem in Artificial Intelligence. Most of the
early attempts at its solution used manually constructed linguistic and
syntactic rules on small and domain-specific data sets. However, with the
advent of big data, the availability of affordable computing power and the
recent popularization of machine learning, the paradigm to tackle this problem
has slowly shifted. Machines are now expected to learn generic causal
extraction rules from labelled data with minimal supervision, in a domain
independent-manner. In this paper, we provide a comprehensive survey of causal
relation extraction techniques from both paradigms, and analyse their relative
strengths and weaknesses, with recommendations for future work.
</summary>
    <author>
      <name>Nabiha Asghar</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07844v1</id>
    <updated>2016-05-25T12:04:43Z</updated>
    <published>2016-05-25T12:04:43Z</published>
    <title>Dimension Projection among Languages based on Pseudo-relevant Documents
  for Query Translation</title>
    <summary>  Taking advantage of top-ranked documents in response to a query for improving
quality of query translation has been shown to be an effective approach for
cross-language information retrieval. In this paper, we propose a new method
for query translation based on dimension projection of embedded vectors from
the pseudo-relevant documents in the source language to their equivalents in
the target language. To this end, first we learn low-dimensional
representations of the words in the pseudo-relevant collections separately and
then aim at finding a query-dependent transformation matrix between the vectors
of translation pairs. At the next step, representation of each query term is
projected to the target language and then, after using a softmax function, a
query-dependent translation model is built. Finally, the model is used for
query translation. Our experiments on four CLEF collections in French, Spanish,
German, and Persian demonstrate that the proposed method outperforms all
competitive baselines in language modelling, particularly when it is combined
with a collection-dependent translation model.
</summary>
    <author>
      <name>Javid Dadashkarimi</name>
    </author>
    <author>
      <name>Mahsa S. Shahshahani</name>
    </author>
    <author>
      <name>Amirhossein Tebbifakhr</name>
    </author>
    <author>
      <name>Heshaam Faili</name>
    </author>
    <author>
      <name>Azadeh Shakery</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07736v1</id>
    <updated>2016-05-25T05:33:21Z</updated>
    <published>2016-05-25T05:33:21Z</published>
    <title>Learning Multiagent Communication with Backpropagation</title>
    <summary>  Many tasks in AI require the collaboration of multiple agents. Typically, the
communication protocol between agents is manually specified and not altered
during training. In this paper we explore a simple neural model, called CommNN,
that uses continuous communication for fully cooperative tasks. The model
consists of multiple agents and the communication between them is learned
alongside their policy. We apply this model to a diverse set of tasks,
demonstrating the ability of the agents to learn to communicate amongst
themselves, yielding improved performance over non-communicative agents and
baselines. In some cases, it is possible to interpret the language devised by
the agents, revealing simple but effective strategies for solving the task at
hand.
</summary>
    <author>
      <name>Sainbayar Sukhbaatar</name>
    </author>
    <author>
      <name>Arthur Szlam</name>
    </author>
    <author>
      <name>Rob Fergus</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07728v1</id>
    <updated>2016-05-25T04:33:41Z</updated>
    <published>2016-05-25T04:33:41Z</published>
    <title>Small Representations of Big Kidney Exchange Graphs</title>
    <summary>  Kidney exchanges are organized markets where patients swap willing but
incompatible donors. In the last decade, kidney exchanges grew from small and
regional to large and national---and soon, international. This growth results
in more lives saved, but exacerbates the empirical hardness of the
$\mathcal{NP}$-complete problem of optimally matching patients to donors.
State-of-the-art matching engines use integer programming techniques to clear
fielded kidney exchanges, but these methods must be tailored to specific models
and objective functions, and may fail to scale to larger exchanges. In this
paper, we observe that if the kidney exchange compatibility graph can be
encoded by a constant number of patient and donor attributes, the clearing
problem is solvable in polynomial time. We give necessary and sufficient
conditions for losslessly shrinking the representation of an arbitrary
compatibility graph. Then, using real compatibility graphs from the UNOS
nationwide kidney exchange, we show how many attributes are needed to encode
real compatibility graphs. The experiments show that, indeed, small numbers of
attributes suffice.
</summary>
    <author>
      <name>John P. Dickerson</name>
    </author>
    <author>
      <name>Aleksandr M. Kazachkov</name>
    </author>
    <author>
      <name>Ariel D. Procaccia</name>
    </author>
    <author>
      <name>Tuomas Sandholm</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07723v1</id>
    <updated>2016-05-25T04:14:59Z</updated>
    <published>2016-05-25T04:14:59Z</published>
    <title>Data Programming: Creating Large Training Sets, Quickly</title>
    <summary>  Large labeled training sets are the critical building blocks of supervised
learning methods and are key enablers of deep learning techniques. For some
applications, creating labeled training sets is the most time-consuming and
expensive part of applying machine learning. We therefore propose a paradigm
for the programmatic creation of training sets called data programming in which
users provide a set of labeling functions, which are programs that
heuristically label large subsets of data points, albeit noisily. By viewing
these labeling functions as implicitly describing a generative model for this
noise, we show that we can recover the parameters of this model to "denoise"
the training set. Then, we show how to modify a discriminative loss function to
make it noise-aware. We demonstrate our method over a range of discriminative
models including logistic regression and LSTMs. We establish theoretically that
we can recover the parameters of these generative models in a handful of
settings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, we
show that data programming would have obtained a winning score, and also show
that applying data programming to an LSTM model leads to a TAC-KBP score almost
6 F1 points over a supervised LSTM baseline (and into second place in the
competition). Additionally, in initial user studies we observed that data
programming may be an easier way to create machine learning models for
non-experts.
</summary>
    <author>
      <name>Alexander Ratner</name>
    </author>
    <author>
      <name>Christopher De Sa</name>
    </author>
    <author>
      <name>Sen Wu</name>
    </author>
    <author>
      <name>Daniel Selsam</name>
    </author>
    <author>
      <name>Christopher Ré</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07722v1</id>
    <updated>2016-05-25T04:13:49Z</updated>
    <published>2016-05-25T04:13:49Z</published>
    <title>Yum-me: Personalized Healthy Meal Recommender System</title>
    <summary>  Many ubiquitous computing projects have addressed health and wellness
behaviors such as healthy eating. Healthy meal recommendations have the
potential to help individuals prevent or manage conditions such as diabetes and
obesity. However, learning people's food preferences and making healthy
recommendations that appeal to their palate is challenging. Existing approaches
either only learn high-level preferences or require a prolonged learning
period. We propose Yum-me, a personalized healthy meal recommender system
designed to meet individuals' health goals, dietary restrictions, and
fine-grained food preferences. Marrying ideas from user preference learning and
healthy eating promotion, Yum-me enables a simple and accurate food preference
profiling procedure via an image-based online learning framework, and projects
the learned profile into the domain of healthy food options to find ones that
will appeal to the user. We present the design and implementation of Yum-me,
and further discuss the most critical component of it: FoodDist, a
state-of-the-art food image analysis model. We demonstrate FoodDist's superior
performance through careful benchmarking, and discuss its applicability across
a wide array of dietary applications. We validate the feasibility and
effectiveness of Yum-me through a 60-person user study, in which Yum-me
improves the recommendation acceptance rate by 42.63% over the traditional food
preference survey.
</summary>
    <author>
      <name>Longqi Yang</name>
    </author>
    <author>
      <name>Cheng-Kang Hsieh</name>
    </author>
    <author>
      <name>Hongjian Yang</name>
    </author>
    <author>
      <name>Nicola Dell</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <author>
      <name>Deborah Estrin</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07700v1</id>
    <updated>2016-05-25T01:33:34Z</updated>
    <published>2016-05-25T01:33:34Z</published>
    <title>Learning Purposeful Behaviour in the Absence of Rewards</title>
    <summary>  Artificial intelligence is commonly defined as the ability to achieve goals
in the world. In the reinforcement learning framework, goals are encoded as
reward functions that guide agent behaviour, and the sum of observed rewards
provide a notion of progress. However, some domains have no such reward signal,
or have a reward signal so sparse as to appear absent. Without reward feedback,
agent behaviour is typically random, often dithering aimlessly and lacking
intentionality. In this paper we present an algorithm capable of learning
purposeful behaviour in the absence of rewards. The algorithm proceeds by
constructing temporally extended actions (options), through the identification
of purposes that are "just out of reach" of the agent's current behaviour.
These purposes establish intrinsic goals for the agent to learn, ultimately
resulting in a suite of behaviours that encourage the agent to visit different
parts of the state space. Moreover, the approach is particularly suited for
settings where rewards are very sparse, and such behaviours can help in the
exploration of the environment until reward is observed.
</summary>
    <author>
      <name>Marlos C. Machado</name>
    </author>
    <author>
      <name>Michael Bowling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the paper presented at the workshop entitled
  Abstraction in Reinforcement Learning, at the 33rd International Conference
  on Machine Learning, New York, NY, USA, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07604v1</id>
    <updated>2016-05-24T19:58:02Z</updated>
    <published>2016-05-24T19:58:02Z</published>
    <title>Posterior Dispersion Indices</title>
    <summary>  Probabilistic modeling is cyclical: we specify a model, infer its posterior,
and evaluate its performance. Evaluation drives the cycle, as we revise our
model based on how it performs. This requires a metric. Traditionally,
predictive accuracy prevails. Yet, predictive accuracy does not tell the whole
story. We propose to evaluate a model through posterior dispersion. The idea is
to analyze how each datapoint fares in relation to posterior uncertainty around
the hidden structure. We propose a family of posterior dispersion indices (PDI)
that capture this idea. A PDI identifies rich patterns of model mismatch in
three real data examples: voting preferences, supermarket shopping, and
population genetics.
</summary>
    <author>
      <name>Alp Kucukelbir</name>
    </author>
    <author>
      <name>David M. Blei</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07574v1</id>
    <updated>2016-05-24T18:28:54Z</updated>
    <published>2016-05-24T18:28:54Z</published>
    <title>Towards Bin Packing (preliminary problem survey, models with multiset
  estimates)</title>
    <summary>  The paper described a generalized integrated glance to bin packing problems
including a brief literature survey and some new problem formulations for the
cases of multiset estimates of items. A new systemic viewpoint to bin packing
problems is suggested: (a) basic element sets (item set, bin set, item subset
assigned to bin), (b) binary relation over the sets: relation over item set as
compatibility, precedence, dominance; relation over items and bins (i.e.,
correspondence of items to bins). A special attention is targeted to the
following versions of bin packing problems: (a) problem with multiset estimates
of items, (b) problem with colored items (and some close problems). Applied
examples of bin packing problems are considered: (i) planning in paper industry
(framework of combinatorial problems), (ii) selection of information messages,
(iii) packing of messages/information packages in WiMAX communication system
(brief description).
</summary>
    <author>
      <name>Mark Sh. Levin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 18 figures, 14 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20, 93A13, 93B51, 90B50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; J.6; K.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07511v1</id>
    <updated>2016-05-24T15:50:26Z</updated>
    <published>2016-05-24T15:50:26Z</published>
    <title>A note on privacy preserving iteratively reweighted least squares</title>
    <summary>  Iteratively reweighted least squares (IRLS) is a widely-used method in
machine learning to estimate the parameters in the generalised linear models.
In particular, IRLS for L1 minimisation under the linear model provides a
closed-form solution in each step, which is a simple multiplication between the
inverse of the weighted second moment matrix and the weighted first moment
vector. When dealing with privacy sensitive data, however, developing a privacy
preserving IRLS algorithm faces two challenges. First, due to the inversion of
the second moment matrix, the usual sensitivity analysis in differential
privacy incorporating a single datapoint perturbation gets complicated and
often requires unrealistic assumptions. Second, due to its iterative nature, a
significant cumulative privacy loss occurs. However, adding a high level of
noise to compensate for the privacy loss hinders from getting accurate
estimates. Here, we develop a practical algorithm that overcomes these
challenges and outputs privatised and accurate IRLS solutions. In our method,
we analyse the sensitivity of each moments separately and treat the matrix
inversion and multiplication as a post-processing step, which simplifies the
sensitivity analysis. Furthermore, we apply the {\it{concentrated differential
privacy}} formalism, a more relaxed version of differential privacy, which
requires adding a significantly less amount of noise for the same level of
privacy guarantee, compared to the conventional and advanced compositions of
differentially private mechanisms.
</summary>
    <author>
      <name>Mijung Park</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07496v1</id>
    <updated>2016-05-24T15:15:57Z</updated>
    <published>2016-05-24T15:15:57Z</published>
    <title>Alternating Optimisation and Quadrature for Robust Reinforcement
  Learning</title>
    <summary>  Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables - state features that are
randomly determined by the environment in a physical setting but are
controllable in a simulator. This paper considers the problem of finding an
optimal policy while taking into account the impact of environment variables.
We present the alternating optimisation and quadrature algorithm which uses
Bayesian optimisation and Bayesian quadrature to address such settings and is
robust to the presence of significant rare events, which may not be observable
under random sampling but have a considerable impact on determining the optimal
policy. Our experimental results show that our approach learns better and
faster than existing methods.
</summary>
    <author>
      <name>Supratik Paul</name>
    </author>
    <author>
      <name>Kamil Ciosek</name>
    </author>
    <author>
      <name>Michael A. Osborne</name>
    </author>
    <author>
      <name>Shimon Whiteson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Excluding supplementary materials</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07364v1</id>
    <updated>2016-05-24T10:27:17Z</updated>
    <published>2016-05-24T10:27:17Z</published>
    <title>Non-Gaussian Random Generators in Bacteria Foraging Algorithm for
  Multiobjective Optimization</title>
    <summary>  Random generators or stochastic engines are a key component in the structure
of metaheuristic algorithms. This work investigates the effects of non-Gaussian
stochastic engines on the performance of metaheuristics when solving a
real-world optimization problem. In this work, the bacteria foraging algorithm
(BFA) was employed in tandem with four random generators (stochastic engines).
The stochastic engines operate using the Weibull distribution, Gamma
distribution, Gaussian distribution and a chaotic mechanism. The two
non-Gaussian distributions are the Weibull and Gamma distributions. In this
work, the approaches developed were implemented on the real-world
multi-objective resin bonded sand mould problem. The Pareto frontiers obtained
were benchmarked using two metrics; the hyper volume indicator (HVI) and the
proposed Average Explorative Rate (AER) metric. Detail discussions from various
perspectives on the effects of non-Gaussian random generators in metaheuristics
are provided.
</summary>
    <author>
      <name>Timothy Ganesan</name>
    </author>
    <author>
      <name>Pandian Vasant</name>
    </author>
    <author>
      <name>Irraivan Elamvazuthi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4172/2169-0316.1000182</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4172/2169-0316.1000182" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; 5 Figures; 6 Tables. Industrial Engineering &amp; Management,
  2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07335v3</id>
    <updated>2016-06-03T21:31:44Z</updated>
    <published>2016-05-24T08:49:36Z</published>
    <title>Differences between Industrial Models of Autonomy and Systemic Models of
  Autonomy</title>
    <summary>  This paper discusses the idea of levels of autonomy of systems - be this
technical or organic - and compares the insights with models employed by
industries used to describe maturity and capability of their products.
</summary>
    <author>
      <name>Aleksander Lodwich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07335v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07335v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07334v2</id>
    <updated>2016-07-11T06:47:19Z</updated>
    <published>2016-05-24T08:25:27Z</published>
    <title>Near-optimal Bayesian Active Learning with Correlated and Noisy Tests</title>
    <summary>  We consider the Bayesian active learning and experimental design problem,
where the goal is to learn the value of some unknown target variable through a
sequence of informative, noisy tests. In contrast to prior work, we focus on
the challenging, yet practically relevant setting where test outcomes can be
conditionally dependent given the hidden target variable. Under such
assumptions, common heuristics, such as greedily performing tests that maximize
the reduction in uncertainty of the target, often perform poorly. In this
paper, we propose ECED, a novel, computationally efficient active learning
algorithm, and prove strong theoretical guarantees that hold with correlated,
noisy tests. Rather than directly optimizing the prediction error, at each
step, ECED picks the test that maximizes the gain in a surrogate objective,
which takes into account the dependencies between tests. Our analysis relies on
an information-theoretic auxiliary function to track the progress of ECED, and
utilizes adaptive submodularity to attain the near-optimal bound. We
demonstrate strong empirical performance of ECED on two problem instances,
including a Bayesian experimental design task intended to distinguish among
economic theories of how people make risky decisions, and an active preference
learning task via pairwise comparisons.
</summary>
    <author>
      <name>Yuxin Chen</name>
    </author>
    <author>
      <name>S. Hamed Hassani</name>
    </author>
    <author>
      <name>Andreas Krause</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07334v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07334v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07260v1</id>
    <updated>2016-05-24T02:05:09Z</updated>
    <published>2016-05-24T02:05:09Z</published>
    <title>Diagnosing editorial strategies of Chilean media on Twitter using an
  automatic news classifier</title>
    <summary>  In Chile, does not exist an independent entity that publishes quantitative or
qualitative surveys to understand the traditional media environment and its
adaptation on the Social Web. Nowadays, Chilean newsreaders are increasingly
using social web platforms as their primary source of information, among which
Twitter plays a central role. Historical media and pure players are developing
different strategies to increase their audience and influence on this platform.
In this article, we propose a methodology based on data mining techniques to
provide a first level of analysis of the new Chilean media environment. We use
a crawling technique to mine news streams of 37 different Chilean media
actively presents on Twitter and propose several indicators to compare them. We
analyze their volumes of production, their potential audience, and using NLP
techniques, we explore the content of their production: their editorial line
and their geographic coverage.
</summary>
    <author>
      <name>Matthieu Vernier</name>
    </author>
    <author>
      <name>Luis Carcamo</name>
    </author>
    <author>
      <name>Eliana Scheihing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Spanish</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07246v3</id>
    <updated>2016-09-09T19:51:17Z</updated>
    <published>2016-05-24T00:48:28Z</published>
    <title>Adaptive ADMM with Spectral Penalty Parameter Selection</title>
    <summary>  The alternating direction method of multipliers (ADMM) is a versatile tool
for solving a wide range of constrained optimization problems, with
differentiable or non-differentiable objective functions. Unfortunately, its
performance is highly sensitive to a penalty parameter, which makes ADMM often
unreliable and hard to automate for a non-expert user. We tackle this weakness
of ADMM by proposing a method to adaptively tune the penalty parameters to
achieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm,
inspired by the successful Barzilai-Borwein spectral method for gradient
descent, yields fast convergence and relative insensitivity to the initial
stepsize and problem scaling.
</summary>
    <author>
      <name>Zheng Xu</name>
    </author>
    <author>
      <name>Mario A. T. Figueiredo</name>
    </author>
    <author>
      <name>Tom Goldstein</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07246v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07246v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07157v3</id>
    <updated>2016-06-09T00:29:37Z</updated>
    <published>2016-05-23T19:45:55Z</published>
    <title>Unsupervised Learning for Physical Interaction through Video Prediction</title>
    <summary>  A core challenge for an agent learning to interact with the world is to
predict how its actions affect objects in its environment. Many existing
methods for learning the dynamics of physical interactions require labeled
object information. However, to scale real-world interaction learning to a
variety of scenes and objects, acquiring labeled data becomes increasingly
impractical. To learn about physical object motion without labels, we develop
an action-conditioned video prediction model that explicitly models pixel
motion, by predicting a distribution over pixel motion from previous frames.
Because our model explicitly predicts motion, it is partially invariant to
object appearance, enabling it to generalize to previously unseen objects. To
explore video prediction for real-world interactive agents, we also introduce a
dataset of 50,000 robot interactions involving pushing motions, including a
test set with novel objects. In this dataset, accurate prediction of videos
conditioned on the robot's future actions amounts to learning a "visual
imagination" of different futures based on different courses of action. Our
experiments show that our proposed method not only produces more accurate video
predictions, but also more accurately predicts object motion, when compared to
prior methods.
</summary>
    <author>
      <name>Chelsea Finn</name>
    </author>
    <author>
      <name>Ian Goodfellow</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixing typo in supplementary materials URL. Correct URL is:
  http://www.sites.google.com/site/robotprediction</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07157v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07157v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07156v1</id>
    <updated>2016-05-23T19:43:08Z</updated>
    <published>2016-05-23T19:43:08Z</published>
    <title>Genetic Architect: Discovering Genomic Structure with Learned Neural
  Architectures</title>
    <summary>  Each human genome is a 3 billion base pair set of encoding instructions.
Decoding the genome using deep learning fundamentally differs from most tasks,
as we do not know the full structure of the data and therefore cannot design
architectures to suit it. As such, architectures that fit the structure of
genomics should be learned not prescribed. Here, we develop a novel search
algorithm, applicable across domains, that discovers an optimal architecture
which simultaneously learns general genomic patterns and identifies the most
important sequence motifs in predicting functional genomic outcomes. The
architectures we find using this algorithm succeed at using only RNA expression
data to predict gene regulatory structure, learn human-interpretable
visualizations of key sequence motifs, and surpass state-of-the-art results on
benchmark genomics challenges.
</summary>
    <author>
      <name>Laura Deming</name>
    </author>
    <author>
      <name>Sasha Targ</name>
    </author>
    <author>
      <name>Nate Sauder</name>
    </author>
    <author>
      <name>Diogo Almeida</name>
    </author>
    <author>
      <name>Chun Jimmie Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07148v2</id>
    <updated>2016-07-17T01:43:34Z</updated>
    <published>2016-05-23T19:28:21Z</published>
    <title>Backprop KF: Learning Discriminative Deterministic State Estimators</title>
    <summary>  Generative state estimators based on probabilistic filters and smoothers are
one of the most popular classes of state estimators for robots and autonomous
vehicles. However, generative models have limited capacity to handle rich
sensory observations, such as camera images, since they must model the entire
distribution over sensor readings. Discriminative models do not suffer from
this limitation, but are typically more complex to train as latent variable
models for state estimation. We present an alternative approach where the
parameters of the latent state distribution are directly optimized as a
deterministic computation graph, resulting in a simple and effective gradient
descent algorithm for training discriminative state estimators. We show that
this procedure can be used to train state estimators that use complex input,
such as raw camera images, which must be processed using expressive nonlinear
function approximators such as convolutional neural networks. Our model can be
viewed as a type of recurrent neural network, and the connection to
probabilistic filtering allows us to design a network architecture that is
particularly well suited for state estimation. We evaluate our approach on
synthetic tracking task with raw image inputs and on the visual odometry task
in the KITTI dataset. The results show significant improvement over both
standard generative approaches and regular recurrent neural networks.
</summary>
    <author>
      <name>Tuomas Haarnoja</name>
    </author>
    <author>
      <name>Anurag Ajay</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07079v1</id>
    <updated>2016-05-23T16:29:51Z</updated>
    <published>2016-05-23T16:29:51Z</published>
    <title>Fast Bayesian Optimization of Machine Learning Hyperparameters on Large
  Datasets</title>
    <summary>  Bayesian optimization has become a successful tool for hyperparameter
optimization of machine learning algorithms, such as support vector machines or
deep neural networks. But it is still costly if each evaluation of the
objective requires training and validating the algorithm being optimized,
which, for large datasets, often takes hours, days, or even weeks. To
accelerate hyperparameter optimization, we propose a generative model for the
validation error as a function of training set size, which is learned during
the optimization process and allows exploration of preliminary configurations
on small subsets, by extrapolating to the full dataset. We construct a Bayesian
optimization procedure, dubbed FABOLAS, which models loss and training time as
a function of dataset size and automatically trades off high information gain
about the global optimum against computational cost. Experiments optimizing
support vector machines and deep neural networks show that FABOLAS often finds
high-quality solutions 10 to 100 times faster than other state-of-the-art
Bayesian optimization methods.
</summary>
    <author>
      <name>Aaron Klein</name>
    </author>
    <author>
      <name>Stefan Falkner</name>
    </author>
    <author>
      <name>Simon Bartels</name>
    </author>
    <author>
      <name>Philipp Hennig</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07026v1</id>
    <updated>2016-05-23T14:21:30Z</updated>
    <published>2016-05-23T14:21:30Z</published>
    <title>Spontaneous vs. Posed smiles - can we tell the difference?</title>
    <summary>  Smile is an irrefutable expression that shows the physical state of the mind
in both true and deceptive ways. Generally, it shows happy state of the mind,
however, `smiles' can be deceptive, for example people can give a smile when
they feel happy and sometimes they might also give a smile (in a different way)
when they feel pity for others. This work aims to distinguish spontaneous
(felt) smile expressions from posed (deliberate) smiles by extracting and
analyzing both global (macro) motion of the face and subtle (micro) changes in
the facial expression features through both tracking a series of facial
fiducial markers as well as using dense optical flow. Specifically the eyes and
lips features are captured and used for analysis. It aims to automatically
classify all smiles into either `spontaneous' or `posed' categories, by using
support vector machines (SVM). Experimental results on large database show
promising results as compared to other relevant methods.
</summary>
    <author>
      <name>Bappaditya Mandal</name>
    </author>
    <author>
      <name>Nizar Ouarti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, 6 tables, International Conference on Computer
  Vision and Image Processing (CVIP 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06996v1</id>
    <updated>2016-05-23T12:37:54Z</updated>
    <published>2016-05-23T12:37:54Z</published>
    <title>Extracting Higher-Order Goals from the Mizar Mathematical Library</title>
    <summary>  Certain constructs allowed in Mizar articles cannot be represented in
first-order logic but can be represented in higher-order logic. We describe a
way to obtain higher-order theorem proving problems from Mizar articles that
make use of these constructs. In particular, higher-order logic is used to
represent schemes, a global choice construct and set level binders. The
higher-order automated theorem provers Satallax and LEO-II have been run on
collections of these problems and the results are discussed.
</summary>
    <author>
      <name>Chad Brown</name>
    </author>
    <author>
      <name>Josef Urban</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CICM 2016. The final publication will be available at
  Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06995v1</id>
    <updated>2016-05-23T12:36:55Z</updated>
    <published>2016-05-23T12:36:55Z</published>
    <title>Practical Privacy For Expectation Maximization</title>
    <summary>  Expectation maximization (EM) is an iterative algorithm that computes maximum
likelihood and maximum a posteriori estimates for models with unobserved
variables. While widely used, the iterative nature of EM presents challenges
for privacy-preserving estimation. Multiple iterations are required to obtain
accurate parameter estimates, yet each iteration increases the amount of noise
that must be added to achieve a reasonable degree of privacy. We propose a
practical algorithm that overcomes this challenge and outputs EM parameter
estimates that are both accurate and private. Our algorithm focuses on the
frequent use case of models whose joint distribution over observed and
unobserved variables remains in the exponential family. For these models, the
EM parameters are functions of moments of the data. Our algorithm leverages
this to preserve privacy by perturbing the moments, for which the amount of
additive noise scales naturally with the data. In addition, our algorithm uses
a relaxed notion of the differential privacy (DP) gold standard, called
concentrated differential privacy (CDP). Rather than focusing on single-query
loss, CDP provides high probability bounds for cumulative privacy loss, which
is well suited for iterative algorithms. For mixture models, we show that our
method requires a significantly smaller privacy budget for the same estimation
accuracy compared to both DP and its (epsilon, delta)-DP relaxation. Our
general approach of moment perturbation equipped with CDP can be readily
extended to many iterative machine learning algorithms, which opens up various
exciting future directions.
</summary>
    <author>
      <name>Mijung Park</name>
    </author>
    <author>
      <name>Jimmy Foulds</name>
    </author>
    <author>
      <name>Kamalika Chaudhuri</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06940v1</id>
    <updated>2016-05-23T08:54:27Z</updated>
    <published>2016-05-23T08:54:27Z</published>
    <title>Elastic Solver: Balancing Solution Time and Energy Consumption</title>
    <summary>  Combinatorial decision problems arise in many different domains such as
scheduling, routing, packing, bioinformatics, and many more. Despite recent
advances in developing scalable solvers, there are still many problems which
are often very hard to solve. Typically the most advanced solvers include
elements which are stochastic in nature. If a same instance is solved many
times using different seeds then depending on the inherent characteristics of a
problem instance and the solver, one can observe a highly-variant distribution
of times spanning multiple orders of magnitude. Therefore, to solve a problem
instance efficiently it is often useful to solve the same instance in parallel
with different seeds. With the proliferation of cloud computing, it is natural
to think about an elastic solver which can scale up by launching searches in
parallel on thousands of machines (or cores). However, this could result in
consuming a lot of energy. Moreover, not every instance would require thousands
of machines. The challenge is to resolve the tradeoff between solution time and
energy consumption optimally for a given problem instance. We analyse the
impact of the number of machines (or cores) on not only solution time but also
on energy consumption. We highlight that although solution time always drops as
the number of machines increases, the relation between the number of machines
and energy consumption is more complicated. In many cases, the optimal energy
consumption may be achieved by a middle ground, we analyse this relationship in
detail. The tradeoff between solution time and energy consumption is studied
further, showing that the energy consumption of a solver can be reduced
drastically if we increase the solution time marginally. We also develop a
prediction model, demonstrating that such insights can be exploited to achieve
faster solutions times in a more energy efficient manor.
</summary>
    <author>
      <name>Barry Hurley</name>
    </author>
    <author>
      <name>Deepak Mehta</name>
    </author>
    <author>
      <name>Barry O'Sullivan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Combinatorial Optimisation, Energy Minimisation, Parallel
  Solving</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06921v1</id>
    <updated>2016-05-23T07:36:49Z</updated>
    <published>2016-05-23T07:36:49Z</published>
    <title>Generative Choreography using Deep Learning</title>
    <summary>  Recent advances in deep learning have enabled the extraction of high-level
features from raw sensor data which has opened up new possibilities in many
different fields, including computer generated choreography. In this paper we
present a system chor-rnn for generating novel choreographic material in the
nuanced choreographic language and style of an individual choreographer. It
also shows promising results in producing a higher level compositional
cohesion, rather than just generating sequences of movement. At the core of
chor-rnn is a deep recurrent neural network trained on raw motion capture data
and that can generate new dance sequences for a solo dancer. Chor-rnn can be
used for collaborative human-machine choreography or as a creative catalyst,
serving as inspiration for a choreographer.
</summary>
    <author>
      <name>Luka Crnkovic-Friis</name>
    </author>
    <author>
      <name>Louise Crnkovic-Friis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article will be presented at the 7th International Conference on
  Computational Creativity, ICCC2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06886v1</id>
    <updated>2016-05-23T03:43:01Z</updated>
    <published>2016-05-23T03:43:01Z</published>
    <title>Stochastic Patching Process</title>
    <summary>  Stochastic partition models tailor a product space into a number of
rectangular regions such that the data within each region exhibit certain types
of homogeneity. Due to constraints of partition strategy, existing models may
cause unnecessary dissections in sparse regions when fitting data in dense
regions. To alleviate this limitation, we propose a parsimonious partition
model, named Stochastic Patching Process (SPP), to deal with multi-dimensional
arrays. SPP adopts an "enclosing" strategy to attach rectangular patches to
dense regions. SPP is self-consistent such that it can be extended to infinite
arrays. We apply SPP to relational modeling and the experimental results
validate its merit compared to the state-of-the-arts.
</summary>
    <author>
      <name>Xuhui Fan</name>
    </author>
    <author>
      <name>Bin Li</name>
    </author>
    <author>
      <name>Yi Wang</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Fang Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06838v1</id>
    <updated>2016-05-22T19:28:25Z</updated>
    <published>2016-05-22T19:28:25Z</published>
    <title>Causality on Longitudinal Data: Stable Specification Search in
  Constrained Structural Equation Modeling</title>
    <summary>  Developing causal models from observational longitudinal studies is an
important, ubiquitous problem in many disciplines. In the medical domain,
especially in the case of rare diseases, revealing causal relationships from a
given data set may lead to improvement of clinical practice, e.g., development
of treatment and medication. Many causal discovery methods have been introduced
in the past decades. A disadvantage of these causal discovery algorithms,
however, is the inherent instability in structure estimation. With finite data
samples small changes in the data can lead to completely different optimal
structures. The present work presents a new causal discovery algorithm for
longitudinal data that is robust for finite data samples. The method works as
follows. We model causal models using structural equation models. Models are
scored along two objectives: the model fit and the model complexity. Since both
objectives are often conflicting we use a multi-objective evolutionary
algorithm to search for Pareto optimal models. To handle the instability of
small finite data samples, we repeatedly subsample the data and select those
substructures (from optimal models) that are both stable and parsimonious which
are then used to infer a causal model. In order to validate, we compare our
method with the state-of-the-art PC algorithm on a simulated data set with the
known ground truth model. Furthermore, we present the results of our discovery
algorithm on three real-world longitudinal data sets about chronic fatigue
syndrome, Alzheimer disease and chronic kidney disease that have been
corroborated by medical experts and literature.
</summary>
    <author>
      <name>Ridho Rahmadi</name>
    </author>
    <author>
      <name>Perry Groot</name>
    </author>
    <author>
      <name>Marieke MHC van Rijn</name>
    </author>
    <author>
      <name>Jan AJG van den Brand</name>
    </author>
    <author>
      <name>Marianne Heins</name>
    </author>
    <author>
      <name>Hans Knoop</name>
    </author>
    <author>
      <name>Tom Heskes</name>
    </author>
    <author>
      <name>the Alzheimer's Disease Neuroimaging Initiatives</name>
    </author>
    <author>
      <name>the MASTERPLAN Study Group</name>
    </author>
    <author>
      <name>the OPTIMISTIC Consortium</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1506.05600</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06676v2</id>
    <updated>2016-05-24T18:16:56Z</updated>
    <published>2016-05-21T17:20:04Z</published>
    <title>Learning to Communicate with Deep Multi-Agent Reinforcement Learning</title>
    <summary>  We consider the problem of multiple agents sensing and acting in environments
with the goal of maximising their shared utility. In these environments, agents
must learn communication protocols in order to share information that is needed
to solve the tasks. By embracing deep neural networks, we are able to
demonstrate end-to-end learning of protocols in complex environments inspired
by communication riddles and multi-agent computer vision problems with partial
observability. We propose two approaches for learning in these domains:
Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning
(DIAL). The former uses deep Q-learning, while the latter exploits the fact
that, during learning, agents can backpropagate error derivatives through
(noisy) communication channels. Hence, this approach uses centralised learning
but decentralised execution. Our experiments introduce new environments for
studying the learning of communication protocols and present a set of
engineering innovations that are essential for success in these domains.
</summary>
    <author>
      <name>Jakob N. Foerster</name>
    </author>
    <author>
      <name>Yannis M. Assael</name>
    </author>
    <author>
      <name>Nando de Freitas</name>
    </author>
    <author>
      <name>Shimon Whiteson</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06676v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06676v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06640v1</id>
    <updated>2016-05-21T13:24:14Z</updated>
    <published>2016-05-21T13:24:14Z</published>
    <title>Programming with a Differentiable Forth Interpreter</title>
    <summary>  There are families of neural networks that can learn to compute any function,
provided sufficient training data. However, given that in practice training
data is scarce for all but a small set of problems, a core question is how to
incorporate prior knowledge into a model. Here we consider the case of prior
procedural knowledge such as knowing the overall recursive structure of a
sequence transduction program or the fact that a program will likely use
arithmetic operations on real numbers to solve a task. To this end we present a
differentiable interpreter for the programming language Forth. Through a neural
implementation of the dual stack machine that underlies Forth, programmers can
write program sketches with slots that can be filled with learnable behaviour.
As the program interpreter is end-to-end differentiable, we can optimize this
behaviour directly through gradient descent techniques on user specified
objectives, and also integrate the program into any larger neural computation
graph. We show empirically that our interpreter is able to effectively leverage
different levels of prior program structure and learn complex transduction
tasks such as sequence sorting or addition with substantially less data and
better generalisation over problem sizes. In addition, we introduce neural
program optimisations based on symbolic computation and parallel branching that
lead to significant speed improvements.
</summary>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <author>
      <name>Matko Bošnjak</name>
    </author>
    <author>
      <name>Tim Rocktäschel</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06588v2</id>
    <updated>2016-05-30T20:38:30Z</updated>
    <published>2016-05-21T05:09:11Z</published>
    <title>Optimal Number of Choices in Rating Contexts</title>
    <summary>  In many settings people must give numerical scores to entities from a small
discrete set. For instance, rating physical attractiveness from 1-5 on dating
sites, or papers from 1-10 for conference reviewing. We study the problem of
understanding when using a different number of options is optimal. For
concreteness we assume the true underlying scores are integers from 1-100. We
consider the case when scores are uniform random and Gaussian. We study when
using 2, 3, 4, 5, and 10 options is optimal in these models. One may expect
that using more options would always improve performance in this model, but we
show that this is not necessarily the case, and that using fewer choices --
even just two -- can surprisingly be optimal in certain situations. While in
theory for this setting it would be optimal to use all 100 options, in practice
this is prohibitive, and it is preferable to utilize a smaller number of
options due to humans' limited computational resources. Our results suggest
that using a smaller number of options than is typical could be optimal in
certain situations. This would have many potential applications, as settings
requiring entities to be ranked by humans are ubiquitous.
</summary>
    <author>
      <name>Sam Ganzfried</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06523v2</id>
    <updated>2016-07-19T21:03:55Z</updated>
    <published>2016-05-20T20:10:46Z</published>
    <title>TensorLog: A Differentiable Deductive Database</title>
    <summary>  Large knowledge bases (KBs) are useful in many tasks, but it is unclear how
to integrate this sort of knowledge into "deep" gradient-based learning
systems. To address this problem, we describe a probabilistic deductive
database, called TensorLog, in which reasoning uses a differentiable process.
In TensorLog, each clause in a logical theory is first converted into certain
type of factor graph. Then, for each type of query to the factor graph, the
message-passing steps required to perform belief propagation (BP) are
"unrolled" into a function, which is differentiable. We show that these
functions can be composed recursively to perform inference in non-trivial
logical theories containing multiple interrelated clauses and predicates. Both
compilation and inference in TensorLog are efficient: compilation is linear in
theory size and proof depth, and inference is linear in database size and the
number of message-passing steps used in BP. We also present experimental
results with TensorLog and discuss its relationship to other first-order
probabilistic logics.
</summary>
    <author>
      <name>William W. Cohen</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06523v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06523v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06466v1</id>
    <updated>2016-05-20T18:43:44Z</updated>
    <published>2016-05-20T18:43:44Z</published>
    <title>Anomaly Detection in XML-Structured SOAP Messages Using Tree-Based
  Association Rule Mining</title>
    <summary>  Web services are software systems designed for supporting interoperable
dynamic cross-enterprise interactions. The result of attacks to Web services
can be catastrophic and causing the disclosure of enterprises' confidential
data. As new approaches of attacking arise every day, anomaly detection systems
seem to be invaluable tools in this context. The aim of this work has been to
target the attacks that reside in the Web service layer and the extensible
markup language (XML)-structured simple object access protocol (SOAP) messages.
After studying the shortcomings of the existing solutions, a new approach for
detecting anomalies in Web services is outlined. More specifically, the
proposed technique illustrates how to identify anomalies by employing mining
methods on XML-structured SOAP messages. This technique also takes the
advantages of tree-based association rule mining to extract knowledge in the
training phase, which is used in the test phase to detect anomalies. In
addition, this novel composition of techniques brings nearly low false alarm
rate while maintaining the detection rate reasonably high, which is shown by a
case study.
</summary>
    <author>
      <name>Reyhaneh Ghassem Esfahani</name>
    </author>
    <author>
      <name>Mohammad Abadollahi Azgomi</name>
    </author>
    <author>
      <name>Reza Fathi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Trustworthy Computing Laboratory, School of Computer Engineering,
  Iran University of Science and Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06450v1</id>
    <updated>2016-05-20T17:40:16Z</updated>
    <published>2016-05-20T17:40:16Z</published>
    <title>Query-Efficient Imitation Learning for End-to-End Autonomous Driving</title>
    <summary>  One way to approach end-to-end autonomous driving is to learn a policy
function that maps from a sensory input, such as an image frame from a
front-facing camera, to a driving action, by imitating an expert driver, or a
reference policy. This can be done by supervised learning, where a policy
function is tuned to minimize the difference between the predicted and
ground-truth actions. A policy function trained in this way however is known to
suffer from unexpected behaviours due to the mismatch between the states
reachable by the reference policy and trained policy functions. More advanced
algorithms for imitation learning, such as DAgger, addresses this issue by
iteratively collecting training examples from both reference and trained
policies. These algorithms often requires a large number of queries to a
reference policy, which is undesirable as the reference policy is often
expensive. In this paper, we propose an extension of the DAgger, called
SafeDAgger, that is query-efficient and more suitable for end-to-end autonomous
driving. We evaluate the proposed SafeDAgger in a car racing simulator and show
that it indeed requires less queries to a reference policy. We observe a
significant speed up in convergence, which we conjecture to be due to the
effect of automated curriculum learning.
</summary>
    <author>
      <name>Jiakai Zhang</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06431v1</id>
    <updated>2016-05-20T16:44:03Z</updated>
    <published>2016-05-20T16:44:03Z</published>
    <title>Residual Networks are Exponential Ensembles of Relatively Shallow
  Networks</title>
    <summary>  In this work, we introduce a novel interpretation of residual networks
showing they are exponential ensembles. This observation is supported by a
large-scale lesion study that demonstrates they behave just like ensembles at
test time. Subsequently, we perform an analysis showing these ensembles mostly
consist of networks that are each relatively shallow. For example, contrary to
our expectations, most of the gradient in a residual network with 110 layers
comes from an ensemble of very short networks, i.e., only 10-34 layers deep.
This suggests that in addition to describing neural networks in terms of width
and depth, there is a third dimension: multiplicity, the size of the implicit
ensemble. Ultimately, residual networks do not resolve the vanishing gradient
problem by preserving gradient flow throughout the entire depth of the network
- rather, they avoid the problem simply by ensembling many short networks
together. This insight reveals that depth is still an open research question
and invites the exploration of the related notion of multiplicity.
</summary>
    <author>
      <name>Andreas Veit</name>
    </author>
    <author>
      <name>Michael Wilber</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06377v1</id>
    <updated>2016-05-20T14:34:49Z</updated>
    <published>2016-05-20T14:34:49Z</published>
    <title>Towards Automation of Knowledge Understanding: An Approach for
  Probabilistic Generative Classifiers</title>
    <summary>  After data selection, pre-processing, transformation, and feature extraction,
knowledge extraction is not the final step in a data mining process. It is then
necessary to understand this knowledge in order to apply it efficiently and
effectively. Up to now, there is a lack of appropriate techniques that support
this significant step. This is partly due to the fact that the assessment of
knowledge is often highly subjective, e.g., regarding aspects such as novelty
or usefulness. These aspects depend on the specific knowledge and requirements
of the data miner. There are, however, a number of aspects that are objective
and for which it is possible to provide appropriate measures. In this article
we focus on classification problems and use probabilistic generative
classifiers based on mixture density models that are quite common in data
mining applications. We define objective measures to assess the
informativeness, uniqueness, importance, discrimination, representativity,
uncertainty, and distinguishability of rules contained in these classifiers
numerically. These measures not only support a data miner in evaluating results
of a data mining process based on such classifiers. As we will see in
illustrative case studies, they may also be used to improve the data mining
process itself or to support the later application of the extracted knowledge.
</summary>
    <author>
      <name>Dominik Fisch</name>
    </author>
    <author>
      <name>Christian Gruhl</name>
    </author>
    <author>
      <name>Edgar Kalkowski</name>
    </author>
    <author>
      <name>Bernhard Sick</name>
    </author>
    <author>
      <name>Seppo J. Ovaska</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages with 9 figures and 4 tables. Currently under review for
  Information Sciences</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06319v1</id>
    <updated>2016-05-20T12:20:27Z</updated>
    <published>2016-05-20T12:20:27Z</published>
    <title>As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in
  Serbian</title>
    <summary>  Similes are natural language expressions used to compare unlikely things,
where the comparison is not taken literally. They are often used in everyday
communication and are an important part of cultural heritage. Having an
up-to-date corpus of similes is challenging, as they are constantly coined
and/or adapted to the contemporary times. In this paper we present a
methodology for semi-automated collection of similes from the world wide web
using text mining techniques. We expanded an existing corpus of traditional
similes (containing 333 similes) by collecting 446 additional expressions. We,
also, explore how crowdsourcing can be used to extract and curate new similes.
</summary>
    <author>
      <name>Nikola Milosevic</name>
    </author>
    <author>
      <name>Goran Nenadic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Phrase modelling, simile extraction, language resource building,
  crowdsourcing</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06201v1</id>
    <updated>2016-05-20T02:55:59Z</updated>
    <published>2016-05-20T02:55:59Z</published>
    <title>Adversarial Delays in Online Strongly-Convex Optimization</title>
    <summary>  We consider the problem of strongly-convex online optimization in presence of
adversarial delays; in a T-iteration online game, the feedback of the player's
query at time t is arbitrarily delayed by an adversary for d_t rounds and
delivered before the game ends, at iteration t+d_t-1. Specifically for
\algo{online-gradient-descent} algorithm we show it has a simple regret bound
of \Oh{\sum_{t=1}^T \log (1+ \frac{d_t}{t})}. This gives a clear and simple
bound without resorting any distributional and limiting assumptions on the
delays. We further show how this result encompasses and generalizes several of
the existing known results in the literature. Specifically it matches the
celebrated logarithmic regret \Oh{\log T} when there are no delays (i.e. d_t =
1) and regret bound of \Oh{\tau \log T} for constant delays d_t = \tau.
</summary>
    <author>
      <name>Daniel Khashabi</name>
    </author>
    <author>
      <name>Kent Quanrud</name>
    </author>
    <author>
      <name>Amirhossein Taghvaei</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06181v1</id>
    <updated>2016-05-20T00:31:07Z</updated>
    <published>2016-05-20T00:31:07Z</published>
    <title>Variational hybridization and transformation for large inaccurate
  noisy-or networks</title>
    <summary>  Variational inference provides approximations to the computationally
intractable posterior distribution in Bayesian networks. A prominent medical
application of noisy-or Bayesian network is to infer potential diseases given
observed symptoms. Previous studies focus on approximating a handful of
complicated pathological cases using variational transformation. Our goal is to
use variational transformation as part of a novel hybridized inference for
serving reliable and real time diagnosis at web scale. We propose a hybridized
inference that allows variational parameters to be estimated without disease
posteriors or priors, making the inference faster and much of its computation
recyclable. In addition, we propose a transformation ranking algorithm that is
very stable to large variances in network prior probabilities, a common issue
that arises in medical applications of Bayesian networks. In experiments, we
perform comparative study on a large real life medical network and scalability
study on a much larger (36,000x) synthesized network.
</summary>
    <author>
      <name>Yusheng Xie</name>
    </author>
    <author>
      <name>Nan Du</name>
    </author>
    <author>
      <name>Wei Fan</name>
    </author>
    <author>
      <name>Jing Zhai</name>
    </author>
    <author>
      <name>Weicheng Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1605.06181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06069v3</id>
    <updated>2016-06-14T02:21:04Z</updated>
    <published>2016-05-19T17:59:02Z</published>
    <title>A Hierarchical Latent Variable Encoder-Decoder Model for Generating
  Dialogues</title>
    <summary>  Sequential data often possesses a hierarchical structure with complex
dependencies between subsequences, such as found between the utterances in a
dialogue. In an effort to model this kind of generative process, we propose a
neural network-based generative architecture, with latent stochastic variables
that span a variable number of time steps. We apply the proposed model to the
task of dialogue response generation and compare it with recent neural network
architectures. We evaluate the model performance through automatic evaluation
metrics and by carrying out a human evaluation. The experiments demonstrate
that our model improves upon recently proposed models and that the latent
variables facilitate the generation of long outputs and maintain the context.
</summary>
    <author>
      <name>Iulian Vlad Serban</name>
    </author>
    <author>
      <name>Alessandro Sordoni</name>
    </author>
    <author>
      <name>Ryan Lowe</name>
    </author>
    <author>
      <name>Laurent Charlin</name>
    </author>
    <author>
      <name>Joelle Pineau</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 tables, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06069v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06069v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06048v1</id>
    <updated>2016-05-19T16:45:12Z</updated>
    <published>2016-05-19T16:45:12Z</published>
    <title>Philosophy in the Face of Artificial Intelligence</title>
    <summary>  In this article, I discuss how the AI community views concerns about the
emergence of superintelligent AI and related philosophical issues.
</summary>
    <author>
      <name>Vincent Conitzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Prospect, May 4, 2016.
  http://www.prospectmagazine.co.uk/science-and-technology/artificial-intelligence-wheres-the-philosophical-scrutiny</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06047v1</id>
    <updated>2016-05-19T16:41:00Z</updated>
    <published>2016-05-19T16:41:00Z</published>
    <title>AMSOM: Adaptive Moving Self-organizing Map for Clustering and
  Visualization</title>
    <summary>  Self-Organizing Map (SOM) is a neural network model which is used to obtain a
topology-preserving mapping from the (usually high dimensional) input/feature
space to an output/map space of fewer dimensions (usually two or three in order
to facilitate visualization). Neurons in the output space are connected with
each other but this structure remains fixed throughout training and learning is
achieved through the updating of neuron reference vectors in feature space.
Despite the fact that growing variants of SOM overcome the fixed structure
limitation they increase computational cost and also do not allow the removal
of a neuron after its introduction. In this paper, a variant of SOM is proposed
called AMSOM (Adaptive Moving Self-Organizing Map) that on the one hand creates
a more flexible structure where neuron positions are dynamically altered during
training and on the other hand tackles the drawback of having a predefined grid
by allowing neuron addition and/or removal during training. Experiments using
multiple literature datasets show that the proposed method improves training
performance of SOM, leads to a better visualization of the input dataset and
provides a framework for determining the optimal number and structure of
neurons.
</summary>
    <author>
      <name>Gerasimos Spanakis</name>
    </author>
    <author>
      <name>Gerhard Weiss</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0005704801290140</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0005704801290140" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICAART 2016 accepted full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05966v1</id>
    <updated>2016-05-19T14:16:39Z</updated>
    <published>2016-05-19T14:16:39Z</published>
    <title>Dynamic Bayesian Networks to simulate occupant behaviours in office
  buildings related to indoor air quality</title>
    <summary>  This paper proposes a new general approach based on Bayesian networks to
model the human behaviour. This approach represents human behaviour with
probabilistic cause-effect relations based on knowledge, but also with
conditional probabilities coming either from knowledge or deduced from
observations. This approach has been applied to the co-simulation of the CO2
concentration in an office coupled with human behaviour.
</summary>
    <author>
      <name>Khadija Tijani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CSTB, LIG Laboratoire d'Informatique de Grenoble, G-SCOP</arxiv:affiliation>
    </author>
    <author>
      <name>Stephane Ploix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">G-SCOP</arxiv:affiliation>
    </author>
    <author>
      <name>Benjamin Haas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CSTB</arxiv:affiliation>
    </author>
    <author>
      <name>Julie Dugdale</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIG Laboratoire d'Informatique de Grenoble</arxiv:affiliation>
    </author>
    <author>
      <name>Quoc Dung Ngo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IBPSA India 2015, Dec 2015, Hyderabad, India. arXiv admin note:
  substantial text overlap with arXiv:1510.01970</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05950v1</id>
    <updated>2016-05-19T13:40:01Z</updated>
    <published>2016-05-19T13:40:01Z</published>
    <title>Interactive Debugging of Knowledge Bases</title>
    <summary>  Many AI applications rely on knowledge about a relevant real-world domain
that is encoded by means of some logical knowledge base (KB). The most
essential benefit of logical KBs is the opportunity to perform automatic
reasoning to derive implicit knowledge or to answer complex queries about the
modeled domain. The feasibility of meaningful reasoning requires KBs to meet
some minimal quality criteria such as logical consistency. Without adequate
tool assistance, the task of resolving violated quality criteria in KBs can be
extremely tough even for domain experts, especially when the problematic KB
includes a large number of logical formulas or comprises complicated logical
formalisms.
  Published non-interactive debugging systems often cannot localize all
possible faults (incompleteness), suggest the deletion or modification of
unnecessarily large parts of the KB (non-minimality), return incorrect
solutions which lead to a repaired KB not satisfying the imposed quality
requirements (unsoundness) or suffer from poor scalability due to the inherent
complexity of the KB debugging problem. Even if a system is complete and sound
and considers only minimal solutions, there are generally exponentially many
solution candidates to select one from. However, any two repaired KBs obtained
from these candidates differ in their semantics in terms of entailments and
non-entailments. Selection of just any of these repaired KBs might result in
unexpected entailments, the loss of desired entailments or unwanted changes to
the KB.
  This work proposes complete, sound and optimal methods for the interactive
debugging of KBs that suggest the one (minimally invasive) error correction of
the faulty KB that yields a repaired KB with exactly the intended semantics.
Users, e.g. domain experts, are involved in the debugging process by answering
automatically generated queries about the intended domain.
</summary>
    <author>
      <name>Patrick Rodler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph.D. Thesis, Alpen-Adria Universit\"at Klagenfurt</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4; I.2.8; I.2.3; I.2.6; D.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05807v2</id>
    <updated>2016-05-22T23:02:35Z</updated>
    <published>2016-05-19T04:22:35Z</published>
    <title>Heuristics for Planning, Plan Recognition and Parsing</title>
    <summary>  In a recent paper, we have shown that Plan Recognition over STRIPS can be
formulated and solved using Classical Planning heuristics and algorithms. In
this work, we show that this formulation subsumes the standard formulation of
Plan Recognition over libraries through a compilation of libraries into STRIPS
theories. The libraries correspond to AND/OR graphs that may be cyclic and
where children of AND nodes may be partially ordered. These libraries include
Context-Free Grammars as a special case, where the Plan Recognition problem
becomes a parsing with missing tokens problem. Plan Recognition over the
standard libraries become Planning problems that can be easily solved by any
modern planner, while recognition over more complex libraries, including
Context-Free Grammars (CFGs), illustrate limitations of current Planning
heuristics and suggest improvements that may be relevant in other Planning
problems too.
</summary>
    <author>
      <name>Miquel Ramirez</name>
    </author>
    <author>
      <name>Hector Geffner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Written: June 2009, Published: May 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05711v1</id>
    <updated>2016-05-18T19:23:43Z</updated>
    <published>2016-05-18T19:23:43Z</published>
    <title>The Information-Collecting Vehicle Routing Problem: Stochastic
  Optimization for Emergency Storm Response</title>
    <summary>  Utilities face the challenge of responding to power outages due to storms and
ice damage, but most power grids are not equipped with sensors to pinpoint the
precise location of the faults causing the outage. Instead, utilities have to
depend primarily on phone calls (trouble calls) from customers who have lost
power to guide the dispatching of utility trucks. In this paper, we develop a
policy that routes a utility truck to restore outages in the power grid as
quickly as possible, using phone calls to create beliefs about outages, but
also using utility trucks as a mechanism for collecting additional information.
This means that routing decisions change not only the physical state of the
truck (as it moves from one location to another) and the grid (as the truck
performs repairs), but also our belief about the network, creating the first
stochastic vehicle routing problem that explicitly models information
collection and belief modeling. We address the problem of managing a single
utility truck, which we start by formulating as a sequential stochastic
optimization model which captures our belief about the state of the grid. We
propose a stochastic lookahead policy, and use Monte Carlo tree search (MCTS)
to produce a practical policy that is asymptotically optimal. Simulation
results show that the developed policy restores the power grid much faster
compared to standard industry heuristics.
</summary>
    <author>
      <name>Lina Al-Kanj</name>
    </author>
    <author>
      <name>Warren B. Powell</name>
    </author>
    <author>
      <name>Belgacem Bouzaiene-Ayari</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05676v1</id>
    <updated>2016-05-18T18:18:38Z</updated>
    <published>2016-05-18T18:18:38Z</published>
    <title>Towards information based spatiotemporal patterns as a foundation for
  agent representation in dynamical systems</title>
    <summary>  We present some arguments why existing methods for representing agents fall
short in applications crucial to artificial life. Using a thought experiment
involving a fictitious dynamical systems model of the biosphere we argue that
the metabolism, motility, and the concept of counterfactual variation should be
compatible with any agent representation in dynamical systems. We then propose
an information-theoretic notion of \emph{integrated spatiotemporal patterns}
which we believe can serve as the basic building block of an agent definition.
We argue that these patterns are capable of solving the problems mentioned
before. We also test this in some preliminary experiments.
</summary>
    <author>
      <name>Martin Biehl</name>
    </author>
    <author>
      <name>Takashi Ikegami</name>
    </author>
    <author>
      <name>Daniel Polani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.2.11; I.5.1; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05448v1</id>
    <updated>2016-05-18T05:53:44Z</updated>
    <published>2016-05-18T05:53:44Z</published>
    <title>The Bees Algorithm for the Vehicle Routing Problem</title>
    <summary>  In this thesis we present a new algorithm for the Vehicle Routing Problem
called the Enhanced Bees Algorithm. It is adapted from a fairly recent
algorithm, the Bees Algorithm, which was developed for continuous optimisation
problems. We show that the results obtained by the Enhanced Bees Algorithm are
competitive with the best meta-heuristics available for the Vehicle Routing
Problem (within 0.5% of the optimal solution for common benchmark problems). We
show that the algorithm has good runtime performance, producing results within
2% of the optimal solution within 60 seconds, making it suitable for use within
real world dispatch scenarios.
</summary>
    <author>
      <name>Aish Fenton</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05433v1</id>
    <updated>2016-05-18T04:10:41Z</updated>
    <published>2016-05-18T04:10:41Z</published>
    <title>Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns
  in Distributional Vectors for Lexical Entailment</title>
    <summary>  We consider the task of predicting lexical entailment using distributional
vectors. We focus experiments on one previous classifier which was shown to
only learn to detect prototypicality of a word pair. Analysis shows that the
model single-mindedly learns to detect Hearst Patterns, which are well known to
be predictive of lexical relations. We present a new model which exploits this
Hearst Detector functionality, matching or outperforming prior work on multiple
data sets.
</summary>
    <author>
      <name>Stephen Roller</name>
    </author>
    <author>
      <name>Katrin Erk</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05365v2</id>
    <updated>2016-06-11T01:04:13Z</updated>
    <published>2016-05-17T20:58:41Z</published>
    <title>Dynamic Frame skip Deep Q Network</title>
    <summary>  Deep Reinforcement Learning methods have achieved state of the art
performance in learning control policies for the games in the Atari 2600
domain. One of the important parameters in the Arcade Learning Environment
(ALE) is the frame skip rate. It decides the granularity at which agents can
control game play. A frame skip value of $k$ allows the agent to repeat a
selected action $k$ number of times. The current state of the art architectures
like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of
a framework with a static frame skip rate, where the action output from the
network is repeated for a fixed number of frames regardless of the current
state. In this paper, we propose a new architecture, Dynamic Frame skip Deep
Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable
parameter. This allows us to choose the number of times an action is to be
repeated based on the current state. We show empirically that such a setting
improves the performance on relatively harder games like Seaquest.
</summary>
    <author>
      <name>Aravind S. Lakshminarayanan</name>
    </author>
    <author>
      <name>Sahil Sharma</name>
    </author>
    <author>
      <name>Balaraman Ravindran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2016 Workshop on Deep Reinforcement Learning: Frontiers and
  Challenges; 6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05365v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05365v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05359v1</id>
    <updated>2016-05-17T20:44:19Z</updated>
    <published>2016-05-17T20:44:19Z</published>
    <title>Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions
  and Deep Neural Networks</title>
    <summary>  This paper introduces an automated skill acquisition framework in
reinforcement learning which involves identifying a hierarchical description of
the given task in terms of abstract states and extended actions between
abstract states. Identifying such structures present in the task provides ways
to simplify and speed up reinforcement learning learning algorithms. These
structures also help to generalize such algorithms over multiple tasks without
relearning policies from scratch. We use ideas from dynamical systems to find
metastable regions in the state space and associate them with abstract states.
The spectral clustering algorithm PCCA+ is used to identify suitable
abstractions aligned to the underlying structure. Skills are defined in terms
of the transitions between such abstract states. The connectivity information
from PCCA+ is used to generate these skills or options. The skills are
independent of the learning task and can be efficiently reused across a variety
of tasks defined over a common state space. Another major advantage of the
approach is that it does not need a prior model of the MDP and can work well
even when the MDPs are constructed from sampled trajectories. Finally, we
present our attempts to extend the automated skills acquisition framework to
complex tasks such as learning to play video games where we use deep learning
techniques for representation learning to aid our spatio-temporal abstraction
framework.
</summary>
    <author>
      <name>Ramnandan Krishnamurthy</name>
    </author>
    <author>
      <name>Aravind S. Lakshminarayanan</name>
    </author>
    <author>
      <name>Peeyush Kumar</name>
    </author>
    <author>
      <name>Balaraman Ravindran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05305v1</id>
    <updated>2016-05-17T19:47:13Z</updated>
    <published>2016-05-17T19:47:13Z</published>
    <title>Combat Models for RTS Games</title>
    <summary>  Game tree search algorithms, such as Monte Carlo Tree Search (MCTS), require
access to a forward model (or "simulator") of the game at hand. However, in
some games such forward model is not readily available. This paper presents
three forward models for two-player attrition games, which we call "combat
models", and show how they can be used to simulate combat in RTS games. We also
show how these combat models can be learned from replay data. We use StarCraft
as our application domain. We report experiments comparing our combat models
predicting a combat output and their impact when used for tactical decisions
during a real game.
</summary>
    <author>
      <name>Alberto Uriarte</name>
    </author>
    <author>
      <name>Santiago Ontañón</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05303v1</id>
    <updated>2016-05-17T19:45:49Z</updated>
    <published>2016-05-17T19:45:49Z</published>
    <title>Fuzzy Sets Across the Natural Language Generation Pipeline</title>
    <summary>  We explore the implications of using fuzzy techniques (mainly those commonly
used in the linguistic description/summarization of data discipline) from a
natural language generation perspective. For this, we provide an extensive
discussion of some general convergence points and an exploration of the
relationship between the different tasks involved in the standard NLG system
pipeline architecture and the most common fuzzy approaches used in linguistic
summarization/description of data, such as fuzzy quantified statements,
evaluation criteria or aggregation operators. Each individual discussion is
illustrated with a related use case. Recent work made in the context of
cross-fertilization of both research fields is also referenced. This paper
encompasses general ideas that emerged as part of the PhD thesis "Application
of fuzzy sets in data-to-text systems". It does not present a specific
application or a formal approach, but rather discusses current high-level
issues and potential usages of fuzzy sets (focused on linguistic summarization
of data) in natural language generation.
</summary>
    <author>
      <name>A. Ramos-Soto</name>
    </author>
    <author>
      <name>A. Bugarín</name>
    </author>
    <author>
      <name>S. Barro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper features: 16 pages, 2 tables, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05273v4</id>
    <updated>2016-06-08T11:40:13Z</updated>
    <published>2016-05-17T18:13:13Z</published>
    <title>Learning Convolutional Neural Networks for Graphs</title>
    <summary>  Numerous important problems can be framed as learning from graph data. We
propose a framework for learning convolutional neural networks for arbitrary
graphs. These graphs may be undirected, directed, and with both discrete and
continuous node and edge attributes. Analogous to image-based convolutional
networks that operate on locally connected regions of the input, we present a
general approach to extracting locally connected regions from graphs. Using
established benchmark data sets, we demonstrate that the learned feature
representations are competitive with state of the art graph kernels and that
their computation is highly efficient.
</summary>
    <author>
      <name>Mathias Niepert</name>
    </author>
    <author>
      <name>Mohamed Ahmed</name>
    </author>
    <author>
      <name>Konstantin Kutzkov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05273v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05273v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05195v1</id>
    <updated>2016-05-17T14:51:54Z</updated>
    <published>2016-05-17T14:51:54Z</published>
    <title>Enhanced Twitter Sentiment Classification Using Contextual Information</title>
    <summary>  The rise in popularity and ubiquity of Twitter has made sentiment analysis of
tweets an important and well-covered area of research. However, the 140
character limit imposed on tweets makes it hard to use standard linguistic
methods for sentiment classification. On the other hand, what tweets lack in
structure they make up with sheer volume and rich metadata. This metadata
includes geolocation, temporal and author information. We hypothesize that
sentiment is dependent on all these contextual factors. Different locations,
times and authors have different emotional valences. In this paper, we explored
this hypothesis by utilizing distant supervision to collect millions of
labelled tweets from different locations, times and authors. We used this data
to analyse the variation of tweet sentiments across different authors, times
and locations. Once we explored and understood the relationship between these
variables and sentiment, we used a Bayesian approach to combine these variables
with more standard linguistic features such as n-grams to create a Twitter
sentiment classifier. This combined classifier outperforms the purely
linguistic classifier, showing that integrating the rich contextual information
available on Twitter into sentiment classification is a promising direction of
research.
</summary>
    <author>
      <name>Soroush Vosoughi</name>
    </author>
    <author>
      <name>Helen Zhou</name>
    </author>
    <author>
      <name>Deb Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2015 workshop, WASSA 2015, Lisbon, Portugal. In proceedings of
  the IEEE ICDM 2015 workshop on Event Analytics using Social Media Data
  (EASM). Atlantic City, New Jersey</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05166v1</id>
    <updated>2016-05-17T13:47:24Z</updated>
    <published>2016-05-17T13:47:24Z</published>
    <title>Digital Stylometry: Linking Profiles Across Social Networks</title>
    <summary>  There is an ever growing number of users with accounts on multiple social
media and networking sites. Consequently, there is increasing interest in
matching user accounts and profiles across different social networks in order
to create aggregate profiles of users. In this paper, we present models for
Digital Stylometry, which is a method for matching users through stylometry
inspired techniques. We experimented with linguistic, temporal, and combined
temporal-linguistic models for matching user accounts, using standard and novel
techniques. Using publicly available data, our best model, a combined
temporal-linguistic one, was able to correctly match the accounts of 31% of
5,612 distinct users across Twitter and Facebook.
</summary>
    <author>
      <name>Soroush Vosoughi</name>
    </author>
    <author>
      <name>Helen Zhou</name>
    </author>
    <author>
      <name>Deb Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SocInfo'15, Beijing, China. In proceedings of the 7th International
  Conference on Social Informatics (SocInfo 2015). Beijing, China</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04934v1</id>
    <updated>2016-05-16T20:22:30Z</updated>
    <published>2016-05-16T20:22:30Z</published>
    <title>Self-Reflective Risk-Aware Artificial Cognitive Modeling for Robot
  Response to Human Behaviors</title>
    <summary>  In order for cooperative robots ("co-robots") to respond to human behaviors
accurately and efficiently in human-robot collaboration, interpretation of
human actions, awareness of new situations, and appropriate decision making are
all crucial abilities for co-robots. For this purpose, the human behaviors
should be interpreted by co-robots in the same manner as human peers. To
address this issue, a novel interpretability indicator is introduced so that
robot actions are appropriate to the current human behaviors. In addition, the
complete consideration of all potential situations of a robot's environment is
nearly impossible in real-world applications, making it difficult for the
co-robot to act appropriately and safely in new scenarios. This is true even
when the pretrained model is highly accurate in a known situation. For
effective and safe teaming with humans, we introduce a new generalizability
indicator that allows a co-robot to self-reflect and reason about when an
observation falls outside the co-robot's learned model. Based on topic modeling
and two novel indicators, we propose a new Self-reflective Risk-aware
Artificial Cognitive (SRAC) model. The co-robots are able to consider action
risks and identify new situations so that better decisions can be made.
Experiments both using real-world datasets and on physical robots suggest that
our SRAC model significantly outperforms the traditional methodology and
enables better decision making in response to human activities.
</summary>
    <author>
      <name>Fei Han</name>
    </author>
    <author>
      <name>Christopher Reardon</name>
    </author>
    <author>
      <name>Lynne E. Parker</name>
    </author>
    <author>
      <name>Hao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04812v2</id>
    <updated>2016-05-24T10:34:21Z</updated>
    <published>2016-05-16T15:47:21Z</published>
    <title>Off-policy evaluation for slate recommendation</title>
    <summary>  This paper studies the evaluation of policies that recommend an ordered set
of items (e.g., a ranking) based on some context---a common scenario in web
search, ads and recommender systems. We develop the first practical technique
for evaluating page-level metrics of such policies offline using logged past
data, alleviating the need for online A/B tests. Our method models the observed
quality of the recommended set (e.g., time to success in web search) as an
additive decomposition across items. Crucially, the per-item quality is not
directly observed or easily modeled from the item's features. A thorough
empirical evaluation reveals that this model fits many realistic measures of
quality and theoretical analysis shows exponential savings in the amount of
required data compared with prior off-policy evaluation approaches.
</summary>
    <author>
      <name>Adith Swaminathan</name>
    </author>
    <author>
      <name>Akshay Krishnamurthy</name>
    </author>
    <author>
      <name>Alekh Agarwal</name>
    </author>
    <author>
      <name>Miroslav Dudík</name>
    </author>
    <author>
      <name>John Langford</name>
    </author>
    <author>
      <name>Damien Jose</name>
    </author>
    <author>
      <name>Imed Zitouni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages (9 main paper, 14 supplementary), 7 figures (2 main paper, 5
  supplementary)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04715v1</id>
    <updated>2016-05-16T10:26:38Z</updated>
    <published>2016-05-16T10:26:38Z</published>
    <title>On the Complexity of Connection Games</title>
    <summary>  In this paper, we study three connection games among the most widely played:
Havannah, Twixt, and Slither. We show that determining the outcome of an
arbitrary input position is PSPACE-complete in all three cases. Our reductions
are based on the popular graph problem Generalized Geography and on Hex itself.
We also consider the complexity of generalizations of Hex parameterized by the
length of the solution and establish that while Short Generalized Hex is
W[1]-hard, Short Hex is FPT. Finally, we prove that the ultra-weak solution to
the empty starting position in hex cannot be fully adapted to any of these
three games.
</summary>
    <author>
      <name>Édouard Bonnet</name>
    </author>
    <author>
      <name>Florian Jamain</name>
    </author>
    <author>
      <name>Abdallah Saffidine</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Subsumes and extends https://arxiv.org/abs/1403.6518 significantly</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04691v1</id>
    <updated>2016-05-16T09:26:53Z</updated>
    <published>2016-05-16T09:26:53Z</published>
    <title>On Avoidance Learning with Partial Observability</title>
    <summary>  We study a framework where agents have to avoid aversive signals. The agents
are given only partial information, in the form of features that are
projections of task states. Additionally, the agents have to cope with
non-determinism, defined as unpredictability on the way that actions are
executed. The goal of each agent is to define its behavior based on
feature-action pairs that reliably avoid aversive signals. We study a learning
algorithm, called A-learning, that exhibits fixpoint convergence, where the
belief of the allowed feature-action pairs eventually becomes fixed. A-learning
is parameter-free and easy to implement.
</summary>
    <author>
      <name>Tom J. Ameloot</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04682v1</id>
    <updated>2016-05-16T09:11:08Z</updated>
    <published>2016-05-16T09:11:08Z</published>
    <title>High-Performance Computing for Scheduling Decision Support: A Parallel
  Depth-First Search Heuristic</title>
    <summary>  Many academic disciplines - including information systems, computer science,
and operations management - face scheduling problems as important decision
making tasks. Since many scheduling problems are NP-hard in the strong sense,
there is a need for developing solution heuristics. For scheduling problems
with setup times on unrelated parallel machines, there is limited research on
solution methods and to the best of our knowledge, parallel computer
architectures have not yet been taken advantage of. We address this gap by
proposing and implementing a new solution heuristic and by testing different
parallelization strategies. In our computational experiments, we show that our
heuristic calculates near-optimal solutions even for large instances and that
computing time can be reduced substantially by our parallelization approach.
</summary>
    <author>
      <name>Gerhard Rauchecker</name>
    </author>
    <author>
      <name>Guido Schryen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISBN# 978-0-646-95337-3 Presented at the Australasian Conference on
  Information Systems 2015 (arXiv:1605.01032)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04672v1</id>
    <updated>2016-05-16T07:43:28Z</updated>
    <published>2016-05-16T07:43:28Z</published>
    <title>A Critical Examination of RESCAL for Completion of Knowledge Bases with
  Transitive Relations</title>
    <summary>  Link prediction in large knowledge graphs has received a lot of attention
recently because of its importance for inferring missing relations and for
completing and improving noisily extracted knowledge graphs. Over the years a
number of machine learning researchers have presented various models for
predicting the presence of missing relations in a knowledge base. Although all
the previous methods are presented with empirical results that show high
performance on select datasets, there is almost no previous work on
understanding the connection between properties of a knowledge base and the
performance of a model. In this paper we analyze the RESCAL method and prove
that it can not encode asymmetric transitive relations in knowledge bases.
</summary>
    <author>
      <name>Pushpendre Rastogi</name>
    </author>
    <author>
      <name>Benjamin Van Durme</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Four and a half page</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04466v1</id>
    <updated>2016-05-14T21:09:10Z</updated>
    <published>2016-05-14T21:09:10Z</published>
    <title>Generalized Linear Models for Aggregated Data</title>
    <summary>  Databases in domains such as healthcare are routinely released to the public
in aggregated form. Unfortunately, naive modeling with aggregated data may
significantly diminish the accuracy of inferences at the individual level. This
paper addresses the scenario where features are provided at the individual
level, but the target variables are only available as histogram aggregates or
order statistics. We consider a limiting case of generalized linear modeling
when the target variables are only known up to permutation, and explore how
this relates to permutation testing; a standard technique for assessing
statistical dependency. Based on this relationship, we propose a simple
algorithm to estimate the model parameters and individual level inferences via
alternating imputation and standard generalized linear model fitting. Our
results suggest the effectiveness of the proposed approach when, in the
original data, permutation testing accurately ascertains the veracity of the
linear relationship. The framework is extended to general histogram data with
larger bins - with order statistics such as the median as a limiting case. Our
experimental results on simulated data and aggregated healthcare data suggest a
diminishing returns property with respect to the granularity of the histogram -
when a linear relationship holds in the original data, the targets can be
predicted accurately given relatively coarse histograms.
</summary>
    <author>
      <name>Avradeep Bhowmik</name>
    </author>
    <author>
      <name>Joydeep Ghosh</name>
    </author>
    <author>
      <name>Oluwasanmi Koyejo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AISTATS 2015, 9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04465v1</id>
    <updated>2016-05-14T20:35:20Z</updated>
    <published>2016-05-14T20:35:20Z</published>
    <title>Monotone Retargeting for Unsupervised Rank Aggregation with Object
  Features</title>
    <summary>  Learning the true ordering between objects by aggregating a set of expert
opinion rank order lists is an important and ubiquitous problem in many
applications ranging from social choice theory to natural language processing
and search aggregation. We study the problem of unsupervised rank aggregation
where no ground truth ordering information in available, neither about the true
preference ordering between any set of objects nor about the quality of
individual rank lists. Aggregating the often inconsistent and poor quality rank
lists in such an unsupervised manner is a highly challenging problem, and
standard consensus-based methods are often ill-defined, and difficult to solve.
In this manuscript we propose a novel framework to bypass these issues by using
object attributes to augment the standard rank aggregation framework. We design
algorithms that learn joint models on both rank lists and object features to
obtain an aggregated rank ordering that is more accurate and robust, and also
helps weed out rank lists of dubious validity. We validate our techniques on
synthetic datasets where our algorithm is able to estimate the true rank
ordering even when the rank lists are corrupted. Experiments on three real
datasets, MQ2008, MQ2008 and OHSUMED, show that using object features can
result in significant improvement in performance over existing rank aggregation
methods that do not use object information. Furthermore, when at least some of
the rank lists are of high quality, our methods are able to effectively exploit
their high expertise to output an aggregated rank ordering of great accuracy.
</summary>
    <author>
      <name>Avradeep Bhowmik</name>
    </author>
    <author>
      <name>Joydeep Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04263v2</id>
    <updated>2016-05-16T09:21:26Z</updated>
    <published>2016-05-13T17:29:28Z</published>
    <title>OBDA Constraints for Effective Query Answering (Extended Version)</title>
    <summary>  In Ontology Based Data Access (OBDA) users pose SPARQL queries over an
ontology that lies on top of relational datasources. These queries are
translated on-the-fly into SQL queries by OBDA systems. Standard SPARQL-to-SQL
translation techniques in OBDA often produce SQL queries containing redundant
joins and unions, even after a number of semantic and structural optimizations.
These redundancies are detrimental to the performance of query answering,
especially in complex industrial OBDA scenarios with large enterprise
databases. To address this issue, we introduce two novel notions of OBDA
constraints and show how to exploit them for efficient query answering. We
conduct an extensive set of experiments on large datasets using real world data
and queries, showing that these techniques strongly improve the performance of
query answering up to orders of magnitude.
</summary>
    <author>
      <name>Dag Hovland</name>
    </author>
    <author>
      <name>Davide Lanti</name>
    </author>
    <author>
      <name>Martin Rezk</name>
    </author>
    <author>
      <name>Guohui Xiao</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04263v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04263v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04218v1</id>
    <updated>2016-05-13T15:40:10Z</updated>
    <published>2016-05-13T15:40:10Z</published>
    <title>Anytime Inference in Valuation Algebras</title>
    <summary>  Anytime inference is inference performed incrementally, with the accuracy of
the inference being controlled by a tunable parameter, usually time. Such
anytime inference algorithms are also usually interruptible, gradually
converging to the exact inference value until terminated. While anytime
inference algorithms for specific domains like probability potentials exist in
the literature, our objective in this article is to obtain an anytime inference
algorithm which is sufficiently generic to cover a wide range of domains. For
this we utilise the theory of generic inference as a basis for constructing an
anytime inference algorithm, and in particular, extending work done on ordered
valuation algebras. The novel contribution of this work is the construction of
anytime algorithms in a generic framework, which automatically gives us
instantiations in various useful domains. We also show how to apply this
generic framework for anytime inference in semiring induced valuation algebras,
an important subclass of valuation algebras, which includes instances like
probability potentials, disjunctive normal forms and distributive lattices.
  Keywords: Approximation; Anytime algorithms; Resource-bounded computation;
Generic inference; Valuation algebras; Local computation; Binary join trees.
</summary>
    <author>
      <name>Abhishek Dasgupta</name>
    </author>
    <author>
      <name>Samson Abramsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04135v3</id>
    <updated>2016-06-13T18:11:54Z</updated>
    <published>2016-05-13T11:14:58Z</published>
    <title>Online Optimization Methods for the Quantification Problem</title>
    <summary>  The estimation of class prevalence, i.e., the fraction of a population that
belongs to a certain class, is a very useful tool in data analytics and
learning, and finds applications in many domains such as sentiment analysis,
epidemiology, etc. For example, in sentiment analysis, the objective is often
not to estimate whether a specific text conveys a positive or a negative
sentiment, but rather estimate the overall distribution of positive and
negative sentiments during an event window. A popular way of performing the
above task, often dubbed quantification, is to use supervised learning to train
a prevalence estimator from labeled data.
  Contemporary literature cites several performance measures used to measure
the success of such prevalence estimators. In this paper we propose the first
online stochastic algorithms for directly optimizing these
quantification-specific performance measures. We also provide algorithms that
optimize hybrid performance measures that seek to balance quantification and
classification performance. Our algorithms present a significant advancement in
the theory of multivariate optimization and we show, by a rigorous theoretical
analysis, that they exhibit optimal convergence. We also report extensive
experiments on benchmark and real data sets which demonstrate that our methods
significantly outperform existing optimization techniques used for these
performance measures.
</summary>
    <author>
      <name>Purushottam Kar</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <author>
      <name>Harikrishna Narasimhan</name>
    </author>
    <author>
      <name>Sanjay Chawla</name>
    </author>
    <author>
      <name>Fabrizio Sebastiani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 6 figures. A short version of this manuscript will appear
  in the proceedings of the 22nd ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining, KDD 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04135v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04135v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04122v1</id>
    <updated>2016-05-13T10:46:22Z</updated>
    <published>2016-05-13T10:46:22Z</published>
    <title>Natural Language Semantics and Computability</title>
    <summary>  This paper is a reflexion on the computability of natural language semantics.
It does not contain a new model or new results in the formal semantics of
natural language: it is rather a computational analysis of the logical models
and algorithms currently used in natural language semantics, defined as the
mapping of a statement to logical formulas - formulas, because a statement can
be ambiguous. We argue that as long as possible world semantics is left out,
one can compute the semantic representation(s) of a given statement, including
aspects of lexical meaning. We also discuss the algorithmic complexity of this
process.
</summary>
    <author>
      <name>Richard Moot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Christian Retoré</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TEXTE</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1605.04122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04074v1</id>
    <updated>2016-05-13T07:50:50Z</updated>
    <published>2016-05-13T07:50:50Z</published>
    <title>Wisdom of Crowds cluster ensemble</title>
    <summary>  The Wisdom of Crowds is a phenomenon described in social science that
suggests four criteria applicable to groups of people. It is claimed that, if
these criteria are satisfied, then the aggregate decisions made by a group will
often be better than those of its individual members. Inspired by this concept,
we present a novel feedback framework for the cluster ensemble problem, which
we call Wisdom of Crowds Cluster Ensemble (WOCCE). Although many conventional
cluster ensemble methods focusing on diversity have recently been proposed,
WOCCE analyzes the conditions necessary for a crowd to exhibit this collective
wisdom. These include decentralization criteria for generating primary results,
independence criteria for the base algorithms, and diversity criteria for the
ensemble members. We suggest appropriate procedures for evaluating these
measures, and propose a new measure to assess the diversity. We evaluate the
performance of WOCCE against some other traditional base algorithms as well as
state-of-the-art ensemble methods. The results demonstrate the efficiency of
WOCCE's aggregate decision-making compared to other algorithms.
</summary>
    <author>
      <name>Hosein Alizadeh</name>
    </author>
    <author>
      <name>Muhammad Yousefnezhad</name>
    </author>
    <author>
      <name>Behrouz Minaei Bidgoli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/IDA-150728</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/IDA-150728" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Intelligent Data Analysis (IDA), IOS Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04072v1</id>
    <updated>2016-05-13T07:31:50Z</updated>
    <published>2016-05-13T07:31:50Z</published>
    <title>Towards Empathetic Human-Robot Interactions</title>
    <summary>  Since the late 1990s when speech companies began providing their
customer-service software in the market, people have gotten used to speaking to
machines. As people interact more often with voice and gesture controlled
machines, they expect the machines to recognize different emotions, and
understand other high level communication features such as humor, sarcasm and
intention. In order to make such communication possible, the machines need an
empathy module in them which can extract emotions from human speech and
behavior and can decide the correct response of the robot. Although research on
empathetic robots is still in the early stage, we described our approach using
signal processing techniques, sentiment analysis and machine learning
algorithms to make robots that can "understand" human emotion. We propose Zara
the Supergirl as a prototype system of empathetic robots. It is a software
based virtual android, with an animated cartoon character to present itself on
the screen. She will get "smarter" and more empathetic through its deep
learning algorithms, and by gathering more data and learning from it. In this
paper, we present our work so far in the areas of deep learning of emotion and
sentiment recognition, as well as humor recognition. We hope to explore the
future direction of android development and how it can help improve people's
lives.
</summary>
    <author>
      <name>Pascale Fung</name>
    </author>
    <author>
      <name>Dario Bertero</name>
    </author>
    <author>
      <name>Yan Wan</name>
    </author>
    <author>
      <name>Anik Dey</name>
    </author>
    <author>
      <name>Ricky Ho Yin Chan</name>
    </author>
    <author>
      <name>Farhad Bin Siddique</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Chien-Sheng Wu</name>
    </author>
    <author>
      <name>Ruixi Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages. Keynote at 17th International Conference on Intelligent
  Text Processing and Computational Linguistics. To appear in Lecture Notes in
  Computer Science</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04071v1</id>
    <updated>2016-05-13T07:27:03Z</updated>
    <published>2016-05-13T07:27:03Z</published>
    <title>Bayesian Network Structure Learning with Integer Programming: Polytopes,
  Facets, and Complexity</title>
    <summary>  The challenging task of learning structures of probabilistic graphical models
is an important problem within modern AI research. Recent years have witnessed
several major algorithmic advances in structure learning for Bayesian
networks---arguably the most central class of graphical models---especially in
what is known as the score-based setting. A successful generic approach to
optimal Bayesian network structure learning (BNSL), based on integer
programming (IP), is implemented in the GOBNILP system. Despite the recent
algorithmic advances, current understanding of foundational aspects underlying
the IP based approach to BNSL is still somewhat lacking. Understanding
fundamental aspects of cutting planes and the related separation problem( is
important not only from a purely theoretical perspective, but also since it
holds out the promise of further improving the efficiency of state-of-the-art
approaches to solving BNSL exactly. In this paper, we make several theoretical
contributions towards these goals: (i) we study the computational complexity of
the separation problem, proving that the problem is NP-hard; (ii) we formalise
and analyse the relationship between three key polytopes underlying the
IP-based approach to BNSL; (iii) we study the facets of the three polytopes
both from the theoretical and practical perspective, providing, via exhaustive
computation, a complete enumeration of facets for low-dimensional
family-variable polytopes; and, furthermore, (iv) we establish a tight
connection of the BNSL problem to the acyclic subgraph problem.
</summary>
    <author>
      <name>James Cussens</name>
    </author>
    <author>
      <name>Matti Järvisalo</name>
    </author>
    <author>
      <name>Janne H. Korhonen</name>
    </author>
    <author>
      <name>Mark Bartlett</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04056v2</id>
    <updated>2016-06-13T22:51:23Z</updated>
    <published>2016-05-13T06:17:54Z</published>
    <title>Causal Discovery for Manufacturing Domains</title>
    <summary>  Yield and quality improvement is of paramount importance to any manufacturing
company. One of the ways of improving yield is through discovery of the root
causal factors affecting yield. We propose the use of data-driven interpretable
causal models to identify key factors affecting yield. We focus on factors that
are measured in different stages of production and testing in the manufacturing
cycle of a product. We apply causal structure learning techniques on real data
collected from this line. Specifically, the goal of this work is to learn
interpretable causal models from observational data produced by manufacturing
lines.
  Emphasis has been given to the interpretability of the models to make them
actionable in the field of manufacturing. We highlight the challenges presented
by assembly line data and propose ways to alleviate them.We also identify
unique characteristics of data originating from assembly lines and how to
leverage them in order to improve causal discovery. Standard evaluation
techniques for causal structure learning shows that the learned causal models
seem to closely represent the underlying latent causal relationship between
different factors in the production process. These results were also validated
by manufacturing domain experts who found them promising. This work
demonstrates how data mining and knowledge discovery can be used for root cause
analysis in the domain of manufacturing and connected industry.
</summary>
    <author>
      <name>Katerina Marazopoulou</name>
    </author>
    <author>
      <name>Rumi Ghosh</name>
    </author>
    <author>
      <name>Prasanth Lade</name>
    </author>
    <author>
      <name>David Jensen</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04056v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04056v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04039v1</id>
    <updated>2016-05-13T03:35:14Z</updated>
    <published>2016-05-13T03:35:14Z</published>
    <title>Cross-Domain Visual Matching via Generalized Similarity Measure and
  Feature Learning</title>
    <summary>  Cross-domain visual data matching is one of the fundamental problems in many
real-world vision tasks, e.g., matching persons across ID photos and
surveillance videos. Conventional approaches to this problem usually involves
two steps: i) projecting samples from different domains into a common space,
and ii) computing (dis-)similarity in this space based on a certain distance.
In this paper, we present a novel pairwise similarity measure that advances
existing models by i) expanding traditional linear projections into affine
transformations and ii) fusing affine Mahalanobis distance and Cosine
similarity by a data-driven combination. Moreover, we unify our similarity
measure with feature representation learning via deep convolutional neural
networks. Specifically, we incorporate the similarity measure matrix into the
deep architecture, enabling an end-to-end way of model optimization. We
extensively evaluate our generalized similarity model in several challenging
cross-domain matching tasks: person re-identification under different views and
face verification over different modalities (i.e., faces from still images and
videos, older and younger faces, and sketch and photo portraits). The
experimental results demonstrate superior performance of our model over other
state-of-the-art methods.
</summary>
    <author>
      <name>Liang Lin</name>
    </author>
    <author>
      <name>Guangrun Wang</name>
    </author>
    <author>
      <name>Wangmeng Zuo</name>
    </author>
    <author>
      <name>Xiangchu Feng</name>
    </author>
    <author>
      <name>Lei Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05247v1</id>
    <updated>2016-05-12T23:56:31Z</updated>
    <published>2016-05-12T23:56:31Z</published>
    <title>Heart Rate Variability and Respiration Signal as Diagnostic Tools for
  Late Onset Sepsis in Neonatal Intensive Care Units</title>
    <summary>  Apnea-bradycardia is one of the major clinical early indicators of late-onset
sepsis occurring in approximately 7% to 10% of all neonates and in more than
25% of very low birth weight infants in NICU. The objective of this paper was
to determine if HRV, respiration and their relationships help to diagnose
infection in premature infants via non-invasive ways in NICU. Therefore, we
implement Mono-Channel (MC) and Bi-Channel (BC) Analysis in two groups: sepsis
(S) vs. non-sepsis (NS). Firstly, we studied RR series not only by linear
methods: time domain and frequency domain, but also by non-linear methods:
chaos theory and information theory. The results show that alpha Slow, alpha
Fast and Sample Entropy are significant parameters to distinguish S from NS.
Secondly, the question about the functional coupling of HRV and nasal
respiration is addressed. Local linear correlation coefficient r2t,f has been
explored, while non-linear regression coefficient h2 was calculated in two
directions. It is obvious that r2t,f within the third frequency band (0.2&lt;f&lt;0.4
Hz) and h2 in two directions were complementary approaches to diagnose sepsis.
Thirdly, feasibility study is carried out on the candidate parameters selected
from MC and BC respectively. We discovered that the proposed test based on
optimal fusion of 6 features shows good performance with the largest AUC and a
reduced probability of false alarm (PFA).
</summary>
    <author>
      <name>Yuan Wang</name>
    </author>
    <author>
      <name>Guy Carrault</name>
    </author>
    <author>
      <name>Alain Beuchee</name>
    </author>
    <author>
      <name>Nathalie Costet</name>
    </author>
    <author>
      <name>Huazhong Shu</name>
    </author>
    <author>
      <name>Lotfi Senhadji</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04874v1</id>
    <updated>2016-05-12T23:29:29Z</updated>
    <published>2016-05-12T23:29:29Z</published>
    <title>Gearbox Fault Detection through PSO Exact Wavelet Analysis and SVM
  Classifier</title>
    <summary>  Time-frequency methods for vibration-based gearbox faults detection have been
considered the most efficient method. Among these methods, continuous wavelet
transform (CWT) as one of the best time-frequency method has been used for both
stationary and transitory signals. Some deficiencies of CWT are problem of
overlapping and distortion ofsignals. In this condition, a large amount of
redundant information exists so that it may cause false alarm or
misinterpretation of the operator. In this paper a modified method called Exact
Wavelet Analysis is used to minimize the effects of overlapping and distortion
in case of gearbox faults. To implement exact wavelet analysis, Particle Swarm
Optimization (PSO) algorithm has been used for this purpose. This method have
been implemented for the acceleration signals from 2D acceleration sensor
acquired by Advantech PCI-1710 card from a gearbox test setup in Amirkabir
University of Technology. Gearbox has been considered in both healthy and
chipped tooth gears conditions. Kernelized Support Vector Machine (SVM) with
radial basis functions has used the extracted features from exact wavelet
analysis for classification. The efficiency of this classifier is then
evaluated with the other signals acquired from the setup test. The results show
that in comparison of CWT, PSO Exact Wavelet Transform has better ability in
feature extraction in price of more computational effort. In addition, PSO
exact wavelet has better speed comparing to Genetic Algorithm (GA) exact
wavelet in condition of equal population because of factoring mutation and
crossover in PSO algorithm. SVM classifier with the extracted features in
gearbox shows very good results and its ability has been proved.
</summary>
    <author>
      <name>Amir Hosein Zamanian</name>
    </author>
    <author>
      <name>Abdolreza Ohadi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.4983.3442</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.4983.3442" rel="related"/>
    <link href="http://arxiv.org/abs/1605.04874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03915v2</id>
    <updated>2016-05-13T01:28:52Z</updated>
    <published>2016-05-12T18:03:38Z</published>
    <title>Optimizing human-interpretable dialog management policy using Genetic
  Algorithm</title>
    <summary>  Automatic optimization of spoken dialog management policies that are robust
to environmental noise has long been the goal for both academia and industry.
Approaches based on reinforcement learning have been proved to be effective.
However, the numerical representation of dialog policy is
human-incomprehensible and difficult for dialog system designers to verify or
modify, which limits its practical application. In this paper we propose a
novel framework for optimizing dialog policies specified in domain language
using genetic algorithm. The human-interpretable representation of policy makes
the method suitable for practical employment. We present learning algorithms
using user simulation and real human-machine dialogs respectively.Empirical
experimental results are given to show the effectiveness of the proposed
approach.
</summary>
    <author>
      <name>Hang Ren</name>
    </author>
    <author>
      <name>Weiqun Xu</name>
    </author>
    <author>
      <name>Yonghong Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This technical report is an updated version of the conference paper:
  "H. Ren, W. Xu, and Y. Yan, Optimizing human-interpretable dialog management
  policy using genetic algorithm, in 2015 IEEE Workshop on Automatic Speech
  Recognition and Understanding (ASRU), 2015, 791-797". Experiments on policy
  training via user simulator have been enriched and the reward function is
  updated</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03915v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03915v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03661v2</id>
    <updated>2016-06-08T17:04:07Z</updated>
    <published>2016-05-12T02:59:40Z</published>
    <title>Learning Representations for Counterfactual Inference</title>
    <summary>  Observational studies are rising in importance due to the widespread
accumulation of data in fields such as healthcare, education, employment and
ecology. We consider the task of answering counterfactual questions such as,
"Would this patient have lower blood sugar had she received a different
medication?". We propose a new algorithmic framework for counterfactual
inference which brings together ideas from domain adaptation and representation
learning. In addition to a theoretical justification, we perform an empirical
comparison with previous approaches to causal inference from observational
data. Our deep learning algorithm significantly outperforms the previous
state-of-the-art.
</summary>
    <author>
      <name>Fredrik D. Johansson</name>
    </author>
    <author>
      <name>Uri Shalit</name>
    </author>
    <author>
      <name>David Sontag</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appearing in ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03661v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03661v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04232v1</id>
    <updated>2016-05-11T17:49:24Z</updated>
    <published>2016-05-11T17:49:24Z</published>
    <title>Review of state-of-the-arts in artificial intelligence with application
  to AI safety problem</title>
    <summary>  Here, I review current state-of-the-arts in many areas of AI to estimate when
it's reasonable to expect human level AI development. Predictions of prominent
AI researchers vary broadly from very pessimistic predictions of Andrew Ng to
much more moderate predictions of Geoffrey Hinton and optimistic predictions of
Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and
this broad range of predictions of AI experts, AI safety questions are also
discussed.
</summary>
    <author>
      <name>Vladimir Shakirov</name>
    </author>
    <link href="http://arxiv.org/abs/1605.04232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03506v1</id>
    <updated>2016-05-11T16:42:37Z</updated>
    <published>2016-05-11T16:42:37Z</published>
    <title>Characterizing Quantifier Fuzzification Mechanisms: a behavioral guide
  for practical applications</title>
    <summary>  Important advances have been made in the fuzzy quantification field.
Nevertheless, some problems remain when we face the decision of selecting the
most convenient model for a specific application. In the literature, several
desirable adequacy properties have been proposed, but theoretical limits impede
quantification models from simultaneously fulfilling every adequacy property
that has been defined. Besides, the complexity of model definitions and
adequacy properties makes very difficult for real users to understand the
particularities of the different models that have been presented. In this work
we will present several criteria conceived to help in the process of selecting
the most adequate Quantifier Fuzzification Mechanisms for specific practical
applications. In addition, some of the best known well-behaved models will be
compared against this list of criteria. Based on this analysis, some guidance
to choose fuzzy quantification models for practical applications will be
provided.
</summary>
    <author>
      <name>F. Diaz-Hermida</name>
    </author>
    <author>
      <name>M. Pereira-Fariña</name>
    </author>
    <author>
      <name>Juan C. Vidal</name>
    </author>
    <author>
      <name>A. Ramos-Soto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03498v2</id>
    <updated>2016-05-23T08:34:50Z</updated>
    <published>2016-05-11T16:22:23Z</published>
    <title>Deep Neural Networks Under Stress</title>
    <summary>  In recent years, deep architectures have been used for transfer learning with
state-of-the-art performance in many datasets. The properties of their features
remain, however, largely unstudied under the transfer perspective. In this
work, we present an extensive analysis of the resiliency of feature vectors
extracted from deep models, with special focus on the trade-off between
performance and compression rate. By introducing perturbations to image
descriptions extracted from a deep convolutional neural network, we change
their precision and number of dimensions, measuring how it affects the final
score. We show that deep features are more robust to these disturbances when
compared to classical approaches, achieving a compression rate of 98.4%, while
losing only 0.88% of their original score for Pascal VOC 2007.
</summary>
    <author>
      <name>Micael Carvalho</name>
    </author>
    <author>
      <name>Matthieu Cord</name>
    </author>
    <author>
      <name>Sandra Avila</name>
    </author>
    <author>
      <name>Nicolas Thome</name>
    </author>
    <author>
      <name>Eduardo Valle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article corresponds to the accepted version at IEEE ICIP 2016.
  We will link the DOI as soon as it is available</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03498v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03498v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03416v1</id>
    <updated>2016-05-11T12:51:19Z</updated>
    <published>2016-05-11T12:51:19Z</published>
    <title>Concept based Attention</title>
    <summary>  Attention endows animals an ability to concentrate on the most relevant
information among a deluge of distractors at any given time, either through
volitionally 'top-down' biasing, or driven by automatically 'bottom-up'
saliency of stimuli, in favour of advantageous competition in neural
modulations for information processing. Nevertheless, instead of being limited
to perceive simple features, human and other advanced animals adaptively learn
the world into categories and abstract concepts from experiences, imparting the
world meanings. This thesis suggests that the high-level cognitive ability of
human is more likely driven by attention basing on abstract perceptions, which
is defined as concept based attention (CbA).
</summary>
    <author>
      <name>Jie You</name>
    </author>
    <author>
      <name>Xin Yang</name>
    </author>
    <author>
      <name>Matthias Hub</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NeuroSci.Proc.Suppl. 89 (2007) 4-11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.03416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03392v1</id>
    <updated>2016-05-11T11:54:26Z</updated>
    <published>2016-05-11T11:54:26Z</published>
    <title>Learning Bounded Treewidth Bayesian Networks with Thousands of Variables</title>
    <summary>  We present a method for learning treewidth-bounded Bayesian networks from
data sets containing thousands of variables. Bounding the treewidth of a
Bayesian greatly reduces the complexity of inferences. Yet, being a global
property of the graph, it considerably increases the difficulty of the learning
process. We propose a novel algorithm for this task, able to scale to large
domains and large treewidths. Our novel approach consistently outperforms the
state of the art on data sets with up to ten thousand variables.
</summary>
    <author>
      <name>Mauro Scanagatta</name>
    </author>
    <author>
      <name>Giorgio Corani</name>
    </author>
    <author>
      <name>Cassio P. de Campos</name>
    </author>
    <author>
      <name>Marco Zaffalon</name>
    </author>
    <link href="http://arxiv.org/abs/1605.03392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03269v1</id>
    <updated>2016-05-11T03:22:13Z</updated>
    <published>2016-05-11T03:22:13Z</published>
    <title>A Hierarchical Emotion Regulated Sensorimotor Model: Case Studies</title>
    <summary>  Inspired by the hierarchical cognitive architecture and the perception-action
model (PAM), we propose that the internal status acts as a kind of
common-coding representation which affects, mediates and even regulates the
sensorimotor behaviours. These regulation can be depicted in the Bayesian
framework, that is why cognitive agents are able to generate behaviours with
subtle differences according to their emotion or recognize the emotion by
perception. A novel recurrent neural network called recurrent neural network
with parametric bias units (RNNPB) runs in three modes, constructing a
two-level emotion regulated learning model, was further applied to testify this
theory in two different cases.
</summary>
    <author>
      <name>Junpei Zhong</name>
    </author>
    <author>
      <name>Rony Novianto</name>
    </author>
    <author>
      <name>Mingjun Dai</name>
    </author>
    <author>
      <name>Xinzheng Zhang</name>
    </author>
    <author>
      <name>Angelo Cangelosi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at The 5th International Conference on Data-Driven Control
  and Learning Systems. 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03143v1</id>
    <updated>2016-05-10T18:28:57Z</updated>
    <published>2016-05-10T18:28:57Z</published>
    <title>Avoiding Wireheading with Value Reinforcement Learning</title>
    <summary>  How can we design good goals for arbitrarily intelligent agents?
Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not
work well for generally intelligent agents, as RL agents are incentivised to
shortcut the reward sensor for maximum reward -- the so-called wireheading
problem. In this paper we suggest an alternative to RL called value
reinforcement learning (VRL). In VRL, agents use the reward signal to learn a
utility function. The VRL setup allows us to remove the incentive to wirehead
by placing a constraint on the agent's actions. The constraint is defined in
terms of the agent's belief distributions, and does not require an explicit
specification of which actions constitute wireheading.
</summary>
    <author>
      <name>Tom Everitt</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial General Intelligence (AGI) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03142v1</id>
    <updated>2016-05-10T18:25:49Z</updated>
    <published>2016-05-10T18:25:49Z</published>
    <title>Self-Modification of Policy and Utility Function in Rational Agents</title>
    <summary>  Any agent that is part of the environment it interacts with and has versatile
actuators (such as arms and fingers), will in principle have the ability to
self-modify -- for example by changing its own source code. As we continue to
create more and more intelligent agents, chances increase that they will learn
about this ability. The question is: will they want to use it? For example,
highly intelligent systems may find ways to change their goals to something
more easily achievable, thereby `escaping' the control of their designers. In
an important paper, Omohundro (2008) argued that goal preservation is a
fundamental drive of any intelligent system, since a goal is more likely to be
achieved if future versions of the agent strive towards the same goal. In this
paper, we formalise this argument in general reinforcement learning, and
explore situations where it fails. Our conclusion is that the self-modification
possibility is harmless if and only if the value function of the agent
anticipates the consequences of self-modifications and use the current utility
function when evaluating the future.
</summary>
    <author>
      <name>Tom Everitt</name>
    </author>
    <author>
      <name>Daniel Filan</name>
    </author>
    <author>
      <name>Mayank Daswani</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial General Intelligence (AGI) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02929v1</id>
    <updated>2016-05-10T10:30:06Z</updated>
    <published>2016-05-10T10:30:06Z</published>
    <title>Function-Described Graphs for Structural Pattern Recognition</title>
    <summary>  We present in this article the model Function-described graph (FDG), which is
a type of compact representation of a set of attributed graphs (AGs) that
borrow from Random Graphs the capability of probabilistic modelling of
structural and attribute information. We define the FDGs, their features and
two distance measures between AGs (unclassified patterns) and FDGs (models or
classes) and we also explain an efficient matching algorithm. Two applications
of FDGs are presented: in the former, FDGs are used for modelling and matching
3D-objects described by multiple views, whereas in the latter, they are used
for representing and recognising human faces, described also by several views.
</summary>
    <author>
      <name>Francesc Serratosa</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02817v2</id>
    <updated>2016-09-01T18:29:13Z</updated>
    <published>2016-05-10T01:39:38Z</published>
    <title>Unethical Research: How to Create a Malevolent Artificial Intelligence</title>
    <summary>  Cybersecurity research involves publishing papers about malicious exploits as
much as publishing information on how to design tools to protect
cyber-infrastructure. It is this information exchange between ethical hackers
and security experts, which results in a well-balanced cyber-ecosystem. In the
blooming domain of AI Safety Engineering, hundreds of papers have been
published on different proposals geared at the creation of a safe machine, yet
nothing, to our knowledge, has been published on how to design a malevolent
machine. Availability of such information would be of great value particularly
to computer scientists, mathematicians, and others who have an interest in AI
safety, and who are attempting to avoid the spontaneous emergence or the
deliberate creation of a dangerous AI, which can negatively affect human
activities and in the worst case cause the complete obliteration of the human
species. This paper provides some general guidelines for the creation of a
Malevolent Artificial Intelligence (MAI).
</summary>
    <author>
      <name>Federico Pistono</name>
    </author>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02817v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02817v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02697v1</id>
    <updated>2016-05-09T19:04:23Z</updated>
    <published>2016-05-09T19:04:23Z</published>
    <title>Ask Your Neurons: A Deep Learning Approach to Visual Question Answering</title>
    <summary>  We address a question answering task on real-world images that is set up as a
Visual Turing Test. By combining latest advances in image representation and
natural language processing, we propose Ask Your Neurons, a scalable, jointly
trained, end-to-end formulation to this problem.
  In contrast to previous efforts, we are facing a multi-modal problem where
the language output (answer) is conditioned on visual and natural language
inputs (image and question). We provide additional insights into the problem by
analyzing how much information is contained only in the language part for which
we provide a new human baseline. To study human consensus, which is related to
the ambiguities inherent in this challenging task, we propose two novel metrics
and collect additional answers which extend the original DAQUAR dataset to
DAQUAR-Consensus.
  Moreover, we also extend our analysis to VQA, a large-scale question
answering about images dataset, where we investigate some particular design
choices and show the importance of stronger visual models. At the same time, we
achieve strong performance of our model that still uses a global image
representation. Finally, based on such analysis, we refine our Ask Your Neurons
on DAQUAR, which also leads to a better performance on this challenging task.
</summary>
    <author>
      <name>Mateusz Malinowski</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02677v1</id>
    <updated>2016-05-09T18:14:52Z</updated>
    <published>2016-05-09T18:14:52Z</published>
    <title>Building a Large Scale Dataset for Image Emotion Recognition: The Fine
  Print and The Benchmark</title>
    <summary>  Psychological research results have confirmed that people can have different
emotional reactions to different visual stimuli. Several papers have been
published on the problem of visual emotion analysis. In particular, attempts
have been made to analyze and predict people's emotional reaction towards
images. To this end, different kinds of hand-tuned features are proposed. The
results reported on several carefully selected and labeled small image data
sets have confirmed the promise of such features. While the recent successes of
many computer vision related tasks are due to the adoption of Convolutional
Neural Networks (CNNs), visual emotion analysis has not achieved the same level
of success. This may be primarily due to the unavailability of confidently
labeled and relatively large image data sets for visual emotion analysis. In
this work, we introduce a new data set, which started from 3+ million weakly
labeled images of different emotions and ended up 30 times as large as the
current largest publicly available visual emotion data set. We hope that this
data set encourages further research on visual emotion analysis. We also
perform extensive benchmarking analyses on this large data set using the state
of the art methods including CNNs.
</summary>
    <author>
      <name>Quanzeng You</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <author>
      <name>Hailin Jin</name>
    </author>
    <author>
      <name>Jianchao Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, AAAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02669v1</id>
    <updated>2016-05-09T17:41:37Z</updated>
    <published>2016-05-09T17:41:37Z</published>
    <title>The GPU-based Parallel Ant Colony System</title>
    <summary>  The Ant Colony System (ACS) is, next to Ant Colony Optimization (ACO) and the
MAX-MIN Ant System (MMAS), one of the most efficient metaheuristic algorithms
inspired by the behavior of ants. In this article we present three novel
parallel versions of the ACS for the graphics processing units (GPUs). To the
best of our knowledge, this is the first such work on the ACS which shares many
key elements of the ACO and the MMAS, but differences in the process of
building solutions and updating the pheromone trails make obtaining an
efficient parallel version for the GPUs a difficult task. The proposed parallel
versions of the ACS differ mainly in their implementations of the pheromone
memory. The first two use the standard pheromone matrix, and the third uses a
novel selective pheromone memory. Computational experiments conducted on
several Travelling Salesman Problem (TSP) instances of sizes ranging from 198
to 2392 cities showed that the parallel ACS on Nvidia Kepler GK104 GPU (1536
CUDA cores) is able to obtain a speedup up to 24.29x vs the sequential ACS
running on a single core of Intel Xeon E5-2670 CPU. The parallel ACS with the
selective pheromone memory achieved speedups up to 16.85x, but in most cases
the obtained solutions were of significantly better quality than for the
sequential ACS.
</summary>
    <author>
      <name>Rafał Skinderowicz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jpdc.2016.04.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jpdc.2016.04.014" rel="related"/>
    <link href="http://arxiv.org/abs/1605.02669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04583v1</id>
    <updated>2016-05-09T13:35:00Z</updated>
    <published>2016-05-09T13:35:00Z</published>
    <title>A Counterexample to the Forward Recursion in Fuzzy Critical Path
  Analysis Under Discrete Fuzzy Sets</title>
    <summary>  Fuzzy logic is an alternate approach for quantifying uncertainty relating to
activity duration. The fuzzy version of the backward recursion has been shown
to produce results that incorrectly amplify the level of uncertainty. However,
the fuzzy version of the forward recursion has been widely proposed as an
approach for determining the fuzzy set of critical path lengths. In this paper,
the direct application of the extension principle leads to a proposition that
must be satisfied in fuzzy critical path analysis. Using a counterexample it is
demonstrated that the fuzzy forward recursion when discrete fuzzy sets are used
to represent activity durations produces results that are not consistent with
the theory presented. The problem is shown to be the application of the fuzzy
maximum. Several methods presented in the literature are described and shown to
provide results that are consistent with the extension principle.
</summary>
    <author>
      <name>Matthew J. Liberatore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, 1 table, 22 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Fuzzy Logic Systems 6 (2016) 53-62</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.04583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62A86" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02442v1</id>
    <updated>2016-05-09T07:14:52Z</updated>
    <published>2016-05-09T07:14:52Z</published>
    <title>Machine Learning Techniques with Ontology for Subjective Answer
  Evaluation</title>
    <summary>  Computerized Evaluation of English Essays is performed using Machine learning
techniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual
Evaluation Understudy and Maximum Entropy. Ontology, a concept map of domain
knowledge, can enhance the performance of these techniques. Use of Ontology
makes the evaluation process holistic as presence of keywords, synonyms, the
right word combination and coverage of concepts can be checked. In this paper,
the above mentioned techniques are implemented both with and without Ontology
and tested on common input data consisting of technical answers of Computer
Science. Domain Ontology of Computer Graphics is designed and developed. The
software used for implementation includes Java Programming Language and tools
such as MATLAB, Prot\'eg\'e, etc. Ten questions from Computer Graphics with
sixty answers for each question are used for testing. The results are analyzed
and it is concluded that the results are more accurate with use of Ontology.
</summary>
    <author>
      <name>M. Syamala Devi</name>
    </author>
    <author>
      <name>Himani Mittal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijnlc.2016.5201</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijnlc.2016.5201" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures, journal,
  http://airccse.org/journal/ijnlc/current.html 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02401v3</id>
    <updated>2016-07-06T05:46:56Z</updated>
    <published>2016-05-09T02:17:12Z</published>
    <title>Audio Event Detection using Weakly Labeled Data</title>
    <summary>  Acoustic event detection is essential for content analysis and description of
multimedia recordings. The majority of current literature on the topic learns
the detectors through fully-supervised techniques employing strongly labeled
data. However, the labels available for majority of multimedia data are
generally weak and do not provide sufficient detail for such methods to be
employed. In this paper we propose a framework for learning acoustic event
detectors using only weakly labeled data. We first show that audio event
detection using weak labels can be formulated as an Multiple Instance Learning
problem. We then suggest two frameworks for solving multiple-instance learning,
one based on support vector machines, and the other on neural networks. The
proposed methods can help in removing the time consuming and expensive process
of manually annotating data to facilitate fully supervised learning. Moreover,
it can not only detect events in a recording but can also provide temporal
locations of events in the recording. This helps in obtaining a complete
description of the recording and is notable since temporal information was
never known in the first place in weakly labeled data.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2964284.2964310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2964284.2964310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02401v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02401v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02321v1</id>
    <updated>2016-05-08T13:52:41Z</updated>
    <published>2016-05-08T13:52:41Z</published>
    <title>Asymmetric Move Selection Strategies in Monte-Carlo Tree Search:
  Minimizing the Simple Regret at Max Nodes</title>
    <summary>  The combination of multi-armed bandit (MAB) algorithms with Monte-Carlo tree
search (MCTS) has made a significant impact in various research fields. The UCT
algorithm, which combines the UCB bandit algorithm with MCTS, is a good example
of the success of this combination. The recent breakthrough made by AlphaGo,
which incorporates convolutional neural networks with bandit algorithms in
MCTS, also highlights the necessity of bandit algorithms in MCTS. However,
despite the various investigations carried out on MCTS, nearly all of them
still follow the paradigm of treating every node as an independent instance of
the MAB problem, and applying the same bandit algorithm and heuristics on every
node. As a result, this paradigm may leave some properties of the game tree
unexploited. In this work, we propose that max nodes and min nodes have
different concerns regarding their value estimation, and different bandit
algorithms should be applied accordingly. We develop the Asymmetric-MCTS
algorithm, which is an MCTS variant that applies a simple regret algorithm on
max nodes, and the UCB algorithm on min nodes. We will demonstrate the
performance of the Asymmetric-MCTS algorithm on the game of $9\times 9$ Go,
$9\times 9$ NoGo, and Othello.
</summary>
    <author>
      <name>Yun-Ching Liu</name>
    </author>
    <author>
      <name>Yoshimasa Tsuruoka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to the 2016 IEEE Computational Intelligence and Games
  Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02160v1</id>
    <updated>2016-05-07T09:09:08Z</updated>
    <published>2016-05-07T09:09:08Z</published>
    <title>Belief Merging by Source Reliability Assessment</title>
    <summary>  Merging beliefs requires the plausibility of the sources of the information
to be merged. They are typically assumed equally reliable in lack of hints
indicating otherwise; yet, a recent line of research spun from the idea of
deriving this information from the revision process itself. In particular, the
history of previous revisions and previous merging examples provide information
for performing subsequent mergings.
  Yet, no examples or previous revisions may be available. In spite of the
apparent lack of information, something can still be inferred by a
try-and-check approach: a relative reliability ordering is assumed, the merging
process is performed based on it, and the result is compared with the original
information. The outcome of this check may be incoherent with the initial
assumption, like when a completely reliable source is rejected some of the
information it provided. In such cases, the reliability ordering assumed in the
first place can be excluded from consideration. The first theorem of this
article proves that such a scenario is indeed possible. Other results are
obtained under various definition of reliability and merging.
</summary>
    <author>
      <name>Paolo Liberatore</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02130v1</id>
    <updated>2016-05-07T02:00:30Z</updated>
    <published>2016-05-07T02:00:30Z</published>
    <title>Robust Dialog State Tracking for Large Ontologies</title>
    <summary>  The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from the
previous three editions as follows: the number of slot-value pairs present in
the ontology is much larger, no spoken language understanding output is given,
and utterances are labeled at the subdialog level. This paper describes a novel
dialog state tracking method designed to work robustly under these conditions,
using elaborate string matching, coreference resolution tailored for dialogs
and a few other improvements. The method can correctly identify many values
that are not explicitly present in the utterance. On the final evaluation, our
method came in first among 7 competing teams and 24 entries. The F1-score
achieved by our method was 9 and 7 percentage points higher than that of the
runner-up for the utterance-level evaluation and for the subdialog-level
evaluation, respectively.
</summary>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <author>
      <name>Ji Young Lee</name>
    </author>
    <author>
      <name>Trung H. Bui</name>
    </author>
    <author>
      <name>Hung H. Bui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted at IWSDS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02129v1</id>
    <updated>2016-05-07T01:55:51Z</updated>
    <published>2016-05-07T01:55:51Z</published>
    <title>Adobe-MIT submission to the DSTC 4 Spoken Language Understanding pilot
  task</title>
    <summary>  The Dialog State Tracking Challenge 4 (DSTC 4) proposes several pilot tasks.
In this paper, we focus on the spoken language understanding pilot task, which
consists of tagging a given utterance with speech acts and semantic slots. We
compare different classifiers: the best system obtains 0.52 and 0.67 F1-scores
on the test set for speech act recognition for the tourist and the guide
respectively, and 0.52 F1-score for semantic tagging for both the guide and the
tourist.
</summary>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <author>
      <name>Ji Young Lee</name>
    </author>
    <author>
      <name>Trung H. Bui</name>
    </author>
    <author>
      <name>Hung H. Bui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted at IWSDS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02097v1</id>
    <updated>2016-05-06T20:46:34Z</updated>
    <published>2016-05-06T20:46:34Z</published>
    <title>ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement
  Learning</title>
    <summary>  The recent advances in deep neural networks have led to effective
vision-based reinforcement learning methods that have been employed to obtain
human-level controllers in Atari 2600 games from pixel data. Atari 2600 games,
however, do not resemble real-world tasks since they involve non-realistic 2D
environments and the third-person perspective. Here, we propose a novel
test-bed platform for reinforcement learning research from raw visual
information which employs the first-person perspective in a semi-realistic 3D
world. The software, called ViZDoom, is based on the classical first-person
shooter video game, Doom. It allows developing bots that play the game using
the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a
convenient mechanism of user scenarios. In the experimental part, we test the
environment by trying to learn bots for two scenarios: a basic move-and-shoot
task and a more complex maze-navigation problem. Using convolutional deep
neural networks with Q-learning and experience replay, for both scenarios, we
were able to train competent bots, which exhibit human-like behaviors. The
results confirm the utility of ViZDoom as an AI research platform and imply
that visual reinforcement learning in 3D realistic first-person perspective
environments is feasible.
</summary>
    <author>
      <name>Michał Kempka</name>
    </author>
    <author>
      <name>Marek Wydmuch</name>
    </author>
    <author>
      <name>Grzegorz Runc</name>
    </author>
    <author>
      <name>Jakub Toczek</name>
    </author>
    <author>
      <name>Wojciech Jaśkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.08098v1</id>
    <updated>2016-05-06T20:04:40Z</updated>
    <published>2016-05-06T20:04:40Z</published>
    <title>The Actias system: supervised multi-strategy learning paradigm using
  categorical logic</title>
    <summary>  One of the most difficult problems in the development of intelligent systems
is the construction of the underlying knowledge base. As a consequence, the
rate of progress in the development of this type of system is directly related
to the speed with which knowledge bases can be assembled, and on its quality.
We attempt to solve the knowledge acquisition problem, for a Business
Information System, developing a supervised multistrategy learning paradigm.
This paradigm is centred on a collaborative data mining strategy, where groups
of experts collaborate using data-mining process on the supervised acquisition
of new knowledge extracted from heterogeneous machine learning data models.
  The Actias system is our approach to this paradigm. It is the result of
applying the graphic logic based language of sketches to knowledge integration.
The system is a data mining collaborative workplace, where the Information
System knowledge base is an algebraic structure. It results from the
integration of background knowledge with new insights extracted from data
models, generated for specific data modelling tasks, and represented as rules
using the sketches language.
</summary>
    <author>
      <name>Carlos Leandro</name>
    </author>
    <author>
      <name>Helder Pita</name>
    </author>
    <author>
      <name>Luís Monteiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, conference ICKEDS'04</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.08098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.08098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="18C30, 03G30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02046v1</id>
    <updated>2016-05-06T19:17:33Z</updated>
    <published>2016-05-06T19:17:33Z</published>
    <title>Low-Complexity Stochastic Generalized Belief Propagation</title>
    <summary>  The generalized belief propagation (GBP), introduced by Yedidia et al., is an
extension of the belief propagation (BP) algorithm, which is widely used in
different problems involved in calculating exact or approximate marginals of
probability distributions. In many problems, it has been observed that the
accuracy of GBP considerably outperforms that of BP. However, because in
general the computational complexity of GBP is higher than BP, its application
is limited in practice.
  In this paper, we introduce a stochastic version of GBP called stochastic
generalized belief propagation (SGBP) that can be considered as an extension to
the stochastic BP (SBP) algorithm introduced by Noorshams et al. They have
shown that SBP reduces the complexity per iteration of BP by an order of
magnitude in alphabet size. In contrast to SBP, SGBP can reduce the computation
complexity if certain topological conditions are met by the region graph
associated to a graphical model. However, this reduction can be larger than
only one order of magnitude in alphabet size. In this paper, we characterize
these conditions and the amount of computation gain that we can obtain by using
SGBP. Finally, using similar proof techniques employed by Noorshams et al., for
general graphical models satisfy contraction conditions, we prove the
asymptotic convergence of SGBP to the unique GBP fixed point, as well as
providing non-asymptotic upper bounds on the mean square error and on the high
probability error.
</summary>
    <author>
      <name>Farzin Haddadpour</name>
    </author>
    <author>
      <name>Mahdi Jafari Siavoshani</name>
    </author>
    <author>
      <name>Morteza Noshad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 11 figures, a shorter version of this paper was accepted in
  ISIT'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02038v1</id>
    <updated>2016-05-06T19:06:56Z</updated>
    <published>2016-05-06T19:06:56Z</published>
    <title>Multi-component approach to the bipartite Boolean quadratic programming
  problem</title>
    <summary>  We study the Bipartite Boolean Quadratic Programming Problem (BBQP) which is
an extension of the well known Boolean Quadratic Programming Problem (BQP).
Applications of the BBQP include mining discrete patterns from binary data,
approximating matrices by rank-one binary matrices, computing the cut-norm of a
matrix, and solving optimisation problems such as maximum weight biclique,
bipartite maximum weight cut, maximum weight induced sub-graph of a bipartite
graph, etc. For the BBQP, we first present several algorithmic components,
specifically, hillclimbers and mutations, and then show how to combine them in
a high-performance metaheuristic. Instead of hand-tuning a standard
metaheuristic to test the efficiency of the hybrid of the components, we chose
to use an automated generation of a multi-component metaheuristic to save human
time, and also improve objectivity in the analysis and comparisons of
components. For this we designed a new metaheuristic schema which we call
Conditional Markov Chain Search (CMCS). We show that CMCS is flexible enough to
model several standard metaheuristics; this flexibility is controlled by
multiple numeric parameters, and so is convenient for automated generation. We
study the configurations revealed by our approach and show that the best of
them outperforms the previous state-of-the-art BBQP algorithm by several orders
of magnitude. In our experiments we use benchmark instances introduced in the
preliminary version of this paper and described here, which have already become
the de facto standard in the BBQP literature.
</summary>
    <author>
      <name>Daniel Karapetyan</name>
    </author>
    <author>
      <name>Abraham P. Punnen</name>
    </author>
    <author>
      <name>Andrew J. Parkes</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01995v2</id>
    <updated>2016-06-17T02:23:48Z</updated>
    <published>2016-05-06T16:24:03Z</published>
    <title>Beyond knowing that: a new generation of epistemic logics</title>
    <summary>  Epistemic logic has become a major field of philosophical logic ever since
the groundbreaking work by Hintikka (1962). Despite its various successful
applications in theoretical computer science, AI, and game theory, the
technical development of the field has been mainly focusing on the
propositional part, i.e., the propositional modal logics of "knowing that".
However, knowledge is expressed in everyday life by using various other
locutions such as "knowing whether", "knowing what", "knowing how" and so on
(knowing-wh hereafter). Such knowledge expressions are better captured in
quantified epistemic logic, as was already discussed by Hintikka (1962) and his
sequel works at length. This paper aims to draw the attention back again to
such a fascinating but largely neglected topic. We first survey what Hintikka
and others did in the literature of quantified epistemic logic, and then
advocate a new quantifier-free approach to study the epistemic logics of
knowing-wh, which we believe can balance expressivity and complexity, and
capture the essential reasoning patterns about knowing-wh. We survey our recent
line of work on the epistemic logics of "knowing whether", "knowing what" and
"knowing how" to demonstrate the use of this new approach.
</summary>
    <author>
      <name>Yanjing Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, to appear in Jaakko Hintikka on knowledge and game
  theoretical semantics, Springer's Outstanding Contributions to Logic Series</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01939v1</id>
    <updated>2016-05-06T13:52:45Z</updated>
    <published>2016-05-06T13:52:45Z</published>
    <title>Energy Disaggregation for Real-Time Building Flexibility Detection</title>
    <summary>  Energy is a limited resource which has to be managed wisely, taking into
account both supply-demand matching and capacity constraints in the
distribution grid. One aspect of the smart energy management at the building
level is given by the problem of real-time detection of flexible demand
available. In this paper we propose the use of energy disaggregation techniques
to perform this task. Firstly, we investigate the use of existing
classification methods to perform energy disaggregation. A comparison is
performed between four classifiers, namely Naive Bayes, k-Nearest Neighbors,
Support Vector Machine and AdaBoost. Secondly, we propose the use of Restricted
Boltzmann Machine to automatically perform feature extraction. The extracted
features are then used as inputs to the four classifiers and consequently shown
to improve their accuracy. The efficiency of our approach is demonstrated on a
real database consisting of detailed appliance-level measurements with high
temporal resolution, which has been used for energy disaggregation in previous
studies, namely the REDD. The results show robustness and good generalization
capabilities to newly presented buildings with at least 96% accuracy.
</summary>
    <author>
      <name>Elena Mocanu</name>
    </author>
    <author>
      <name>Phuong H. Nguyen</name>
    </author>
    <author>
      <name>Madeleine Gibescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE PES General Meeting, 2016, Boston, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01846v1</id>
    <updated>2016-05-06T07:39:19Z</updated>
    <published>2016-05-06T07:39:19Z</published>
    <title>The KB paradigm and its application to interactive configuration</title>
    <summary>  The knowledge base paradigm aims to express domain knowledge in a rich formal
language, and to use this domain knowledge as a knowledge base to solve various
problems and tasks that arise in the domain by applying multiple forms of
inference. As such, the paradigm applies a strict separation of concerns
between information and problem solving. In this paper, we analyze the
principles and feasibility of the knowledge base paradigm in the context of an
important class of applications: interactive configuration problems. In
interactive configuration problems, a configuration of interrelated objects
under constraints is searched, where the system assists the user in reaching an
intended configuration. It is widely recognized in industry that good software
solutions for these problems are very difficult to develop. We investigate such
problems from the perspective of the KB paradigm. We show that multiple
functionalities in this domain can be achieved by applying different forms of
logical inferences on a formal specification of the configuration domain. We
report on a proof of concept of this approach in a real-life application with a
banking company. To appear in Theory and Practice of Logic Programming (TPLP).
</summary>
    <author>
      <name>Pieter Van Hertum</name>
    </author>
    <author>
      <name>Ingmar Dasseville</name>
    </author>
    <author>
      <name>Gerda Janssens</name>
    </author>
    <author>
      <name>Marc Denecker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/S1471068416000156</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/S1471068416000156" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Theory and Practice of Logic Programming (TPLP)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01778v1</id>
    <updated>2016-05-05T22:20:00Z</updated>
    <published>2016-05-05T22:20:00Z</published>
    <title>Combinatorial Aspects of the Distribution of Rough Objects</title>
    <summary>  The inverse problem of general rough sets, considered by the present author
in some of her earlier papers, in one of its manifestations is essentially the
question of when an agent's view about crisp and non crisp objects over a set
of objects has a rough evolution. In this research the nature of the problem is
examined from number-theoretic and combinatorial perspectives under very few
assumptions about the nature of data and some necessary conditions are proved.
</summary>
    <author>
      <name>A. Mani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages, 5 Figures, Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05E99, 94D99, 06A11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01703v1</id>
    <updated>2016-05-05T19:34:08Z</updated>
    <published>2016-05-05T19:34:08Z</published>
    <title>A note on adjusting $R^2$ for using with cross-validation</title>
    <summary>  We show how to adjust the coefficient of determination ($R^2$) when used for
measuring predictive accuracy via leave-one-out cross-validation.
</summary>
    <author>
      <name>Indre Zliobaite</name>
    </author>
    <author>
      <name>Nikolaj Tatti</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01681v1</id>
    <updated>2016-05-05T18:29:56Z</updated>
    <published>2016-05-05T18:29:56Z</published>
    <title>Brain Emotional Learning-Based Prediction Model (For Long-Term Chaotic
  Prediction Applications)</title>
    <summary>  This study suggests a new prediction model for chaotic time series inspired
by the brain emotional learning of mammals. We describe the structure and
function of this model, which is referred to as BELPM (Brain Emotional
Learning-Based Prediction Model). Structurally, the model mimics the connection
between the regions of the limbic system, and functionally it uses weighted k
nearest neighbors to imitate the roles of those regions. The learning algorithm
of BELPM is defined using steepest descent (SD) and the least square estimator
(LSE). Two benchmark chaotic time series, Lorenz and Henon, have been used to
evaluate the performance of BELPM. The obtained results have been compared with
those of other prediction methods. The results show that BELPM has the
capability to achieve a reasonable accuracy for long-term prediction of chaotic
time series, using a limited amount of training data and a reasonably low
computational time.
</summary>
    <author>
      <name>Mahboobeh Parsapoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01652v1</id>
    <updated>2016-05-05T17:00:44Z</updated>
    <published>2016-05-05T17:00:44Z</published>
    <title>LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues</title>
    <summary>  We introduce an LSTM-based method for dynamically integrating several
word-prediction experts to obtain a conditional language model which can be
good simultaneously at several subtasks. We illustrate this general approach
with an application to dialogue where we integrate a neural chat model, good at
conversational aspects, with a neural question-answering model, good at
retrieving precise information from a knowledge-base, and show how the
integration combines the strengths of the independent components. We hope that
this focused contribution will attract attention on the benefits of using such
mixtures of experts in NLP.
</summary>
    <author>
      <name>Phong Le</name>
    </author>
    <author>
      <name>Marc Dymetman</name>
    </author>
    <author>
      <name>Jean-Michel Renders</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01622v1</id>
    <updated>2016-05-05T15:20:33Z</updated>
    <published>2016-05-05T15:20:33Z</published>
    <title>Improving abcdSAT by At-Least-One Recently Used Clause Management
  Strategy</title>
    <summary>  We improve further the 2015 version of abcdSAT by various heuristics such as
at-least-one recently used strategy, learnt clause database approximation
reduction etc. Based on the requirement of different tracks at the SAT
Competition 2016, we develop three versions of abcdSAT: drup, inc and lim,
which participate in the competition of main (agile), incremental library and
no-limit track, respectively.
</summary>
    <author>
      <name>Jingchao Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01534v1</id>
    <updated>2016-05-05T09:15:55Z</updated>
    <published>2016-05-05T09:15:55Z</published>
    <title>ODE - Augmented Training Improves Anomaly Detection in Sensor Data from
  Machines</title>
    <summary>  Machines of all kinds from vehicles to industrial equipment are increasingly
instrumented with hundreds of sensors. Using such data to detect anomalous
behaviour is critical for safety and efficient maintenance. However, anomalies
occur rarely and with great variety in such systems, so there is often
insufficient anomalous data to build reliable detectors. A standard approach to
mitigate this problem is to use one class methods relying only on data from
normal behaviour. Unfortunately, even these approaches are more likely to fail
in the scenario of a dynamical system with manual control input(s). Normal
behaviour in response to novel control input(s) might look very different to
the learned detector which may be incorrectly detected as anomalous. In this
paper, we address this issue by modelling time-series via Ordinary Differential
Equations (ODE) and utilising such an ODE model to simulate the behaviour of
dynamical systems under varying control inputs. The available data is then
augmented with data generated from the ODE, and the anomaly detector is
retrained on this augmented dataset. Experiments demonstrate that ODE-augmented
training data allows better coverage of possible control input(s) and results
in learning more accurate distinctions between normal and anomalous behaviour
in time-series.
</summary>
    <author>
      <name>Mohit Yadav</name>
    </author>
    <author>
      <name>Pankaj Malhotra</name>
    </author>
    <author>
      <name>Lovekesh Vig</name>
    </author>
    <author>
      <name>K Sriram</name>
    </author>
    <author>
      <name>Gautam Shroff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at NIPS Time-series Workshop - 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01459v1</id>
    <updated>2016-05-04T23:48:16Z</updated>
    <published>2016-05-04T23:48:16Z</published>
    <title>Movement Coordination in Human-Robot Teams: A Dynamical Systems Approach</title>
    <summary>  In order to be effective teammates, robots need to be able to understand
high-level human behavior to recognize, anticipate, and adapt to human motion.
We have designed a new approach to enable robots to perceive human group motion
in real-time, anticipate future actions, and synthesize their own motion
accordingly. We explore this within the context of joint action, where humans
and robots move together synchronously. In this paper, we present an
anticipation method which takes high-level group behavior into account. We
validate the method within a human-robot interaction scenario, where an
autonomous mobile robot observes a team of human dancers, and then successfully
and contingently coordinates its movements to "join the dance". We compared the
results of our anticipation method to move the robot with another method which
did not rely on high-level group behavior, and found our method performed
better both in terms of more closely synchronizing the robot's motion to the
team, and also exhibiting more contingent and fluent motion. These findings
suggest that the robot performs better when it has an understanding of
high-level group behavior than when it does not. This work will help enable
others in the robotics community to build more fluent and adaptable robots in
the future.
</summary>
    <author>
      <name>Tariq Iqbal</name>
    </author>
    <author>
      <name>Samantha Rack</name>
    </author>
    <author>
      <name>Laurel D. Riek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures, IEEE Transactions on Robotics 2016 preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.9; I.2.11; H.5.3; I.5.5; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03009v2</id>
    <updated>2016-06-28T20:44:09Z</updated>
    <published>2016-05-04T20:19:05Z</published>
    <title>Consciousness is Pattern Recognition</title>
    <summary>  This is a proof of the strong AI hypothesis, i.e. that machines can be
conscious. It is a phenomenological proof that pattern-recognition and
subjective consciousness are the same activity in different terms. Therefore,
it proves that essential subjective processes of consciousness are computable,
and identifies significant traits and requirements of a conscious system. Since
Husserl, many philosophers have accepted that consciousness consists of
memories of logical connections between an ego and external objects. These
connections are called "intentions." Pattern recognition systems are achievable
technical artifacts. The proof links this respected introspective philosophical
theory of consciousness with technical art. The proof therefore endorses the
strong AI hypothesis and may therefore also enable a theoretically-grounded
form of artificial intelligence called a "synthetic intentionality," able to
synthesize, generalize, select and repeat intentions. If the pattern
recognition is reflexive, able to operate on the set of intentions, and
flexible, with several methods of synthesizing intentions, an SI may be a
particularly strong form of AI. Similarities and possible applications to
several AI paradigms are discussed. The article then addresses some problems:
The proof's limitations, reflexive cognition, Searles' Chinese room, and how an
SI could "understand" "meanings" and "be creative."
</summary>
    <author>
      <name>Ray Van De Walker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; Now describes the utility of the proof. Lemma A3 is
  improved. The root lemma is clarified. Included and excused some basic
  objections. Reordered the speculations, objections and excuses to be more
  coherent. Added paragraphs and references to aid some AI paradigms. Added my
  orcid and revised the abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03009v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03009v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01335v1</id>
    <updated>2016-05-04T16:23:34Z</updated>
    <published>2016-05-04T16:23:34Z</published>
    <title>Learning from the memory of Atari 2600</title>
    <summary>  We train a number of neural networks to play games Bowling, Breakout and
Seaquest using information stored in the memory of a video game console Atari
2600. We consider four models of neural networks which differ in size and
architecture: two networks which use only information contained in the RAM and
two mixed networks which use both information in the RAM and information from
the screen. As the benchmark we used the convolutional model proposed in NIPS
and received comparable results in all considered games. Quite surprisingly, in
the case of Seaquest we were able to train RAM-only agents which behave better
than the benchmark screen-only agent. Mixing screen and RAM did not lead to an
improved performance comparing to screen-only and RAM-only agents.
</summary>
    <author>
      <name>Jakub Sygnowski</name>
    </author>
    <author>
      <name>Henryk Michalewski</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01596v1</id>
    <updated>2016-05-04T16:16:10Z</updated>
    <published>2016-05-04T16:16:10Z</published>
    <title>Notes on a model for fuzzy computing</title>
    <summary>  In these notes we propose a setting for fuzzy computing in a framework
similar to that of well-established theories of computation: boolean, and
quantum computing. Our efforts have been directed towards stressing the formal
similarities: there is a common pattern underlying these three theories. We
tried to conform our approach, as much as possible, to this pattern. This work
was part of a project jointly with Professor Vittorio Cafagna. Professor
Cafagna passed away unexpectedly in 2007. His intellectual breadth and
inspiring passion for mathematics is still very well alive.
</summary>
    <author>
      <name>Vittorio Cafagna</name>
    </author>
    <author>
      <name>Gianluca Caterina</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01207v1</id>
    <updated>2016-05-04T10:10:37Z</updated>
    <published>2016-05-04T10:10:37Z</published>
    <title>Ontology-Mediated Queries: Combined Complexity and Succinctness of
  Rewritings via Circuit Complexity</title>
    <summary>  We give solutions to two fundamental computational problems in ontology-based
data access with the W3C standard ontology language OWL 2 QL: the succinctness
problem for first-order rewritings of ontology-mediated queries (OMQs), and the
complexity problem for OMQ answering. We classify OMQs according to the shape
of their conjunctive queries (treewidth, the number of leaves) and the
existential depth of their ontologies. For each of these classes, we determine
the combined complexity of OMQ answering, and whether all OMQs in the class
have polynomial-size first-order, positive existential, and nonrecursive
datalog rewritings. We obtain the succinctness results using hypergraph
programs, a new computational model for Boolean functions, which makes it
possible to connect the size of OMQ rewritings and circuit complexity.
</summary>
    <author>
      <name>Meghyn Bienvenu</name>
    </author>
    <author>
      <name>Stanislav Kikot</name>
    </author>
    <author>
      <name>Roman Kontchakov</name>
    </author>
    <author>
      <name>Vladimir Podolskii</name>
    </author>
    <author>
      <name>Michael Zakharyaschev</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01207v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01207v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01180v1</id>
    <updated>2016-05-04T08:34:17Z</updated>
    <published>2016-05-04T08:34:17Z</published>
    <title>A Step from Probabilistic Programming to Cognitive Architectures</title>
    <summary>  Probabilistic programming is considered as a framework, in which basic
components of cognitive architectures can be represented in unified and elegant
fashion. At the same time, necessity of adopting some component of cognitive
architectures for extending capabilities of probabilistic programming languages
is pointed out. In particular, implicit specification of generative models via
declaration of concepts and links between them is proposed, and usefulness of
declarative knowledge for achieving efficient inference is briefly discussed.
</summary>
    <author>
      <name>Alexey Potapov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01138v1</id>
    <updated>2016-05-04T04:26:06Z</updated>
    <published>2016-05-04T04:26:06Z</published>
    <title>A Comparative Evaluation of Approximate Probabilistic Simulation and
  Deep Neural Networks as Accounts of Human Physical Scene Understanding</title>
    <summary>  Humans demonstrate remarkable abilities to predict physical events in complex
scenes. Two classes of models for physical scene understanding have recently
been proposed: "Intuitive Physics Engines", or IPEs, which posit that people
make predictions by running approximate probabilistic simulations in causal
mental models similar in nature to video-game physics engines, and memory-based
models, which make judgments based on analogies to stored experiences of
previously encountered scenes and physical outcomes. Versions of the latter
have recently been instantiated in convolutional neural network (CNN)
architectures. Here we report four experiments that, to our knowledge, are the
first rigorous comparisons of simulation-based and CNN-based models, where both
approaches are concretely instantiated in algorithms that can run on raw image
inputs and produce as outputs physical judgments such as whether a stack of
blocks will fall. Both approaches can achieve super-human accuracy levels and
can quantitatively predict human judgments to a similar degree, but only the
simulation-based models generalize to novel situations in ways that people do,
and are qualitatively consistent with systematic perceptual illusions and
judgment asymmetries that people show.
</summary>
    <author>
      <name>Renqiao Zhang</name>
    </author>
    <author>
      <name>Jiajun Wu</name>
    </author>
    <author>
      <name>Chengkai Zhang</name>
    </author>
    <author>
      <name>William T. Freeman</name>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CogSci 2016 as an oral presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00788v2</id>
    <updated>2016-05-29T14:20:01Z</updated>
    <published>2016-05-03T08:38:34Z</published>
    <title>Online Learning of Commission Avoidant Portfolio Ensembles</title>
    <summary>  We present a novel online ensemble learning strategy for portfolio selection.
The new strategy controls and exploits any set of commission-oblivious
portfolio selection algorithms. The strategy handles transaction costs using a
novel commission avoidance mechanism. We prove a logarithmic regret bound for
our strategy with respect to optimal mixtures of the base algorithms. Numerical
examples validate the viability of our method and show significant improvement
over the state-of-the-art.
</summary>
    <author>
      <name>Guy Uziel</name>
    </author>
    <author>
      <name>Ran El-Yaniv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1604.03266</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00787v1</id>
    <updated>2016-05-03T08:34:45Z</updated>
    <published>2016-05-03T08:34:45Z</published>
    <title>Obstacle evasion using fuzzy logic in a sliding blades problem
  environment</title>
    <summary>  This paper discusses obstacle avoidance using fuzzy logic and shortest path
algorithm. This paper also introduces the sliding blades problem and
illustrates how a drone can navigate itself through the swinging blade
obstacles while tracing a semi-optimal path and also maintaining constant
velocity
</summary>
    <author>
      <name>Shadrack Kimutai</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00702v2</id>
    <updated>2016-05-04T00:26:14Z</updated>
    <published>2016-05-02T22:44:54Z</published>
    <title>A heuristic algorithm for a single vehicle static bike sharing
  rebalancing problem</title>
    <summary>  The static bike rebalancing problem (SBRP) concerns the task of repositioning
bikes among stations in self-service bike-sharing systems. This problem can be
seen as a variant of the one-commodity pickup and delivery vehicle routing
problem, where multiple visits are allowed to be performed at each station,
i.e., the demand of a station is allowed to be split. Moreover, a vehicle may
temporarily drop its load at a station, leaving it in excess or, alternatively,
collect more bikes from a station (even all of them), thus leaving it in
default. Both cases require further visits in order to meet the actual demands
of such station. This paper deals with a particular case of the SBRP, in which
only a single vehicle is available and the objective is to find a least-cost
route that meets the demand of all stations and does not violate the minimum
(zero) and maximum (vehicle capacity) load limits along the tour. Therefore,
the number of bikes to be collected or delivered at each station should be
appropriately determined in order to respect such constraints. We propose an
iterated local search (ILS) based heuristic to solve the problem. The ILS
algorithm was tested on 980 benchmark instances from the literature and the
results obtained are quite competitive when compared to other existing methods.
Moreover, our heuristic was capable of finding most of the known optimal
solutions and also of improving the results on a number of open instances.
</summary>
    <author>
      <name>Fábio Cruz</name>
    </author>
    <author>
      <name>Anand Subramanian</name>
    </author>
    <author>
      <name>Bruno P. Bruck</name>
    </author>
    <author>
      <name>Manuel Iori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report Universidade Federal da Para\'iba-UFPB, Brazil</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00702v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00702v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C59" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00686v1</id>
    <updated>2016-05-02T20:51:43Z</updated>
    <published>2016-05-02T20:51:43Z</published>
    <title>Disjunctive Normal Form Schemes for Heterogeneous Attributed Graphs</title>
    <summary>  Several 'edge-discovery' applications over graph-based data models are known
to have worst-case quadratic complexity, even if the discovered edges are
sparse. One example is the generic link discovery problem between two graphs,
which has invited research interest in several communities. Specific versions
of this problem include link prediction in social networks, ontology alignment
between metadata-rich RDF data, approximate joins, and entity resolution
between instance-rich data. As large datasets continue to proliferate, reducing
quadratic complexity to make the task practical is an important research
problem. Within the entity resolution community, the problem is commonly
referred to as blocking. A particular class of learnable blocking schemes is
known as Disjunctive Normal Form (DNF) blocking schemes, and has emerged as
state-of-the art for homogeneous (i.e. same-schema) tabular data. Despite the
promise of these schemes, a formalism or learning framework has not been
developed for them when input data instances are generic, attributed graphs
possessing both node and edge heterogeneity. With such a development, the
complexity-reducing scope of DNF schemes becomes applicable to a variety of
problems, including entity resolution and type alignment between heterogeneous
RDF graphs, and link prediction in networks represented as attributed graphs.
This paper presents a graph-theoretic formalism for DNF schemes, and
investigates their learnability in an optimization framework. Experimentally,
the DNF schemes learned on pairs of heterogeneous RDF graphs are demonstrated
to achieve high complexity-reductions (98.25% across ten RDF test cases) at
little cost to coverage, and with high reliability (&lt;2.5% standard deviation).
Finally, one extant class of RDF blocking schemes is shown to be a special case
of DNF schemes.
</summary>
    <author>
      <name>Mayank Kejriwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, currently submitted to ISWC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00596v1</id>
    <updated>2016-05-02T18:13:04Z</updated>
    <published>2016-05-02T18:13:04Z</published>
    <title>Graph Clustering Bandits for Recommendation</title>
    <summary>  We investigate an efficient context-dependent clustering technique for
recommender systems based on exploration-exploitation strategies through
multi-armed bandits over multiple users. Our algorithm dynamically groups users
based on their observed behavioral similarity during a sequence of logged
activities. In doing so, the algorithm reacts to the currently served user by
shaping clusters around him/her but, at the same time, it explores the
generation of clusters over users which are not currently engaged. We motivate
the effectiveness of this clustering policy, and provide an extensive empirical
analysis on real-world datasets, showing scalability and improved prediction
performance over state-of-the-art methods for sequential clustering of users in
multi-armed bandit scenarios.
</summary>
    <author>
      <name>Shuai Li</name>
    </author>
    <author>
      <name>Claudio Gentile</name>
    </author>
    <author>
      <name>Alexandros Karatzoglou</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00495v1</id>
    <updated>2016-05-02T14:08:23Z</updated>
    <published>2016-05-02T14:08:23Z</published>
    <title>Coalition Formability Semantics with Conflict-Eliminable Sets of
  Arguments</title>
    <summary>  We consider abstract-argumentation-theoretic coalition formability in this
work. Taking a model from political alliance among political parties, we will
contemplate profitability, and then formability, of a coalition. As is commonly
understood, a group forms a coalition with another group for a greater good,
the goodness measured against some criteria. As is also commonly understood,
however, a coalition may deliver benefits to a group X at the sacrifice of
something that X was able to do before coalition formation, which X may be no
longer able to do under the coalition. Use of the typical conflict-free sets of
arguments is not very fitting for accommodating this aspect of coalition, which
prompts us to turn to a weaker notion, conflict-eliminability, as a property
that a set of arguments should primarily satisfy. We require numerical
quantification of attack strengths as well as of argument strengths for its
characterisation. We will first analyse semantics of profitability of a given
conflict-eliminable set forming a coalition with another conflict-eliminable
set, and will then provide four coalition formability semantics, each of which
formalises certain utility postulate(s) taking the coalition profitability into
account.
</summary>
    <author>
      <name>Ryuta Arisaka</name>
    </author>
    <author>
      <name>Ken Satoh</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00303v1</id>
    <updated>2016-05-01T20:19:25Z</updated>
    <published>2016-05-01T20:19:25Z</published>
    <title>A Self-Taught Artificial Agent for Multi-Physics Computational Model
  Personalization</title>
    <summary>  Personalization is the process of fitting a model to patient data, a critical
step towards application of multi-physics computational models in clinical
practice. Designing robust personalization algorithms is often a tedious,
time-consuming, model- and data-specific process. We propose to use artificial
intelligence concepts to learn this task, inspired by how human experts
manually perform it. The problem is reformulated in terms of reinforcement
learning. In an off-line phase, Vito, our self-taught artificial agent, learns
a representative decision process model through exploration of the
computational model: it learns how the model behaves under change of
parameters. The agent then automatically learns an optimal strategy for on-line
personalization. The algorithm is model-independent; applying it to a new model
requires only adjusting few hyper-parameters of the agent and defining the
observations to match. The full knowledge of the model itself is not required.
Vito was tested in a synthetic scenario, showing that it could learn how to
optimize cost functions generically. Then Vito was applied to the inverse
problem of cardiac electrophysiology and the personalization of a whole-body
circulation model. The obtained results suggested that Vito could achieve
equivalent, if not better goodness of fit than standard methods, while being
more robust (up to 11% higher success rates) and with faster (up to seven
times) convergence rate. Our artificial intelligence approach could thus make
personalization algorithms generalizable and self-adaptable to any patient and
any model.
</summary>
    <author>
      <name>Dominik Neumann</name>
    </author>
    <author>
      <name>Tommaso Mansi</name>
    </author>
    <author>
      <name>Lucian Itu</name>
    </author>
    <author>
      <name>Bogdan Georgescu</name>
    </author>
    <author>
      <name>Elham Kayvanpour</name>
    </author>
    <author>
      <name>Farbod Sedaghat-Hamedani</name>
    </author>
    <author>
      <name>Ali Amr</name>
    </author>
    <author>
      <name>Jan Haas</name>
    </author>
    <author>
      <name>Hugo Katus</name>
    </author>
    <author>
      <name>Benjamin Meder</name>
    </author>
    <author>
      <name>Stefan Steidl</name>
    </author>
    <author>
      <name>Joachim Hornegger</name>
    </author>
    <author>
      <name>Dorin Comaniciu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Medical Image Analysis, Elsevier</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00241v1</id>
    <updated>2016-05-01T11:56:01Z</updated>
    <published>2016-05-01T11:56:01Z</published>
    <title>Common-Description Learning: A Framework for Learning Algorithms and
  Generating Subproblems from Few Examples</title>
    <summary>  Current learning algorithms face many difficulties in learning simple
patterns and using them to learn more complex ones. They also require more
examples than humans do to learn the same pattern, assuming no prior knowledge.
In this paper, a new learning framework is introduced that is called
common-description learning (CDL). This framework has been tested on 32 small
multi-task datasets, and the results show that it was able to learn complex
algorithms from a few number of examples. The final model is perfectly
interpretable and its depth depends on the question. What is meant by depth
here is that whenever needed, the model learns to break down the problem into
simpler subproblems and solves them using previously learned models. Finally,
we explain the capabilities of our framework in discovering complex relations
in data and how it can help in improving language understanding in machines.
</summary>
    <author>
      <name>Basem G. El-Barashy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00164v2</id>
    <updated>2016-08-05T22:15:48Z</updated>
    <published>2016-04-30T20:39:16Z</published>
    <title>Look-ahead before you leap: end-to-end active recognition by forecasting
  the effect of motion</title>
    <summary>  Visual recognition systems mounted on autonomous moving agents face the
challenge of unconstrained data, but simultaneously have the opportunity to
improve their performance by moving to acquire new views of test data. In this
work, we first show how a recurrent neural network-based system may be trained
to perform end-to-end learning of motion policies suited for this "active
recognition" setting. Further, we hypothesize that active vision requires an
agent to have the capacity to reason about the effects of its motions on its
view of the world. To verify this hypothesis, we attempt to induce this
capacity in our active recognition pipeline, by simultaneously learning to
forecast the effects of the agent's motions on its internal representation of
the environment conditional on all past views. Results across two challenging
datasets confirm both that our end-to-end system successfully learns meaningful
policies for active category recognition, and that "learning to look ahead"
further boosts recognition performance.
</summary>
    <author>
      <name>Dinesh Jayaraman</name>
    </author>
    <author>
      <name>Kristen Grauman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A preliminary version of the material in this document was filed as
  University of Texas technical report no. UT AI15-06, December, 2015, at:
  http://apps.cs.utexas.edu/tech_reports/reports/ai/AI-2214.pdf, ECCV 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00164v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00164v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00064v1</id>
    <updated>2016-04-30T05:04:08Z</updated>
    <published>2016-04-30T05:04:08Z</published>
    <title>Higher Order Recurrent Neural Networks</title>
    <summary>  In this paper, we study novel neural network structures to better model long
term dependency in sequential data. We propose to use more memory units to keep
track of more preceding states in recurrent neural networks (RNNs), which are
all recurrently fed to the hidden layers as feedback through different weighted
paths. By extending the popular recurrent structure in RNNs, we provide the
models with better short-term memory mechanism to learn long term dependency in
sequences. Analogous to digital filters in signal processing, we call these
structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be
learned using the back-propagation through time method. HORNNs are generally
applicable to a variety of sequence modelling tasks. In this work, we have
examined HORNNs for the language modeling task using two popular data sets,
namely the Penn Treebank (PTB) and English text8 data sets. Experimental
results have shown that the proposed HORNNs yield the state-of-the-art
performance on both data sets, significantly outperforming the regular RNNs as
well as the popular LSTMs.
</summary>
    <author>
      <name>Rohollah Soltani</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08934v1</id>
    <updated>2016-04-29T18:48:53Z</updated>
    <published>2016-04-29T18:48:53Z</published>
    <title>An efficient and expressive similarity measure for relational clustering
  using neighbourhood trees</title>
    <summary>  Clustering is an underspecified task: there are no universal criteria for
what makes a good clustering. This is especially true for relational data,
where similarity can be based on the features of individuals, the relationships
between them, or a mix of both. Existing methods for relational clustering have
strong and often implicit biases in this respect. In this paper, we introduce a
novel similarity measure for relational data. It is the first measure to
incorporate a wide variety of types of similarity, including similarity of
attributes, similarity of relational context, and proximity in a hypergraph. We
experimentally evaluate how using this similarity affects the quality of
clustering on very different types of datasets. The experiments demonstrate
that (a) using this similarity in standard clustering methods consistently
gives good results, whereas other measures work well only on datasets that
match their bias; and (b) on most datasets, the novel similarity outperforms
even the best among the existing ones.
</summary>
    <author>
      <name>Sebastijan Dumancic</name>
    </author>
    <author>
      <name>Hendrik Blockeel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, 4 tables, submitted to ECAI2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08880v1</id>
    <updated>2016-04-29T15:38:44Z</updated>
    <published>2016-04-29T15:38:44Z</published>
    <title>Deep, Convolutional, and Recurrent Models for Human Activity Recognition
  using Wearables</title>
    <summary>  Human activity recognition (HAR) in ubiquitous computing is beginning to
adopt deep learning to substitute for well-established analysis techniques that
rely on hand-crafted feature extraction and classification techniques. From
these isolated applications of custom deep architectures it is, however,
difficult to gain an overview of their suitability for problems ranging from
the recognition of manipulative gestures to the segmentation and identification
of physical activities like running or ascending stairs. In this paper we
rigorously explore deep, convolutional, and recurrent approaches across three
representative datasets that contain movement data captured with wearable
sensors. We describe how to train recurrent approaches in this setting,
introduce a novel regularisation approach, and illustrate how they outperform
the state-of-the-art on a large benchmark dataset. Across thousands of
recognition experiments with randomly sampled model configurations we
investigate the suitability of each model for different tasks in HAR, explore
the impact of hyperparameters using the fANOVA framework, and provide
guidelines for the practitioner who wants to apply deep learning in their
problem setting.
</summary>
    <author>
      <name>Nils Y. Hammerla</name>
    </author>
    <author>
      <name>Shane Halloran</name>
    </author>
    <author>
      <name>Thomas Ploetz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version has been accepted for publication at International
  Joint Conference on Artificial Intelligence (IJCAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08859v2</id>
    <updated>2016-05-27T15:17:34Z</updated>
    <published>2016-04-29T14:53:00Z</published>
    <title>The Z-loss: a shift and scale invariant classification loss belonging to
  the Spherical Family</title>
    <summary>  Despite being the standard loss function to train multi-class neural
networks, the log-softmax has two potential limitations. First, it involves
computations that scale linearly with the number of output classes, which can
restrict the size of problems we are able to tackle with current hardware.
Second, it remains unclear how close it matches the task loss such as the top-k
error rate or other non-differentiable evaluation metrics which we aim to
optimize ultimately. In this paper, we introduce an alternative classification
loss function, the Z-loss, which is designed to address these two issues.
Unlike the log-softmax, it has the desirable property of belonging to the
spherical loss family (Vincent et al., 2015), a class of loss functions for
which training can be performed very efficiently with a complexity independent
of the number of output classes. We show experimentally that it significantly
outperforms the other spherical loss functions previously investigated.
Furthermore, we show on a word language modeling task that it also outperforms
the log-softmax with respect to certain ranking scores, such as top-k scores,
suggesting that the Z-loss has the flexibility to better match the task loss.
These qualities thus makes the Z-loss an appealing candidate to train very
efficiently large output networks such as word-language models or other extreme
classification problems. On the One Billion Word (Chelba et al., 2014) dataset,
we are able to train a model with the Z-loss 40 times faster than the
log-softmax and more than 4 times faster than the hierarchical softmax.
</summary>
    <author>
      <name>Alexandre de Brébisson</name>
    </author>
    <author>
      <name>Pascal Vincent</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08859v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08859v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08781v2</id>
    <updated>2016-06-28T05:29:24Z</updated>
    <published>2016-04-29T11:36:25Z</published>
    <title>Teaching natural language to computers</title>
    <summary>  "Natural Language," whether spoken and attended to by humans, or processed
and generated by computers, requires networked structures that reflect creative
processes in semantic, syntactic, phonetic, linguistic, social, emotional, and
cultural modules. Being able to produce novel and useful behavior following
repeated practice gets to the root of both artificial intelligence and human
language. This paper investigates the modalities involved in language-like
applications that computers -- and programmers -- engage with, and aims to fine
tune the questions we ask to better account for context, self-awareness, and
embodiment.
</summary>
    <author>
      <name>Joseph Corneli</name>
    </author>
    <author>
      <name>Miriam Corneli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, including 1 figure and 3 tables; accepted for presentation
  at IJCAI2016 Workshop on Language Sense on Computers</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08781v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08781v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; D.1.2; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08768v1</id>
    <updated>2016-04-29T10:38:04Z</updated>
    <published>2016-04-29T10:38:04Z</published>
    <title>Supervisory Control for Behavior Composition</title>
    <summary>  We relate behavior composition, a synthesis task studied in AI, to
supervisory control theory from the discrete event systems field. In
particular, we show that realizing (i.e., implementing) a target behavior
module (e.g., a house surveillance system) by suitably coordinating a
collection of available behaviors (e.g., automatic blinds, doors, lights,
cameras, etc.) amounts to imposing a supervisor onto a special discrete event
system. Such a link allows us to leverage on the solid foundations and
extensive work on discrete event systems, including borrowing tools and ideas
from that field. As evidence of that we show how simple it is to introduce
preferences in the mapped framework.
</summary>
    <author>
      <name>Paolo Felli</name>
    </author>
    <author>
      <name>Nitin Yadav</name>
    </author>
    <author>
      <name>Sebastian Sardina</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08709v2</id>
    <updated>2016-06-12T16:48:41Z</updated>
    <published>2016-04-29T07:22:27Z</published>
    <title>"Knowing value" logic as a normal modal logic</title>
    <summary>  Recent years witness a growing interest in nonstandard epistemic logics of
"knowing whether", "knowing what", "knowing how", and so on. These logics are
usually not normal, i.e., the standard axioms and reasoning rules for modal
logic may be invalid. In this paper, we show that the conditional "knowing
value" logic proposed by Wang and Fan \cite{WF13} can be viewed as a disguised
normal modal logic by treating the negation of the Kv operator as a special
diamond. Under this perspective, it turns out that the original first-order
Kripke semantics can be greatly simplified by introducing a ternary relation
$R_i^c$ in standard Kripke models, which associates one world with two
$i$-accessible worlds that do not agree on the value of constant $c$. Under
intuitive constraints, the modal logic based on such Kripke models is exactly
the one studied by Wang and Fan (2013,2014}. Moreover, there is a very natural
binary generalization of the "knowing value" diamond, which, surprisingly, does
not increase the expressive power of the logic. The resulting logic with the
binary diamond has a transparent normal modal system, which sharpens our
understanding of the "knowing value" logic and simplifies some previously hard
problems.
</summary>
    <author>
      <name>Tao Gu</name>
    </author>
    <author>
      <name>Yanjing Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, to appear in Advances in Modal Logic Vol 11</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08709v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08709v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08642v1</id>
    <updated>2016-04-28T22:42:38Z</updated>
    <published>2016-04-28T22:42:38Z</published>
    <title>On the representation and embedding of knowledge bases beyond binary
  relations</title>
    <summary>  The models developed to date for knowledge base embedding are all based on
the assumption that the relations contained in knowledge bases are binary. For
the training and testing of these embedding models, multi-fold (or n-ary)
relational data are converted to triples (e.g., in FB15K dataset) and
interpreted as instances of binary relations. This paper presents a canonical
representation of knowledge bases containing multi-fold relations. We show that
the existing embedding models on the popular FB15K datasets correspond to a
sub-optimal modelling framework, resulting in a loss of structural information.
We advocate a novel modelling framework, which models multi-fold relations
directly using this canonical representation. Using this framework, the
existing TransH model is generalized to a new model, m-TransH. We demonstrate
experimentally that m-TransH outperforms TransH by a large margin, thereby
establishing a new state of the art.
</summary>
    <author>
      <name>Jianfeng Wen</name>
    </author>
    <author>
      <name>Jianxin Li</name>
    </author>
    <author>
      <name>Yongyi Mao</name>
    </author>
    <author>
      <name>Shini Chen</name>
    </author>
    <author>
      <name>Richong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, to appear in IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08612v1</id>
    <updated>2016-04-28T20:41:25Z</updated>
    <published>2016-04-28T20:41:25Z</published>
    <title>Mysteries of Visual Experience</title>
    <summary>  Science is a crowning glory of the human spirit and its applications remain
our best hope for social progress. But there are limitations to current science
and perhaps to any science. The general mind-body problem is known to be
intractable and currently mysterious. This is one of many deep problems that
are universally agreed to be beyond the current purview of Science, including
quantum phenomena, etc. But all of these famous unsolved problems are either
remote from everyday experience (entanglement, dark matter) or are hard to even
define sharply (phenomenology, consciousness, etc.).
  In this note, we will consider some obvious computational problems in vision
that arise every time that we open our eyes and yet are demonstrably
incompatible with current theories of neural computation. The focus will be on
two related phenomena, known as the neural binding problem and the illusion of
a detailed stable visual world.
</summary>
    <author>
      <name>Jerome Feldman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00854v1</id>
    <updated>2016-04-28T16:29:39Z</updated>
    <published>2016-04-28T16:29:39Z</published>
    <title>Fast Simulation of Probabilistic Boolean Networks (Technical Report)</title>
    <summary>  Probabilistic Boolean networks (PBNs) is an important mathematical framework
widely used for modelling and analysing biological systems. PBNs are suited for
modelling large biological systems, which more and more often arise in systems
biology. However, the large system size poses a~significant challenge to the
analysis of PBNs, in particular, to the crucial analysis of their steady-state
behaviour. Numerical methods for performing steady-state analyses suffer from
the state-space explosion problem, which makes the utilisation of statistical
methods the only viable approach. However, such methods require long
simulations of PBNs, rendering the simulation speed a crucial efficiency
factor. For large PBNs and high estimation precision requirements, a slow
simulation speed becomes an obstacle. In this paper, we propose a
structure-based method for fast simulation of PBNs. This method first performs
a network reduction operation and then divides nodes into groups for parallel
simulation. Experimental results show that our method can lead to an
approximately 10 times speedup for computing steady-state probabilities of a
real-life biological network.
</summary>
    <author>
      <name>Andrzej Mizera</name>
    </author>
    <author>
      <name>Jun Pang</name>
    </author>
    <author>
      <name>Qixia Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 3 figures, for CMSB 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.00854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08448v1</id>
    <updated>2016-04-28T15:04:08Z</updated>
    <published>2016-04-28T15:04:08Z</published>
    <title>Exploiting variable associations to configure efficient local search
  algorithms in large-scale binary integer programs</title>
    <summary>  We present a data mining approach for reducing the search space of local
search algorithms in a class of binary integer programs including the set
covering and partitioning problems. We construct a k-nearest neighbor graph by
extracting variable associations from the instance to be solved, in order to
identify promising pairs of flipping variables in the neighborhood search. We
also develop a 4-flip neighborhood local search algorithm that flips four
variables alternately along 4-paths or 4-cycles in the k-nearest neighbor
graph. We incorporate an efficient incremental evaluation of solutions and an
adaptive control of penalty weights into the 4-flip neighborhood local search
algorithm. Computational comparison with the latest solvers shows that our
algorithm performs effectively for large-scale set covering and partitioning
problems.
</summary>
    <author>
      <name>Shunji Umetani</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04186v1</id>
    <updated>2016-04-28T08:37:43Z</updated>
    <published>2016-04-28T08:37:43Z</published>
    <title>Large-scale Analysis of Chess Games with Chess Engines: A Preliminary
  Report</title>
    <summary>  The strength of chess engines together with the availability of numerous
chess games have attracted the attention of chess players, data scientists, and
researchers during the last decades. State-of-the-art engines now provide an
authoritative judgement that can be used in many applications like cheating
detection, intrinsic ratings computation, skill assessment, or the study of
human decision-making. A key issue for the research community is to gather a
large dataset of chess games together with the judgement of chess engines.
Unfortunately the analysis of each move takes lots of times. In this paper, we
report our effort to analyse almost 5 millions chess games with a computing
grid. During summer 2015, we processed 270 millions unique played positions
using the Stockfish engine with a quite high depth (20). We populated a
database of 1+ tera-octets of chess evaluations, representing an estimated time
of 50 years of computation on a single machine. Our effort is a first step
towards the replication of research results, the supply of open data and
procedures for exploring new directions, and the investigation of software
engineering/scalability issues when computing billions of moves.
</summary>
    <author>
      <name>Mathieu Acher</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DiverSe</arxiv:affiliation>
    </author>
    <author>
      <name>François Esnault</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DiverSe</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1607.04186v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04186v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08340v1</id>
    <updated>2016-04-28T08:17:56Z</updated>
    <published>2016-04-28T08:17:56Z</published>
    <title>Semantic Reasoning for Context-aware Internet of Things Applications</title>
    <summary>  Advances in ICT are bringing into reality the vision of a large number of
uniquely identifiable, interconnected objects and things that gather
information from diverse physical environments and deliver the information to a
variety of innovative applications and services. These sensing objects and
things form the Internet of Things (IoT) that can improve energy and cost
efficiency and automation in many different industry fields such as
transportation and logistics, health care and manufacturing, and facilitate our
everyday lives as well. IoT applications rely on real-time context data and
allow sending information for driving the behaviors of users in intelligent
environments.
</summary>
    <author>
      <name>Altti Ilari Maarala</name>
    </author>
    <author>
      <name>Xiang Su</name>
    </author>
    <author>
      <name>Jukka Riekki</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08270v1</id>
    <updated>2016-04-27T23:40:11Z</updated>
    <published>2016-04-27T23:40:11Z</published>
    <title>Quantum cognition beyond Hilbert space II: Applications</title>
    <summary>  The research on human cognition has recently benefited from the use of the
mathematical formalism of quantum theory in Hilbert space. However, cognitive
situations exist which indicate that the Hilbert space structure, and the
associated Born rule, would be insufficient to provide a satisfactory modeling
of the collected data, so that one needs to go beyond Hilbert space. In Part I
of this paper we follow this direction and present a general tension-reduction
(GTR) model, in the ambit of an operational and realistic framework for human
cognition. In this Part II we apply this non-Hilbertian quantum-like model to
faithfully reproduce the probabilities of the 'Clinton/Gore' and 'Rose/Jackson'
experiments on question order effects. We also explain why the GTR-model is
needed if one wants to deal, in a fully consistent way, with response
replicability and unpacking effects.
</summary>
    <author>
      <name>Diederik Aerts</name>
    </author>
    <author>
      <name>Lyneth Beltran</name>
    </author>
    <author>
      <name>Massimiliano Sassoli de Bianchi</name>
    </author>
    <author>
      <name>Sandro Sozzo</name>
    </author>
    <author>
      <name>Tomas Veloz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08268v1</id>
    <updated>2016-04-27T23:30:29Z</updated>
    <published>2016-04-27T23:30:29Z</published>
    <title>Quantum Cognition Beyond Hilbert Space I: Fundamentals</title>
    <summary>  The formalism of quantum theory in Hilbert space has been applied with
success to the modeling and explanation of several cognitive phenomena, whereas
traditional cognitive approaches were problematical. However, this 'quantum
cognition paradigm' was recently challenged by its proven impossibility to
simultaneously model 'question order effects' and 'response replicability'. In
Part I of this paper we describe sequential dichotomic measurements within an
operational and realistic framework for human cognition elaborated by
ourselves, and represent them in a quantum-like 'extended Bloch representation'
where the Born rule of quantum probability does not necessarily hold. In Part
II we apply this mathematical framework to successfully model question order
effects, response replicability and unpacking effects, thus opening the way
toward quantum cognition beyond Hilbert space.
</summary>
    <author>
      <name>Diederik Aerts</name>
    </author>
    <author>
      <name>Lyneth Beltran</name>
    </author>
    <author>
      <name>Massimiliano Sassoli de Bianchi</name>
    </author>
    <author>
      <name>Sandro Sozzo</name>
    </author>
    <author>
      <name>Tomas Veloz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08229v1</id>
    <updated>2016-04-27T20:29:01Z</updated>
    <published>2016-04-27T20:29:01Z</published>
    <title>Propositional Abduction with Implicit Hitting Sets</title>
    <summary>  Logic-based abduction finds important applications in artificial intelligence
and related areas. One application example is in finding explanations for
observed phenomena. Propositional abduction is a restriction of abduction to
the propositional domain, and complexity-wise is in the second level of the
polynomial hierarchy. Recent work has shown that exploiting implicit hitting
sets and propositional satisfiability (SAT) solvers provides an efficient
approach for propositional abduction. This paper investigates this earlier work
and proposes a number of algorithmic improvements. These improvements are shown
to yield exponential reductions in the number of SAT solver calls. More
importantly, the experimental results show significant performance improvements
compared to the the best approaches for propositional abduction.
</summary>
    <author>
      <name>Alexey Ignatiev</name>
    </author>
    <author>
      <name>Antonio Morgado</name>
    </author>
    <author>
      <name>Joao Marques-Silva</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08153v2</id>
    <updated>2016-06-08T16:05:31Z</updated>
    <published>2016-04-27T17:48:39Z</published>
    <title>Classifying Options for Deep Reinforcement Learning</title>
    <summary>  In this paper we combine one method for hierarchical reinforcement learning -
the options framework - with deep Q-networks (DQNs) through the use of
different "option heads" on the policy network, and a supervisory network for
choosing between the different options. We utilise our setup to investigate the
effects of architectural constraints in subtasks with positive and negative
transfer, across a range of network capacities. We empirically show that our
augmented DQN has lower sample complexity when simultaneously learning subtasks
with negative transfer, without degrading performance when learning subtasks
with positive transfer.
</summary>
    <author>
      <name>Kai Arulkumaran</name>
    </author>
    <author>
      <name>Nat Dilokthanakul</name>
    </author>
    <author>
      <name>Murray Shanahan</name>
    </author>
    <author>
      <name>Anil Anthony Bharath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2016 Workshop on Deep Reinforcement Learning: Frontiers and
  Challenges</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08153v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08153v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08055v1</id>
    <updated>2016-04-27T13:14:44Z</updated>
    <published>2016-04-27T13:14:44Z</published>
    <title>Selecting the Selection</title>
    <summary>  Modern saturation-based Automated Theorem Provers typically implement the
superposition calculus for reasoning about first-order logic with or without
equality. Practical implementations of this calculus use a variety of literal
selections and term orderings to tame the growth of the search space and help
steer proof search. This paper introduces the notion of lookahead selection
that estimates (looks ahead) the effect on the search space of selecting a
literal. There is also a case made for the use of incomplete selection
functions that attempt to restrict the search space instead of satisfying some
completeness criteria. Experimental evaluation in the \Vampire\ theorem prover
shows that both lookahead selection and incomplete selection significantly
contribute to solving hard problems unsolvable by other methods.
</summary>
    <author>
      <name>Giles Reger</name>
    </author>
    <author>
      <name>Martin Suda</name>
    </author>
    <author>
      <name>Andrei Voronkov</name>
    </author>
    <author>
      <name>Krystof Hoder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAR 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07990v1</id>
    <updated>2016-04-27T09:28:27Z</updated>
    <published>2016-04-27T09:28:27Z</published>
    <title>Probabilistic Graphical Models on Multi-Core CPUs using Java 8</title>
    <summary>  In this paper, we discuss software design issues related to the development
of parallel computational intelligence algorithms on multi-core CPUs, using the
new Java 8 functional programming features. In particular, we focus on
probabilistic graphical models (PGMs) and present the parallelisation of a
collection of algorithms that deal with inference and learning of PGMs from
data. Namely, maximum likelihood estimation, importance sampling, and greedy
search for solving combinatorial optimisation problems. Through these concrete
examples, we tackle the problem of defining efficient data structures for PGMs
and parallel processing of same-size batches of data sets using Java 8
features. We also provide straightforward techniques to code parallel
algorithms that seamlessly exploit multi-core processors. The experimental
analysis, carried out using our open source AMIDST (Analysis of MassIve Data
STreams) Java toolbox, shows the merits of the proposed solutions.
</summary>
    <author>
      <name>Andres R. Masegosa</name>
    </author>
    <author>
      <name>Ana M. Martinez</name>
    </author>
    <author>
      <name>Hanen Borchani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MCI.2016.2532267</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MCI.2016.2532267" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print version of the paper presented in the special issue on
  Computational Intelligence Software at IEEE Computational Intelligence
  Magazine journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Computational Intelligence Magazine, 11(2), 41-54. 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.07990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07981v2</id>
    <updated>2016-05-29T12:53:08Z</updated>
    <published>2016-04-27T09:00:17Z</published>
    <title>The Power of Arc Consistency for CSPs Defined by Partially-Ordered
  Forbidden Patterns</title>
    <summary>  Characterising tractable fragments of the constraint satisfaction problem
(CSP) is an important challenge in theoretical computer science and artificial
intelligence. Forbidding patterns (generic sub-instances) provides a means of
defining CSP fragments which are neither exclusively language-based nor
exclusively structure-based. It is known that the class of binary CSP instances
in which the broken-triangle pattern (BTP) does not occur, a class which
includes all tree-structured instances, are decided by arc consistency (AC), a
ubiquitous reduction operation in constraint solvers. We provide a
characterisation of simple partially-ordered forbidden patterns which have this
AC-solvability property. It turns out that BTP is just one of five such
AC-solvable patterns. The four other patterns allow us to exhibit new tractable
classes.
</summary>
    <author>
      <name>Martin C. Cooper</name>
    </author>
    <author>
      <name>Stanislav Živný</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A full version of a LICS'16 paper. Expanded proofs compared to v1</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07981v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07981v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07928v2</id>
    <updated>2016-05-22T00:00:23Z</updated>
    <published>2016-04-27T04:18:32Z</published>
    <title>Distributed Flexible Nonlinear Tensor Factorization</title>
    <summary>  Tensor factorization is a powerful tool to analyse multi-way data. Compared
with traditional multi-linear methods, nonlinear tensor factorization models
are capable of capturing more complex relationships in the data. However, they
are computationally expensive and may suffer severe learning bias in case of
extreme data sparsity. To overcome these limitations, in this paper we propose
a distributed, flexible nonlinear tensor factorization model. Our model can
effectively avoid the expensive computations and structural restrictions of the
Kronecker-product in existing TGP formulations, allowing an arbitrary subset of
tensorial entries to be selected to contribute to the training. At the same
time, we derive a tractable and tight variational evidence lower bound (ELBO)
that enables highly decoupled, parallel computations and high-quality
inference. Based on the new bound, we develop a distributed inference algorithm
in the MapReduce framework, which is key-value-free and can fully exploit the
memory cache mechanism in fast MapReduce systems such as SPARK. Experimental
results fully demonstrate the advantages of our method over several
state-of-the-art approaches, in terms of both predictive performance and
computational efficiency. Moreover, our approach shows a promising potential in
the application of Click-Through-Rate (CTR) prediction for online advertising.
</summary>
    <author>
      <name>Shandian Zhe</name>
    </author>
    <author>
      <name>Kai Zhang</name>
    </author>
    <author>
      <name>Pengyuan Wang</name>
    </author>
    <author>
      <name>Kuang-chih Lee</name>
    </author>
    <author>
      <name>Zenglin Xu</name>
    </author>
    <author>
      <name>Yuan Qi</name>
    </author>
    <author>
      <name>Zoubin Ghahramani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Gaussian process, tensor factorization, multidimensional arrays,
  large scale, spark, map-reduce</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07928v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07928v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07906v1</id>
    <updated>2016-04-27T02:21:28Z</updated>
    <published>2016-04-27T02:21:28Z</published>
    <title>Procedural Generation of Angry Birds Levels using Building Constructive
  Grammar with Chinese-Style and/or Japanese-Style Models</title>
    <summary>  This paper presents a procedural generation method that creates visually
attractive levels for the Angry Birds game. Besides being an immensely popular
mobile game, Angry Birds has recently become a test bed for various artificial
intelligence technologies. We propose a new approach for procedurally
generating Angry Birds levels using Chinese style and Japanese style building
structures. A conducted experiment confirms the effectiveness of our approach
with statistical significance.
</summary>
    <author>
      <name>YuXuan Jiang</name>
    </author>
    <author>
      <name>Misaki Kaidan</name>
    </author>
    <author>
      <name>Chun Yin Chu</name>
    </author>
    <author>
      <name>Tomohiro Harada</name>
    </author>
    <author>
      <name>Ruck Thawonmas</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of ASIAGRAPH 2016, Toyama, Japan, pp. 53-54, Mar. 5-6, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.07906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07806v1</id>
    <updated>2016-04-26T19:24:52Z</updated>
    <published>2016-04-26T19:24:52Z</published>
    <title>Using Indirect Encoding of Multiple Brains to Produce Multimodal
  Behavior</title>
    <summary>  An important challenge in neuroevolution is to evolve complex neural networks
with multiple modes of behavior. Indirect encodings can potentially answer this
challenge. Yet in practice, indirect encodings do not yield effective
multimodal controllers. Thus, this paper introduces novel multimodal extensions
to HyperNEAT, a popular indirect encoding. A previous multimodal HyperNEAT
approach called situational policy geometry assumes that multiple brains
benefit from being embedded within an explicit geometric space. However,
experiments here illustrate that this assumption unnecessarily constrains
evolution, resulting in lower performance. Specifically, this paper introduces
HyperNEAT extensions for evolving many brains without assuming geometric
relationships between them. The resulting Multi-Brain HyperNEAT can exploit
human-specified task divisions to decide when each brain controls the agent, or
can automatically discover when brains should be used, by means of preference
neurons. A further extension called module mutation allows evolution to
discover the number of brains, enabling multimodal behavior with even less
expert knowledge. Experiments in several multimodal domains highlight that
multi-brain approaches are more effective than HyperNEAT without multimodal
extensions, and show that brains without a geometric relation to each other
outperform situational policy geometry. The conclusion is that Multi-Brain
HyperNEAT provides several promising techniques for evolving complex multimodal
behavior.
</summary>
    <author>
      <name>Jacob Schrum</name>
    </author>
    <author>
      <name>Joel Lehman</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07706v3</id>
    <updated>2016-06-07T08:06:23Z</updated>
    <published>2016-04-26T14:59:43Z</published>
    <title>Distributed Clustering of Linear Bandits in Peer to Peer Networks</title>
    <summary>  We provide two distributed confidence ball algorithms for solving linear
bandit problems in peer to peer networks with limited communication
capabilities. For the first, we assume that all the peers are solving the same
linear bandit problem, and prove that our algorithm achieves the optimal
asymptotic regret rate of any centralised algorithm that can instantly
communicate information between the peers. For the second, we assume that there
are clusters of peers solving the same bandit problem within each cluster, and
we prove that our algorithm discovers these clusters, while achieving the
optimal asymptotic regret rate within each one. Through experiments on several
real-world datasets, we demonstrate the performance of proposed algorithms
compared to the state-of-the-art.
</summary>
    <author>
      <name>Nathan Korda</name>
    </author>
    <author>
      <name>Balazs Szorenyi</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 33rd ICML, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07706v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07706v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07704v1</id>
    <updated>2016-04-26T14:57:56Z</updated>
    <published>2016-04-26T14:57:56Z</published>
    <title>Tournament selection in zeroth-level classifier systems based on average
  reward reinforcement learning</title>
    <summary>  As a genetics-based machine learning technique, zeroth-level classifier
system (ZCS) is based on a discounted reward reinforcement learning algorithm,
bucket-brigade algorithm, which optimizes the discounted total reward received
by an agent but is not suitable for all multi-step problems, especially
large-size ones. There are some undiscounted reinforcement learning methods
available, such as R-learning, which optimize the average reward per time step.
In this paper, R-learning is used as the reinforcement learning employed by
ZCS, to replace its discounted reward reinforcement learning approach, and
tournament selection is used to replace roulette wheel selection in ZCS. The
modification results in classifier systems that can support long action chains,
and thus is able to solve large multi-step problems.
</summary>
    <author>
      <name>Zhaoxiang Zang</name>
    </author>
    <author>
      <name>Zhao Li</name>
    </author>
    <author>
      <name>Junying Wang</name>
    </author>
    <author>
      <name>Zhiping Dan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07625v1</id>
    <updated>2016-04-26T11:31:02Z</updated>
    <published>2016-04-26T11:31:02Z</published>
    <title>Mutual Transformation of Information and Knowledge</title>
    <summary>  Information and knowledge are transformable into each other. Information
transformation into knowledge by the example of rule generation from OWL (Web
Ontology Language) ontology has been shown during the development of the SWES
(Semantic Web Expert System). The SWES is expected as an expert system for
searching OWL ontologies from the Web, generating rules from the found
ontologies and supplementing the SWES knowledge base with these rules. The
purpose of this paper is to show knowledge transformation into information by
the example of ontology generation from rules.
</summary>
    <author>
      <name>Olegs Verhodubs</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07513v1</id>
    <updated>2016-04-26T04:31:31Z</updated>
    <published>2016-04-26T04:31:31Z</published>
    <title>Semantic Change Detection with Hypermaps</title>
    <summary>  Change detection is the study of detecting changes between two different
images of a scene taken at different times. This paper proposes the concept of
semantic change detection, which involves intuitively inserting semantic
meaning into detected change areas. The problem to be solved consists of two
parts, semantic segmentation and change detection. In order to solve this
problem and obtain a high-level of performance, we propose an improvement to
the hypercolumns representation, hereafter known as hypermaps, which
effectively uses convolutional maps obtained from convolutional neural networks
(CNNs). We also employ multi-scale feature representation captured by different
image patches. We applied our method to the TSUNAMI Panoramic Change Detection
dataset, and re-annotated the changed areas of the dataset via semantic
classes. The results show that our multi-scale hypermaps provided outstanding
performance on the re-annotated TSUNAMI dataset.
</summary>
    <author>
      <name>Hirokatsu Kataoka</name>
    </author>
    <author>
      <name>Soma Shirakabe</name>
    </author>
    <author>
      <name>Yudai Miyashita</name>
    </author>
    <author>
      <name>Akio Nakamura</name>
    </author>
    <author>
      <name>Kenji Iwata</name>
    </author>
    <author>
      <name>Yutaka Satoh</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07429v1</id>
    <updated>2016-04-25T20:14:35Z</updated>
    <published>2016-04-25T20:14:35Z</published>
    <title>Balancing Appearance and Context in Sketch Interpretation</title>
    <summary>  We describe a sketch interpretation system that detects and classifies clock
numerals created by subjects taking the Clock Drawing Test, a clinical tool
widely used to screen for cognitive impairments (e.g., dementia). We describe
how it balances appearance and context, and document its performance on some
2,000 drawings (about 24K clock numerals) produced by a wide spectrum of
patients. We calibrate the utility of different forms of context, describing
experiments with Conditional Random Fields trained and tested using a variety
of features. We identify context that contributes to interpreting otherwise
ambiguous or incomprehensible strokes. We describe ST-slices, a novel
representation that enables "unpeeling" the layers of ink that result when
people overwrite, which often produces ink impossible to analyze if only the
final drawing is examined. We characterize when ST-slices work, calibrate their
impact on performance, and consider their breadth of applicability.
</summary>
    <author>
      <name>Yale Song</name>
    </author>
    <author>
      <name>Randall Davis</name>
    </author>
    <author>
      <name>Kaichen Ma</name>
    </author>
    <author>
      <name>Dana L. Penny</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07407v1</id>
    <updated>2016-04-25T20:00:02Z</updated>
    <published>2016-04-25T20:00:02Z</published>
    <title>Conversational Markers of Constructive Discussions</title>
    <summary>  Group discussions are essential for organizing every aspect of modern life,
from faculty meetings to senate debates, from grant review panels to papal
conclaves. While costly in terms of time and organization effort, group
discussions are commonly seen as a way of reaching better decisions compared to
solutions that do not require coordination between the individuals (e.g.
voting)---through discussion, the sum becomes greater than the parts. However,
this assumption is not irrefutable: anecdotal evidence of wasteful discussions
abounds, and in our own experiments we find that over 30% of discussions are
unproductive.
  We propose a framework for analyzing conversational dynamics in order to
determine whether a given task-oriented discussion is worth having or not. We
exploit conversational patterns reflecting the flow of ideas and the balance
between the participants, as well as their linguistic choices. We apply this
framework to conversations naturally occurring in an online collaborative world
exploration game developed and deployed to support this research. Using this
setting, we show that linguistic cues and conversational patterns extracted
from the first 20 seconds of a team discussion are predictive of whether it
will be a wasteful or a productive one.
</summary>
    <author>
      <name>Vlad Niculae</name>
    </author>
    <author>
      <name>Cristian Danescu-Niculescu-Mizil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at NAACL-HLT 2016. 11pp, 5 fig. Data and other info
  available at http://vene.ro/constructive/</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07379v1</id>
    <updated>2016-04-25T19:42:46Z</updated>
    <published>2016-04-25T19:42:46Z</published>
    <title>Context Encoders: Feature Learning by Inpainting</title>
    <summary>  We present an unsupervised visual feature learning algorithm driven by
context-based pixel prediction. By analogy with auto-encoders, we propose
Context Encoders -- a convolutional neural network trained to generate the
contents of an arbitrary image region conditioned on its surroundings. In order
to succeed at this task, context encoders need to both understand the content
of the entire image, as well as produce a plausible hypothesis for the missing
part(s). When training context encoders, we have experimented with both a
standard pixel-wise reconstruction loss, as well as a reconstruction plus an
adversarial loss. The latter produces much sharper results because it can
better handle multiple modes in the output. We found that a context encoder
learns a representation that captures not just appearance but also the
semantics of visual structures. We quantitatively demonstrate the effectiveness
of our learned features for CNN pre-training on classification, detection, and
segmentation tasks. Furthermore, context encoders can be used for semantic
inpainting tasks, either stand-alone or as initialization for non-parametric
methods.
</summary>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <author>
      <name>Philipp Krahenbuhl</name>
    </author>
    <author>
      <name>Jeff Donahue</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Alexei A. Efros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07322v2</id>
    <updated>2016-04-27T06:16:40Z</updated>
    <published>2016-04-25T16:34:17Z</published>
    <title>Predictive No-Reference Assessment of Video Quality</title>
    <summary>  Among the various means to evaluate the quality of video streams,
No-Reference (NR) methods have low computation and may be executed on thin
clients. Thus, NR algorithms would be perfect candidates in cases of real-time
quality assessment, automated quality control and, particularly, in adaptive
mobile streaming. Yet, existing NR approaches are often inaccurate, in
comparison to Full-Reference (FR) algorithms, especially under lossy network
conditions. In this work, we present an NR method that combines machine
learning with simple NR metrics to achieve a quality index comparably as
accurate as the Video Quality Metric (VQM) Full-Reference algorithm. Our method
is tested in an extensive dataset (960 videos), under lossy network conditions
and considering nine different machine learning algorithms. Overall, we achieve
an over 97% correlation with VQM, while allowing real-time assessment of video
quality of experience in realistic streaming scenarios.
</summary>
    <author>
      <name>Maria Torres Vega</name>
    </author>
    <author>
      <name>Decebal Constantin Mocanu</name>
    </author>
    <author>
      <name>Antonio Liotta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures, IEEE Selected Topics on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07322v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07322v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07312v1</id>
    <updated>2016-04-25T15:35:38Z</updated>
    <published>2016-04-25T15:35:38Z</published>
    <title>Endgame Analysis of Dou Shou Qi</title>
    <summary>  Dou Shou Qi is a game in which two players control a number of pieces, each
of them aiming to move one of their pieces onto a given square. We implemented
an engine for analyzing the game. Moreover, we created a series of endgame
tablebases containing all configurations with up to four pieces. These
tablebases are the first steps towards theoretically solving the game. Finally,
we constructed decision trees based on the endgame tablebases. In this note we
report on some interesting patterns.
</summary>
    <author>
      <name>Jan N. van Rijn</name>
    </author>
    <author>
      <name>Jonathan K. Vis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, ICGA Journal, Vol. 37, pp. 120-124, 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICGA Journal, Vol. 37, pp. 120-124, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.07312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07255v1</id>
    <updated>2016-04-25T13:45:50Z</updated>
    <published>2016-04-25T13:45:50Z</published>
    <title>A Deep Hierarchical Approach to Lifelong Learning in Minecraft</title>
    <summary>  The ability to reuse or transfer knowledge from one task to another in
lifelong learning problems, such as Minecraft, is one of the major challenges
faced in AI. Reusing knowledge across tasks is crucial to solving tasks
efficiently with lower sample complexity. We provide a Reinforcement Learning
agent with the ability to transfer knowledge by learning reusable skills, a
type of temporally extended action (also know as Options (Sutton et. al.
1999)). The agent learns reusable skills using Deep Q Networks (Mnih et. al.
2015) to solve tasks in Minecraft, a popular video game which is an unsolved
and high-dimensional lifelong learning problem. These reusable skills, which we
refer to as Deep Skill Networks (DSNs), are then incorporated into our novel
Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture. The
H-DRLN is a hierarchical version of Deep QNetworks and learns to efficiently
solve tasks by reusing knowledge from previously learned DSNs. The H-DRLN
exhibits superior performance and lower learning sample complexity (by taking
advantage of temporal extension) compared to the regular Deep Q Network (Mnih
et. al. 2015) in subdomains of Minecraft. We also show the potential to
transfer knowledge between related Minecraft tasks without any additional
learning.
</summary>
    <author>
      <name>Chen Tessler</name>
    </author>
    <author>
      <name>Shahar Givony</name>
    </author>
    <author>
      <name>Tom Zahavy</name>
    </author>
    <author>
      <name>Daniel J. Mankowitz</name>
    </author>
    <author>
      <name>Shie Mannor</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07183v2</id>
    <updated>2016-04-26T16:10:38Z</updated>
    <published>2016-04-25T09:44:02Z</published>
    <title>AGM-Style Revision of Beliefs and Intentions from a Database Perspective
  (Preliminary Version)</title>
    <summary>  We introduce a logic for temporal beliefs and intentions based on Shoham's
database perspective. We separate strong beliefs from weak beliefs. Strong
beliefs are independent from intentions, while weak beliefs are obtained by
adding intentions to strong beliefs and everything that follows from that. We
formalize coherence conditions on strong beliefs and intentions. We provide
AGM-style postulates for the revision of strong beliefs and intentions. We show
in a representation theorem that a revision operator satisfying our postulates
can be represented by a pre-order on interpretations of the beliefs, together
with a selection function for the intentions.
</summary>
    <author>
      <name>Marc van Zee</name>
    </author>
    <author>
      <name>Dragan Doder</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07183v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07183v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07178v1</id>
    <updated>2016-04-25T09:29:21Z</updated>
    <published>2016-04-25T09:29:21Z</published>
    <title>Weighted Spectral Cluster Ensemble</title>
    <summary>  Clustering explores meaningful patterns in the non-labeled data sets. Cluster
Ensemble Selection (CES) is a new approach, which can combine individual
clustering results for increasing the performance of the final results.
Although CES can achieve better final results in comparison with individual
clustering algorithms and cluster ensemble methods, its performance can be
dramatically affected by its consensus diversity metric and thresholding
procedure. There are two problems in CES: 1) most of the diversity metrics is
based on heuristic Shannon's entropy and 2) estimating threshold values are
really hard in practice. The main goal of this paper is proposing a robust
approach for solving the above mentioned problems. Accordingly, this paper
develops a novel framework for clustering problems, which is called Weighted
Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community
detection arena and graph based clustering. Under this framework, a new version
of spectral clustering, which is called Two Kernels Spectral Clustering, is
used for generating graphs based individual clustering results. Further, by
using modularity, which is a famous metric in the community detection, on the
transformed graph representation of individual clustering results, our approach
provides an effective diversity estimation for individual clustering results.
Moreover, this paper introduces a new approach for combining the evaluated
individual clustering results without the procedure of thresholding.
Experimental study on varied data sets demonstrates that the prosed approach
achieves superior performance to state-of-the-art methods.
</summary>
    <author>
      <name>Muhammad Yousefnezhad</name>
    </author>
    <author>
      <name>Daoqiang Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICDM.2015.145</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICDM.2015.145" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Data Mining (ICDM), 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07176v1</id>
    <updated>2016-04-25T09:17:18Z</updated>
    <published>2016-04-25T09:17:18Z</published>
    <title>Protein Secondary Structure Prediction Using Cascaded Convolutional and
  Recurrent Neural Networks</title>
    <summary>  Protein secondary structure prediction is an important problem in
bioinformatics. Inspired by the recent successes of deep neural networks, in
this paper, we propose an end-to-end deep network that predicts protein
secondary structures from integrated local and global contextual features. Our
deep architecture leverages convolutional neural networks with different kernel
sizes to extract multiscale local contextual features. In addition, considering
long-range dependencies existing in amino acid sequences, we set up a
bidirectional neural network consisting of gated recurrent unit to capture
global contextual features. Furthermore, multi-task learning is utilized to
predict secondary structure labels and amino-acid solvent accessibility
simultaneously. Our proposed deep network demonstrates its effectiveness by
achieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the public
benchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11.
Our model and results are publicly available.
</summary>
    <author>
      <name>Zhen Li</name>
    </author>
    <author>
      <name>Yizhou Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, Accepted by International Joint Conferences on
  Artificial Intelligence (IJCAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07102v1</id>
    <updated>2016-04-25T01:01:51Z</updated>
    <published>2016-04-25T01:01:51Z</published>
    <title>Makeup like a superstar: Deep Localized Makeup Transfer Network</title>
    <summary>  In this paper, we propose a novel Deep Localized Makeup Transfer Network to
automatically recommend the most suitable makeup for a female and synthesis the
makeup on her face. Given a before-makeup face, her most suitable makeup is
determined automatically. Then, both the beforemakeup and the reference faces
are fed into the proposed Deep Transfer Network to generate the after-makeup
face. Our end-to-end makeup transfer network have several nice properties
including: (1) with complete functions: including foundation, lip gloss, and
eye shadow transfer; (2) cosmetic specific: different cosmetics are transferred
in different manners; (3) localized: different cosmetics are applied on
different facial regions; (4) producing naturally looking results without
obvious artifacts; (5) controllable makeup lightness: various results from
light makeup to heavy makeup can be generated. Qualitative and quantitative
experiments show that our network performs much better than the methods of [Guo
and Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a].
</summary>
    <author>
      <name>Si Liu</name>
    </author>
    <author>
      <name>Xinyu Ou</name>
    </author>
    <author>
      <name>Ruihe Qian</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Xiaochun Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7pages, 11 figures, to appear in IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07097v2</id>
    <updated>2016-04-26T02:26:14Z</updated>
    <published>2016-04-24T23:56:37Z</published>
    <title>Neurohex: A Deep Q-learning Hex Agent</title>
    <summary>  DeepMind's recent spectacular success in using deep convolutional neural nets
and machine learning to build superhuman level agents --- e.g. for Atari games
via deep Q-learning and for the game of Go via Reinforcement Learning ---
raises many questions, including to what extent these methods will succeed in
other domains. In this paper we consider DQL for the game of Hex: after
supervised initialization, we use selfplay to train NeuroHex, an 11-layer CNN
that plays Hex on the 13x13 board. Hex is the classic two-player alternate-turn
stone placement game played on a rhombus of hexagonal cells in which the winner
is whomever connects their two opposing sides. Despite the large action and
state space, our system trains a Q-network capable of strong play with no
search. After two weeks of Q-learning, NeuroHex achieves win-rates of 20.4% as
first player and 2.1% as second player against a 1-second/move version of
MoHex, the current ICGA Olympiad Hex champion. Our data suggests further
improvement might be possible with more training time.
</summary>
    <author>
      <name>Kenny Young</name>
    </author>
    <author>
      <name>Ryan Hayward</name>
    </author>
    <author>
      <name>Gautham Vasan</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07097v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07097v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07095v1</id>
    <updated>2016-04-24T23:51:18Z</updated>
    <published>2016-04-24T23:51:18Z</published>
    <title>Deep Learning for Reward Design to Improve Monte Carlo Tree Search in
  ATARI Games</title>
    <summary>  Monte Carlo Tree Search (MCTS) methods have proven powerful in planning for
sequential decision-making problems such as Go and video games, but their
performance can be poor when the planning depth and sampling trajectories are
limited or when the rewards are sparse. We present an adaptation of PGRD
(policy-gradient for reward-design) for learning a reward-bonus function to
improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which
the space of reward-bonus functions was limited to linear functions of
hand-coded state-action-features, we use PGRD with a multi-layer convolutional
neural network to automatically learn features from raw perception as well as
to adapt the non-linear reward-bonus function parameters. We also adopt a
variance-reducing gradient method to improve PGRD's performance. The new method
improves UCT's performance on multiple ATARI games compared to UCT without the
reward bonus. Combining PGRD and Deep Learning in this way should make adapting
rewards for MCTS algorithms far more widely and practically applicable than
before.
</summary>
    <author>
      <name>Xiaoxiao Guo</name>
    </author>
    <author>
      <name>Satinder Singh</name>
    </author>
    <author>
      <name>Richard Lewis</name>
    </author>
    <author>
      <name>Honglak Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In 25th International Joint Conference on Artificial Intelligence
  (IJCAI), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07093v1</id>
    <updated>2016-04-24T23:36:36Z</updated>
    <published>2016-04-24T23:36:36Z</published>
    <title>Semi-supervised Vocabulary-informed Learning</title>
    <summary>  Despite significant progress in object categorization, in recent years, a
number of important challenges remain, mainly, ability to learn from limited
labeled data and ability to recognize object classes within large, potentially
open, set of labels. Zero-shot learning is one way of addressing these
challenges, but it has only been shown to work with limited sized class
vocabularies and typically requires separation between supervised and
unsupervised classes, allowing former to inform the latter but not vice versa.
We propose the notion of semi-supervised vocabulary-informed learning to
alleviate the above mentioned challenges and address problems of supervised,
zero-shot and open set recognition using a unified framework. Specifically, we
propose a maximum margin framework for semantic manifold-based recognition that
incorporates distance constraints from (both supervised and unsupervised)
vocabulary atoms, ensuring that labeled samples are projected closest to their
correct prototypes, in the embedding space, than to others. We show that
resulting model shows improvements in supervised, zero-shot, and large open set
recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.
</summary>
    <author>
      <name>Yanwei Fu</name>
    </author>
    <author>
      <name>Leonid Sigal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, Accepted by CVPR 2016 as an oral presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07063v2</id>
    <updated>2016-06-20T13:43:50Z</updated>
    <published>2016-04-24T17:56:44Z</published>
    <title>The Dichotomy for Conservative Constraint Satisfaction is Polynomially
  Decidable</title>
    <summary>  Given a fixed constraint language $\Gamma$, the conservative CSP over
$\Gamma$ (denoted by c-CSP($\Gamma$)) is a variant of CSP($\Gamma$) where the
domain of each variable can be restricted arbitrarily. A dichotomy is known for
conservative CSP: for every fixed language $\Gamma$, c-CSP($\Gamma$) is either
in P or NP-complete. However, the characterization of conservatively tractable
languages is of algebraic nature and the naive recognition algorithm is
super-exponential in the domain size. The main contribution of this paper is a
polynomial-time algorithm that, given a constraint language $\Gamma$ as input,
decides if c-CSP($\Gamma$) is tractable. In addition, if $\Gamma$ is proven
tractable the algorithm also outputs its coloured graph, which contains
valuable information on the structure of $\Gamma$.
</summary>
    <author>
      <name>Clément Carbonnel</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07063v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07063v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06976v1</id>
    <updated>2016-04-24T02:39:16Z</updated>
    <published>2016-04-24T02:39:16Z</published>
    <title>Extracted Social Network Mining</title>
    <summary>  In this paper we study the relationship between the resources of social
networks by exploring the Web as big data based on a simple search engine. We
have used set theory by utilizing the occurrence and co-occurrence for defining
the singleton or doubleton spaces of event in a search engine model, and then
provided them as representation of social actors and their relationship in
clusters. Thus, there are behaviors of social actors and their relation based
on Web.
</summary>
    <author>
      <name>Mahyuddin K. M. Nasution</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages. Proceeding of International Conference on Information
  Technology and Engineering Application (5-th ICIBA), 86-91, February 19-20,
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06970v1</id>
    <updated>2016-04-24T00:55:27Z</updated>
    <published>2016-04-24T00:55:27Z</published>
    <title>Bayesian Inference of Recursive Sequences of Group Activities from
  Tracks</title>
    <summary>  We present a probabilistic generative model for inferring a description of
coordinated, recursively structured group activities at multiple levels of
temporal granularity based on observations of individuals' trajectories. The
model accommodates: (1) hierarchically structured groups, (2) activities that
are temporally and compositionally recursive, (3) component roles assigning
different subactivity dynamics to subgroups of participants, and (4) a
nonparametric Gaussian Process model of trajectories. We present an MCMC
sampling framework for performing joint inference over recursive activity
descriptions and assignment of trajectories to groups, integrating out
continuous parameters. We demonstrate the model's expressive power in several
simulated and complex real-world scenarios from the VIRAT and UCLA Aerial Event
video data sets.
</summary>
    <author>
      <name>Ernesto Brau</name>
    </author>
    <author>
      <name>Colin Dawson</name>
    </author>
    <author>
      <name>Alfredo Carrillo</name>
    </author>
    <author>
      <name>David Sidi</name>
    </author>
    <author>
      <name>Clayton T. Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, in Proceedings of the 30th AAAI Conference on
  Artificial Intelligence (AAAI'16), Phoenix, AZ, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06963v1</id>
    <updated>2016-04-23T23:01:29Z</updated>
    <published>2016-04-23T23:01:29Z</published>
    <title>Limits to Verification and Validation of Agentic Behavior</title>
    <summary>  Verification and validation of agentic behavior have been suggested as
important research priorities in efforts to reduce risks associated with the
creation of general artificial intelligence (Russell et al 2015). In this paper
we question the appropriateness of using language of certainty with respect to
efforts to manage that risk. We begin by establishing a very general formalism
to characterize agentic behavior and to describe standards of acceptable
behavior. We show that determination of whether an agent meets any particular
standard is not computable. We discuss the extent of the burden associated with
verification by manual proof and by automated behavioral governance. We show
that to ensure decidability of the behavioral standard itself, one must further
limit the capabilities of the agent. We then demonstrate that if our concerns
relate to outcomes in the physical world, attempts at validation are futile.
Finally, we show that layered architectures aimed at making these challenges
tractable mistakenly equate intentions with actions or outcomes, thereby
failing to provide any guarantees. We conclude with a discussion of why
language of certainty should be eradicated from the conversation about the
safety of general artificial intelligence.
</summary>
    <author>
      <name>David J. Jilk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; F.3.1; D.2.4; K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06954v1</id>
    <updated>2016-04-23T21:03:45Z</updated>
    <published>2016-04-23T21:03:45Z</published>
    <title>RHOG: A Refinement-Operator Library for Directed Labeled Graphs</title>
    <summary>  This document provides the foundations behind the functionality provided by
the $\rho$G library (https://github.com/santiontanon/RHOG), focusing on the
basic operations the library provides: subsumption, refinement of directed
labeled graphs, and distance/similarity assessment between directed labeled
graphs. $\rho$G development was initially supported by the National Science
Foundation, by the EAGER grant IIS-1551338.
</summary>
    <author>
      <name>Santiago Ontañón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Report of the theory behind the RHOG library developed under NSF
  EAGER grant IIS-1551338</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06849v1</id>
    <updated>2016-04-23T02:23:14Z</updated>
    <published>2016-04-23T02:23:14Z</published>
    <title>A Computational Model for Situated Task Learning with Interactive
  Instruction</title>
    <summary>  Learning novel tasks is a complex cognitive activity requiring the learner to
acquire diverse declarative and procedural knowledge. Prior ACT-R models of
acquiring task knowledge from instruction focused on learning procedural
knowledge from declarative instructions encoded in semantic memory. In this
paper, we identify the requirements for designing compu- tational models that
learn task knowledge from situated task- oriented interactions with an expert
and then describe and evaluate a model of learning from situated interactive
instruc- tion that is implemented in the Soar cognitive architecture.
</summary>
    <author>
      <name>Shiwali Mohan</name>
    </author>
    <author>
      <name>James Kirk</name>
    </author>
    <author>
      <name>John Laird</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Cognitive Modeling, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06790v1</id>
    <updated>2016-04-22T19:35:49Z</updated>
    <published>2016-04-22T19:35:49Z</published>
    <title>DisCSPs with Privacy Recast as Planning Problems for Utility-based
  Agents</title>
    <summary>  Privacy has traditionally been a major motivation for decentralized problem
solving. However, even though several metrics have been proposed to quantify
it, none of them is easily integrated with common solvers. Constraint
programming is a fundamental paradigm used to approach various families of
problems. We introduce Utilitarian Distributed Constraint Satisfaction Problems
(UDisCSP) where the utility of each state is estimated as the difference
between the the expected rewards for agreements on assignments for shared
variables, and the expected cost of privacy loss. Therefore, a traditional
DisCSP with privacy requirements is viewed as a planning problem. The actions
available to agents are: communication and local inference. Common
decentralized solvers are evaluated here from the point of view of their
interpretation as greedy planners. Further, we investigate some simple
extensions where these solvers start taking into account the utility function.
In these extensions we assume that the planning problem is further restricting
the set of communication actions to only the communication primitives present
in the corresponding solver protocols. The solvers obtained for the new type of
problems propose the action (communication/inference) to be performed in each
situation, defining thereby the policy.
</summary>
    <author>
      <name>Julien Savaux</name>
    </author>
    <author>
      <name>Julien Vion</name>
    </author>
    <author>
      <name>Sylvain Piechowiak</name>
    </author>
    <author>
      <name>René Mandiau</name>
    </author>
    <author>
      <name>Toshihiro Matsui</name>
    </author>
    <author>
      <name>Katsutoshi Hirayama</name>
    </author>
    <author>
      <name>Makoto Yokoo</name>
    </author>
    <author>
      <name>Shakre Elmane</name>
    </author>
    <author>
      <name>Marius Silaghi</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06787v1</id>
    <updated>2016-04-22T19:26:30Z</updated>
    <published>2016-04-22T19:26:30Z</published>
    <title>Utilitarian Distributed Constraint Optimization Problems</title>
    <summary>  Privacy has been a major motivation for distributed problem optimization.
However, even though several methods have been proposed to evaluate it, none of
them is widely used. The Distributed Constraint Optimization Problem (DCOP) is
a fundamental model used to approach various families of distributed problems.
As privacy loss does not occur when a solution is accepted, but when it is
proposed, privacy requirements cannot be interpreted as a criteria of the
objective function of the DCOP. Here we approach the problem by letting both
the optimized costs found in DCOPs and the privacy requirements guide the
agents' exploration of the search space. We introduce Utilitarian Distributed
Constraint Optimization Problem (UDCOP) where the costs and the privacy
requirements are used as parameters to a heuristic modifying the search
process. Common stochastic algorithms for decentralized constraint optimization
problems are evaluated here according to how well they preserve privacy.
Further, we propose some extensions where these solvers modify their search
process to take into account their privacy requirements, succeeding in
significantly reducing their privacy loss without significant degradation of
the solution quality.
</summary>
    <author>
      <name>Julien Savaux</name>
    </author>
    <author>
      <name>Julien Vion</name>
    </author>
    <author>
      <name>Sylvain Piechowiak</name>
    </author>
    <author>
      <name>René Mandiau</name>
    </author>
    <author>
      <name>Toshihiro Matsui</name>
    </author>
    <author>
      <name>Katsutoshi Hirayama</name>
    </author>
    <author>
      <name>Makoto Yokoo</name>
    </author>
    <author>
      <name>Shakre Elmane</name>
    </author>
    <author>
      <name>Marius Silaghi</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06778v3</id>
    <updated>2016-05-27T19:25:59Z</updated>
    <published>2016-04-22T18:57:24Z</published>
    <title>Benchmarking Deep Reinforcement Learning for Continuous Control</title>
    <summary>  Recently, researchers have made significant progress combining the advances
in deep learning for learning feature representations with reinforcement
learning. Some notable examples include training agents to play Atari games
based on raw pixel data and to acquire advanced manipulation skills using raw
sensory inputs. However, it has been difficult to quantify progress in the
domain of continuous control due to the lack of a commonly adopted benchmark.
In this work, we present a benchmark suite of continuous control tasks,
including classic tasks like cart-pole swing-up, tasks with very high state and
action dimensionality such as 3D humanoid locomotion, tasks with partial
observations, and tasks with hierarchical structure. We report novel findings
based on the systematic evaluation of a range of implemented reinforcement
learning algorithms. Both the benchmark and reference implementations are
released at https://github.com/rllab/rllab in order to facilitate experimental
reproducibility and to encourage adoption by other researchers.
</summary>
    <author>
      <name>Yan Duan</name>
    </author>
    <author>
      <name>Xi Chen</name>
    </author>
    <author>
      <name>Rein Houthooft</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, ICML 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06778v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06778v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06770v2</id>
    <updated>2016-07-25T18:22:01Z</updated>
    <published>2016-04-22T18:46:10Z</published>
    <title>A Hybrid Approach to Query Answering under Expressive Datalog+/-</title>
    <summary>  Datalog+/- is a family of ontology languages that combine good computational
properties with high expressive power. Datalog+/- languages are provably able
to capture the most relevant Semantic Web languages. In this paper we consider
the class of weakly-sticky (WS) Datalog+/- programs, which allow for certain
useful forms of joins in rule bodies as well as extending the well-known class
of weakly-acyclic TGDs. So far, only non-deterministic algorithms were known
for answering queries on WS Datalog+/- programs. We present novel deterministic
query answering algorithms under WS Datalog+/-. In particular, we propose: (1)
a bottom-up grounding algorithm based on a query-driven chase, and (2) a hybrid
approach based on transforming a WS program into a so-called sticky one, for
which query rewriting techniques are known. We discuss how our algorithms can
be optimized and effectively applied for query answering in real-world
scenarios.
</summary>
    <author>
      <name>Mostafa Milani</name>
    </author>
    <author>
      <name>Andrea Cali</name>
    </author>
    <author>
      <name>Leopoldo Bertossi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of RR'16 paper, to appear</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06770v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06770v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06743v1</id>
    <updated>2016-04-22T16:47:04Z</updated>
    <published>2016-04-22T16:47:04Z</published>
    <title>Latent Contextual Bandits and their Application to Personalized
  Recommendations for New Users</title>
    <summary>  Personalized recommendations for new users, also known as the cold-start
problem, can be formulated as a contextual bandit problem. Existing contextual
bandit algorithms generally rely on features alone to capture user variability.
Such methods are inefficient in learning new users' interests. In this paper we
propose Latent Contextual Bandits. We consider both the benefit of leveraging a
set of learned latent user classes for new users, and how we can learn such
latent classes from prior users. We show that our approach achieves a better
regret bound than existing algorithms. We also demonstrate the benefit of our
approach using a large real world dataset and a preliminary user study.
</summary>
    <author>
      <name>Li Zhou</name>
    </author>
    <author>
      <name>Emma Brunskill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25th International Joint Conference on Artificial Intelligence (IJCAI
  2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06721v1</id>
    <updated>2016-04-22T15:58:18Z</updated>
    <published>2016-04-22T15:58:18Z</published>
    <title>Exploiting Deep Semantics and Compositionality of Natural Language for
  Human-Robot-Interaction</title>
    <summary>  We develop a natural language interface for human robot interaction that
implements reasoning about deep semantics in natural language. To realize the
required deep analysis, we employ methods from cognitive linguistics, namely
the modular and compositional framework of Embodied Construction Grammar (ECG)
[Feldman, 2009]. Using ECG, robots are able to solve fine-grained reference
resolution problems and other issues related to deep semantics and
compositionality of natural language. This also includes verbal interaction
with humans to clarify commands and queries that are too ambiguous to be
executed safely. We implement our NLU framework as a ROS package and present
proof-of-concept scenarios with different robots, as well as a survey on the
state of the art.
</summary>
    <author>
      <name>Manfred Eppe</name>
    </author>
    <author>
      <name>Sean Trott</name>
    </author>
    <author>
      <name>Jerome Feldman</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06715v1</id>
    <updated>2016-04-22T15:37:14Z</updated>
    <published>2016-04-22T15:37:14Z</published>
    <title>Parameterized Compilation Lower Bounds for Restricted CNF-formulas</title>
    <summary>  We show unconditional parameterized lower bounds in the area of knowledge
compilation, more specifically on the size of circuits in decomposable negation
normal form (DNNF) that encode CNF-formulas restricted by several graph width
measures. In particular, we show that
  - there are CNF formulas of size $n$ and modular incidence treewidth $k$
whose smallest DNNF-encoding has size $n^{\Omega(k)}$, and
  - there are CNF formulas of size $n$ and incidence neighborhood diversity $k$
whose smallest DNNF-encoding has size $n^{\Omega(\sqrt{k})}$.
  These results complement recent upper bounds for compiling CNF into DNNF and
strengthen---quantitatively and qualitatively---known conditional low\-er
bounds for cliquewidth. Moreover, they show that, unlike for many graph
problems, the parameters considered here behave significantly differently from
treewidth.
</summary>
    <author>
      <name>Stefan Mengel</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06710v1</id>
    <updated>2016-04-22T15:23:27Z</updated>
    <published>2016-04-22T15:23:27Z</published>
    <title>Using Reinforcement Learning to Validate Empirical Game-Theoretic
  Analysis: A Continuous Double Auction Study</title>
    <summary>  Empirical game-theoretic analysis (EGTA) has recently been applied
successfully to analyze the behavior of large numbers of competing traders in a
continuous double auction market. Multiagent simulation methods like EGTA are
useful for studying complex strategic environments like a stock market, where
it is not feasible to solve analytically for the rational behavior of each
agent. A weakness of simulation-based methods in strategic settings, however,
is that it is typically impossible to prove that the strategy profile assigned
to the simulated agents is stable, as in a Nash equilibrium. I propose using
reinforcement learning to analyze the regret of supposed Nash-equilibrium
strategy profiles found by EGTA. I have developed a new library of
reinforcement learning tools, which I have integrated into an extended version
of the market simulator from our prior work. I provide evidence for the
effectiveness of our library methods, both on a suite of benchmark problems
from the literature, and on non-equilibrium strategy profiles in our market
environment. Finally, I use our new reinforcement learning tools to provide
evidence that the equilibria found by EGTA in our recent continuous double
auction study are likely to have only negligible regret, even with respect to
an extended strategy space.
</summary>
    <author>
      <name>Mason Wright</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report, 37 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.1; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06641v1</id>
    <updated>2016-04-22T13:12:38Z</updated>
    <published>2016-04-22T13:12:38Z</published>
    <title>Compact-Table: Efficiently Filtering Table Constraints with Reversible
  Sparse Bit-Sets</title>
    <summary>  In this paper, we describe Compact-Table (CT), a bitwise algorithm to enforce
Generalized Arc Consistency (GAC) on table con- straints. Although this
algorithm is the default propagator for table constraints in or-tools and
OscaR, two publicly available CP solvers, it has never been described so far.
Importantly, CT has been recently improved further with the introduction of
residues, resetting operations and a data-structure called reversible sparse
bit-set, used to maintain tables of supports (following the idea of tabular
reduction): tuples are invalidated incrementally on value removals by means of
bit-set operations. The experimentation that we have conducted with OscaR shows
that CT outperforms state-of-the-art algorithms STR2, STR3, GAC4R, MDD4R and
AC5-TC on standard benchmarks.
</summary>
    <author>
      <name>Jordan Demeulenaere</name>
    </author>
    <author>
      <name>Renaud Hartert</name>
    </author>
    <author>
      <name>Christophe Lecoutre</name>
    </author>
    <author>
      <name>Guillaume Perez</name>
    </author>
    <author>
      <name>Laurent Perron</name>
    </author>
    <author>
      <name>Jean-Charles Régin</name>
    </author>
    <author>
      <name>Pierre Schaus</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06635v1</id>
    <updated>2016-04-22T12:51:11Z</updated>
    <published>2016-04-22T12:51:11Z</published>
    <title>Bridging LSTM Architecture and the Neural Dynamics during Reading</title>
    <summary>  Recently, the long short-term memory neural network (LSTM) has attracted wide
interest due to its success in many tasks. LSTM architecture consists of a
memory cell and three gates, which looks similar to the neuronal networks in
the brain. However, there still lacks the evidence of the cognitive
plausibility of LSTM architecture as well as its working mechanism. In this
paper, we study the cognitive plausibility of LSTM by aligning its internal
architecture with the brain activity observed via fMRI when the subjects read a
story. Experiment results show that the artificial memory vector in LSTM can
accurately predict the observed sequential brain activities, indicating the
correlation between LSTM architecture and the cognitive process of story
reading.
</summary>
    <author>
      <name>Peng Qian</name>
    </author>
    <author>
      <name>Xipeng Qiu</name>
    </author>
    <author>
      <name>Xuanjing Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25th International Joint Conference on Artificial Intelligence
  IJCAI-16</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06614v1</id>
    <updated>2016-04-22T11:53:37Z</updated>
    <published>2016-04-22T11:53:37Z</published>
    <title>Agenda Separability in Judgment Aggregation</title>
    <summary>  One of the better studied properties for operators in judgment aggregation is
independence, which essentially dictates that the collective judgment on one
issue should not depend on the individual judgments given on some other
issue(s) in the same agenda. Independence, although considered a desirable
property, is too strong, because together with mild additional conditions it
implies dictatorship. We propose here a weakening of independence, named agenda
separability: a judgment aggregation rule satisfies it if, whenever the agenda
is composed of several independent sub-agendas, the resulting collective
judgment sets can be computed separately for each sub-agenda and then put
together. We show that this property is discriminant, in the sense that among
judgment aggregation rules so far studied in the literature, some satisfy it
and some do not. We briefly discuss the implications of agenda separability on
the computation of judgment aggregation rules.
</summary>
    <author>
      <name>Jérôme Lang</name>
    </author>
    <author>
      <name>Marija Slavkovik</name>
    </author>
    <author>
      <name>Srdjan Vesic</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06484v1</id>
    <updated>2016-04-21T20:40:35Z</updated>
    <published>2016-04-21T20:40:35Z</published>
    <title>Parallel Strategies Selection</title>
    <summary>  We consider the problem of selecting the best variable-value strategy for
solving a given problem in constraint programming. We show that the recent
Embarrassingly Parallel Search method (EPS) can be used for this purpose. EPS
proposes to solve a problem by decomposing it in a lot of subproblems and to
give them on-demand to workers which run in parallel. Our method uses a part of
these subproblems as a simple sample as defined in statistics for comparing
some strategies in order to select the most promising one that will be used for
solving the remaining subproblems. For each subproblem of the sample, the
parallelism helps us to control the running time of the strategies because it
gives us the possibility to introduce timeouts by stopping a strategy when it
requires more than twice the time of the best one. Thus, we can deal with the
great disparity in solving times for the strategies. The selections we made are
based on the Wilcoxon signed rank tests because no assumption has to be made on
the distribution of the solving times and because these tests can deal with the
censored data that we obtain after introducing timeouts. The experiments we
performed on a set of classical benchmarks for satisfaction and optimization
problems show that our method obtain good performance by selecting almost all
the time the best variable-value strategy and by almost never choosing a
variable-value strategy which is dramatically slower than the best one. Our
method also outperforms the portfolio approach consisting in running some
strategies in parallel and is competitive with the multi armed bandit
framework.
</summary>
    <author>
      <name>Anthony Palmieri</name>
    </author>
    <author>
      <name>Jean-Charles Régin</name>
    </author>
    <author>
      <name>Pierre Schaus</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06356v3</id>
    <updated>2016-07-12T12:36:46Z</updated>
    <published>2016-04-21T15:26:02Z</published>
    <title>Iterative Judgment Aggregation</title>
    <summary>  Judgment aggregation problems form a class of collective decision-making
problems represented in an abstract way, subsuming some well known problems
such as voting. A collective decision can be reached in many ways, but a direct
one-step aggregation of individual decisions is arguably most studied. Another
way to reach collective decisions is by iterative consensus building --
allowing each decision-maker to change their individual decision in response to
the choices of the other agents until a consensus is reached. Iterative
consensus building has so far only been studied for voting problems. Here we
propose an iterative judgment aggregation algorithm, based on movements in an
undirected graph, and we study for which instances it terminates with a
consensus. We also compare the computational complexity of our iterative
procedure with that of related judgment aggregation operators.
</summary>
    <author>
      <name>Marija Slavkovik</name>
    </author>
    <author>
      <name>Wojciech Jamroga</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/978-1-61499-672-9-1528</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/978-1-61499-672-9-1528" rel="related"/>
    <link href="http://arxiv.org/abs/1604.06356v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06356v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06223v1</id>
    <updated>2016-04-21T09:21:21Z</updated>
    <published>2016-04-21T09:21:21Z</published>
    <title>Task scheduling system for UAV operations in indoor environment</title>
    <summary>  Application of UAV in indoor environment is emerging nowadays due to the
advancements in technology. UAV brings more space-flexibility in an occupied or
hardly-accessible indoor environment, e.g., shop floor of manufacturing
industry, greenhouse, nuclear powerplant. UAV helps in creating an autonomous
manufacturing system by executing tasks with less human intervention in
time-efficient manner. Consequently, a scheduler is one essential component to
be focused on; yet the number of reported studies on UAV scheduling has been
minimal. This work proposes a methodology with a heuristic (based on Earliest
Available Time algorithm) which assigns tasks to UAVs with an objective of
minimizing the makespan. In addition, a quick response towards uncertain events
and a quick creation of new high-quality feasible schedule are needed. Hence,
the proposed heuristic is incorporated with Particle Swarm Optimization (PSO)
algorithm to find a quick near optimal schedule. This proposed methodology is
implemented into a scheduler and tested on a few scales of datasets generated
based on a real flight demonstration. Performance evaluation of scheduler is
discussed in detail and the best solution obtained from a selected set of
parameters is reported.
</summary>
    <author>
      <name>Yohanes Khosiawan</name>
    </author>
    <author>
      <name>Young Soo Park</name>
    </author>
    <author>
      <name>Ilkyeong Moon</name>
    </author>
    <author>
      <name>Janardhanan Mukund Nilakantan</name>
    </author>
    <author>
      <name>Izabela Nielsen</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06076v1</id>
    <updated>2016-04-20T19:48:07Z</updated>
    <published>2016-04-20T19:48:07Z</published>
    <title>Question Answering via Integer Programming over Semi-Structured
  Knowledge</title>
    <summary>  Answering science questions posed in natural language is an important AI
challenge. Answering such questions often requires non-trivial inference and
knowledge that goes beyond factoid retrieval. Yet, most systems for this task
are based on relatively shallow Information Retrieval (IR) and statistical
correlation techniques operating on large unstructured corpora. We propose a
structured inference system for this task, formulated as an Integer Linear
Program (ILP), that answers natural language questions using a semi-structured
knowledge base derived from text, including questions requiring multi-step
inference and a combination of multiple facts. On a dataset of real, unseen
science questions, our system significantly outperforms (+14%) the best
previous attempt at structured reasoning for this task, which used Markov Logic
Networks (MLNs). It also improves upon a previous ILP formulation by 17.7%.
When combined with unstructured inference methods, the ILP system significantly
boosts overall performance (+10%). Finally, we show our approach is
substantially more robust to a simple answer perturbation compared to
statistical correlation methods.
</summary>
    <author>
      <name>Daniel Khashabi</name>
    </author>
    <author>
      <name>Tushar Khot</name>
    </author>
    <author>
      <name>Ashish Sabharwal</name>
    </author>
    <author>
      <name>Peter Clark</name>
    </author>
    <author>
      <name>Oren Etzioni</name>
    </author>
    <author>
      <name>Dan Roth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the paper accepted to IJCAI'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06057v2</id>
    <updated>2016-05-31T14:45:58Z</updated>
    <published>2016-04-20T18:47:48Z</published>
    <title>Hierarchical Deep Reinforcement Learning: Integrating Temporal
  Abstraction and Intrinsic Motivation</title>
    <summary>  Learning goal-directed behavior in environments with sparse feedback is a
major challenge for reinforcement learning algorithms. The primary difficulty
arises due to insufficient exploration, resulting in an agent being unable to
learn robust value functions. Intrinsically motivated agents can explore new
behavior for its own sake rather than to directly solve problems. Such
intrinsic behaviors could eventually help the agent solve tasks posed by the
environment. We present hierarchical-DQN (h-DQN), a framework to integrate
hierarchical value functions, operating at different temporal scales, with
intrinsically motivated deep reinforcement learning. A top-level value function
learns a policy over intrinsic goals, and a lower-level function learns a
policy over atomic actions to satisfy the given goals. h-DQN allows for
flexible goal specifications, such as functions over entities and relations.
This provides an efficient space for exploration in complicated environments.
We demonstrate the strength of our approach on two problems with very sparse,
delayed feedback: (1) a complex discrete stochastic decision process, and (2)
the classic ATARI game `Montezuma's Revenge'.
</summary>
    <author>
      <name>Tejas D. Kulkarni</name>
    </author>
    <author>
      <name>Karthik R. Narasimhan</name>
    </author>
    <author>
      <name>Ardavan Saeedi</name>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06057v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06057v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06020v1</id>
    <updated>2016-04-20T16:22:01Z</updated>
    <published>2016-04-20T16:22:01Z</published>
    <title>Constructive Preference Elicitation by Setwise Max-margin Learning</title>
    <summary>  In this paper we propose an approach to preference elicitation that is
suitable to large configuration spaces beyond the reach of existing
state-of-the-art approaches. Our setwise max-margin method can be viewed as a
generalization of max-margin learning to sets, and can produce a set of
"diverse" items that can be used to ask informative queries to the user.
Moreover, the approach can encourage sparsity in the parameter space, in order
to favor the assessment of utility towards combinations of weights that
concentrate on just few features. We present a mixed integer linear programming
formulation and show how our approach compares favourably with Bayesian
preference elicitation alternatives and easily scales to realistic datasets.
</summary>
    <author>
      <name>Stefano Teso</name>
    </author>
    <author>
      <name>Andrea Passerini</name>
    </author>
    <author>
      <name>Paolo Viappiani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages. A conference version of this work is accepted by the 25th
  International Joint Conference on Artificial Intelligence (IJCAI-16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05978v2</id>
    <updated>2016-07-18T20:14:41Z</updated>
    <published>2016-04-20T14:35:12Z</published>
    <title>A topological insight into restricted Boltzmann machines</title>
    <summary>  Restricted Boltzmann Machines (RBMs) and models derived from them have been
successfully used as basic building blocks in deep artificial neural networks
for automatic features extraction, unsupervised weights initialization, but
also as density estimators. Thus, their generative and discriminative
capabilities, but also their computational time are instrumental to a wide
range of applications. Our main contribution is to look at RBMs from a
topological perspective, bringing insights from network science. Firstly, here
we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which
naturally have a small-world topology. Secondly, we demonstrate both on
synthetic and real-world datasets that by constraining RBMs and GRBMs to a
scale-free topology (while still considering local neighborhoods and data
distribution), we reduce the number of weights that need to be computed by a
few orders of magnitude, at virtually no loss in generative performance.
Thirdly, we show that, for a fixed number of weights, our proposed sparse
models (which by design have a higher number of hidden neurons) achieve better
generative capabilities than standard fully connected RBMs and GRBMs (which by
design have a smaller number of hidden neurons), at no additional computational
costs.
</summary>
    <author>
      <name>Decebal Constantin Mocanu</name>
    </author>
    <author>
      <name>Elena Mocanu</name>
    </author>
    <author>
      <name>Phuong H. Nguyen</name>
    </author>
    <author>
      <name>Madeleine Gibescu</name>
    </author>
    <author>
      <name>Antonio Liotta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10994-016-5570-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10994-016-5570-z" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://link.springer.com/article/10.1007/s10994-016-5570-z, Machine
  Learning, issn=1573-0565, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05942v1</id>
    <updated>2016-04-20T13:12:45Z</updated>
    <published>2016-04-20T13:12:45Z</published>
    <title>Multiplayer Games for Learning Multirobot Coordination Algorithms</title>
    <summary>  Humans have an impressive ability to solve complex coordination problems in a
fully distributed manner. This ability, if learned as a set of distributed
multirobot coordination strategies, can enable programming large groups of
robots to collaborate towards complex coordination objectives in a way similar
to humans. Such strategies would offer robustness, adaptability,
fault-tolerance, and, importantly, distributed decision-making. To that end, we
have designed a networked gaming platform to investigate human group behavior,
specifically in solving complex collaborative coordinated tasks. Through this
platform, we are able to limit the communication, sensing, and actuation
capabilities provided to the players. With the aim of learning coordination
algorithms for robots in mind, we define these capabilities to mimic those of a
simple ground robot.
</summary>
    <author>
      <name>Arash Tavakoli</name>
    </author>
    <author>
      <name>Haig Nalbandian</name>
    </author>
    <author>
      <name>Nora Ayanian</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05878v1</id>
    <updated>2016-04-20T09:58:56Z</updated>
    <published>2016-04-20T09:58:56Z</published>
    <title>A Factorization Machine Framework for Testing Bigram Embeddings in
  Knowledgebase Completion</title>
    <summary>  Embedding-based Knowledge Base Completion models have so far mostly combined
distributed representations of individual entities or relations to compute
truth scores of missing links. Facts can however also be represented using
pairwise embeddings, i.e. embeddings for pairs of entities and relations. In
this paper we explore such bigram embeddings with a flexible Factorization
Machine model and several ablations from it. We investigate the relevance of
various bigram types on the fb15k237 dataset and find relative improvements
compared to a compositional model.
</summary>
    <author>
      <name>Johannes Welbl</name>
    </author>
    <author>
      <name>Guillaume Bouchard</name>
    </author>
    <author>
      <name>Sebastian Riedel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for AKBC 2016 workshop, 6pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05865v1</id>
    <updated>2016-04-20T09:08:50Z</updated>
    <published>2016-04-20T09:08:50Z</published>
    <title>Estimating 3D Trajectories from 2D Projections via Disjunctive Factored
  Four-Way Conditional Restricted Boltzmann Machines</title>
    <summary>  Estimation, recognition, and near-future prediction of 3D trajectories based
on their two dimensional projections available from one camera source is an
exceptionally difficult problem due to uncertainty in the trajectories and
environment, high dimensionality of the specific trajectory states, lack of
enough labeled data and so on. In this article, we propose a solution to solve
this problem based on a novel deep learning model dubbed Disjunctive Factored
Four-Way Conditional Restricted Boltzmann Machine (DFFW-CRBM). Our method
improves state-of-the-art deep learning techniques for high dimensional
time-series modeling by introducing a novel tensor factorization capable of
driving forth order Boltzmann machines to considerably lower energy levels, at
no computational costs. DFFW-CRBMs are capable of accurately estimating,
recognizing, and performing near-future prediction of three-dimensional
trajectories from their 2D projections while requiring limited amount of
labeled data. We evaluate our method on both simulated and real-world data,
showing its effectiveness in predicting and classifying complex ball
trajectories and human activities.
</summary>
    <author>
      <name>Decebal Constantin Mocanu</name>
    </author>
    <author>
      <name>Haitham Bou Ammar</name>
    </author>
    <author>
      <name>Luis Puig</name>
    </author>
    <author>
      <name>Eric Eaton</name>
    </author>
    <author>
      <name>Antonio Liotta</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05791v1</id>
    <updated>2016-04-20T02:39:04Z</updated>
    <published>2016-04-20T02:39:04Z</published>
    <title>Procedural urban environments for FPS games</title>
    <summary>  This paper presents a novel approach to procedural generation of urban maps
for First Person Shooter (FPS) games. A multi-agent evolutionary system is
employed to place streets, buildings and other items inside the Unity3D game
engine, resulting in playable video game levels. A computational agent is
trained using machine learning techniques to capture the intent of the game
designer as part of the multi-agent system, and to enable a semi-automated
aesthetic selection for the underlying genetic algorithm.
</summary>
    <author>
      <name>Jan Kruse</name>
    </author>
    <author>
      <name>Ricardo Sosa</name>
    </author>
    <author>
      <name>Andy M. Connor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2843043.2843479</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2843043.2843479" rel="related"/>
    <link href="http://arxiv.org/abs/1604.05791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05753v1</id>
    <updated>2016-04-19T21:22:29Z</updated>
    <published>2016-04-19T21:22:29Z</published>
    <title>Sketching and Neural Networks</title>
    <summary>  High-dimensional sparse data present computational and statistical challenges
for supervised learning. We propose compact linear sketches for reducing the
dimensionality of the input, followed by a single layer neural network. We show
that any sparse polynomial function can be computed, on nearly all sparse
binary vectors, by a single layer neural network that takes a compact sketch of
the vector as input. Consequently, when a set of sparse binary vectors is
approximately separable using a sparse polynomial, there exists a single-layer
neural network that takes a short sketch as input and correctly classifies
nearly all the points. Previous work has proposed using sketches to reduce
dimensionality while preserving the hypothesis class. However, the sketch size
has an exponential dependence on the degree in the case of polynomial
classifiers. In stark contrast, our approach of using improper learning, using
a larger hypothesis class allows the sketch size to have a logarithmic
dependence on the degree. Even in the linear case, our approach allows us to
improve on the pesky $O({1}/{{\gamma}^2})$ dependence of random projections, on
the margin $\gamma$. We empirically show that our approach leads to more
compact neural networks than related methods such as feature hashing at equal
or better performance.
</summary>
    <author>
      <name>Amit Daniely</name>
    </author>
    <author>
      <name>Nevena Lazic</name>
    </author>
    <author>
      <name>Yoram Singer</name>
    </author>
    <author>
      <name>Kunal Talwar</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05692v3</id>
    <updated>2016-06-16T12:19:30Z</updated>
    <published>2016-04-19T19:01:55Z</published>
    <title>Proving the Incompatibility of Efficiency and Strategyproofness via SMT
  Solving</title>
    <summary>  Two important requirements when aggregating the preferences of multiple
agents are that the outcome should be economically efficient and the
aggregation mechanism should not be manipulable. In this paper, we provide a
computer-aided proof of a sweeping impossibility using these two conditions for
randomized aggregation mechanisms. More precisely, we show that every efficient
aggregation mechanism can be manipulated for all expected utility
representations of the agents' preferences. This settles a conjecture by Aziz
et al. [2013b] and strengthens a number of existing theorems, including
statements that were shown within the special domain of assignment. Our proof
is obtained by formulating the claim as a satisfiability problem over
predicates from real-valued arithmetic, which is then checked using an SMT
(satisfiability modulo theories) solver. To the best of our knowledge, this is
the first application of SMT solvers in computational social choice.
</summary>
    <author>
      <name>Florian Brandl</name>
    </author>
    <author>
      <name>Felix Brandt</name>
    </author>
    <author>
      <name>Christian Geist</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05692v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05692v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05636v1</id>
    <updated>2016-04-19T16:08:33Z</updated>
    <published>2016-04-19T16:08:33Z</published>
    <title>Pattern-Based Approach to the Workflow Satisfiability Problem with
  User-Independent Constraints</title>
    <summary>  The fixed parameter tractable (FPT) approach is a powerful tool in tackling
computationally hard problems. In this paper we link FPT results to classic
artificial intelligence techniques to show how they complement each other.
Specifically, we consider the workflow satisfiability problem (WSP) which asks
whether there exists an assignment of authorised users to the steps in a
workflow specification, subject to certain constraints on the assignment. It
was shown that WSP restricted to the class of user-independent constraints
(UI), covering many practical cases, admits FPT algorithms.
  We show that the FPT nature of WSP with UI constraints decomposes the problem
into two levels, and exploit this in a new FPT algorithm that is by many orders
of magnitude faster then the previous state-of-the-art WSP algorithm.
  The WSP with UI constraints can also be viewed as an extension of the
hypergraph list colouring problem. Inspired by a classic graph colouring method
called Zykov's Contraction, we designed a new pseudo-boolean (PB) formulation
of WSP with UI constraints that also exploits the two-level split of the
problem. Our experiments showed that, in many cases, this formulation being
solved with a general purpose PB solver demonstrated performance comparable to
that of our bespoke FPT algorithm. This raises the potential of using general
purpose solvers to tackle FPT problems efficiently.
  We also study the practical, average-case, performance of various algorithms.
To support this we extend studies of phase transition phenomena in the
understanding of the average computational effort needed to solve decision
problems. We investigate, for the first time, the phase transition properties
of the WSP, under a model for generation of random instances, and note that the
methods of the phase transition study need to be adjusted to FPT problems.
</summary>
    <author>
      <name>Daniel Karapetyan</name>
    </author>
    <author>
      <name>Andrew J. Parkes</name>
    </author>
    <author>
      <name>Gregory Gutin</name>
    </author>
    <author>
      <name>Andrei Gagarin</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05535v2</id>
    <updated>2016-05-12T11:29:50Z</updated>
    <published>2016-04-19T12:04:14Z</published>
    <title>The SP theory of intelligence and the representation and processing of
  knowledge in the brain</title>
    <summary>  The "SP theory of intelligence", with its realisation in the "SP computer
model", aims to simplify and integrate observations and concepts across
AI-related fields, with information compression as a unifying theme. This paper
describes how abstract structures and processes in the theory may be realised
in terms of neurons, their interconnections, and the transmission of signals
between neurons. This part of the SP theory -- "SP-neural" -- is a tentative
and partial model for the representation and processing of knowledge in the
brain. In the SP theory (apart from SP-neural), all kinds of knowledge are
represented with "patterns", where a pattern is an array of atomic symbols in
one or two dimensions. In SP-neural, the concept of a "pattern" is realised as
an array of neurons called a "pattern assembly", similar to Hebb's concept of a
"cell assembly" but with important differences. Central to the processing of
information in the SP system is the powerful concept of "multiple alignment",
borrowed and adapted from bioinformatics. Processes such as pattern
recognition, reasoning and problem solving are achieved via the building of
multiple alignments, while unsupervised learning -- significantly different
from the "Hebbian" kinds of learning -- is achieved by creating patterns from
sensory information and also by creating patterns from multiple alignments in
which there is a partial match between one pattern and another. Short-lived
neural structures equivalent to multiple alignments will be created via an
inter-play of excitatory and inhibitory neural signals. The paper discusses
several associated issues, with relevant empirical evidence.
</summary>
    <author>
      <name>J Gerard Wolff</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05535v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05535v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05472v2</id>
    <updated>2016-07-13T14:30:23Z</updated>
    <published>2016-04-19T08:51:03Z</published>
    <title>Demand Prediction and Placement Optimization for Electric Vehicle
  Charging Stations</title>
    <summary>  Effective placement of charging stations plays a key role in Electric Vehicle
(EV) adoption. In the placement problem, given a set of candidate sites, an
optimal subset needs to be selected with respect to the concerns of both (a)
the charging station service provider, such as the demand at the candidate
sites and the budget for deployment, and (b) the EV user, such as charging
station reachability and short waiting times at the station. This work
addresses these concerns, making the following three novel contributions: (i) a
supervised multi-view learning framework using Canonical Correlation Analysis
(CCA) for demand prediction at candidate sites, using multiple datasets such as
points of interest information, traffic density, and the historical usage at
existing charging stations; (ii) a mixed-packing-and- covering optimization
framework that models competing concerns of the service provider and EV users;
(iii) an iterative heuristic to solve these problems by alternately invoking
knapsack and set cover algorithms. The performance of the demand prediction
model and the placement optimization heuristic are evaluated using real world
data.
</summary>
    <author>
      <name>Ragavendran Gopalakrishnan</name>
    </author>
    <author>
      <name>Arpita Biswas</name>
    </author>
    <author>
      <name>Alefiya Lightwala</name>
    </author>
    <author>
      <name>Skanda Vasudevan</name>
    </author>
    <author>
      <name>Partha Dutta</name>
    </author>
    <author>
      <name>Abhishek Tripathi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the proceedings of the 25th International Joint
  Conference on Artificial Intelligence IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05472v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05472v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05471v2</id>
    <updated>2016-07-14T15:20:26Z</updated>
    <published>2016-04-19T08:42:14Z</published>
    <title>Managing Overstaying Electric Vehicles in Park-and-Charge Facilities</title>
    <summary>  With the increase in adoption of Electric Vehicles (EVs), proper utilization
of the charging infrastructure is an emerging challenge for service providers.
Overstaying of an EV after a charging event is a key contributor to low
utilization. Since overstaying is easily detectable by monitoring the power
drawn from the charger, managing this problem primarily involves designing an
appropriate "penalty" during the overstaying period. Higher penalties do
discourage overstaying; however, due to uncertainty in parking duration, less
people would find such penalties acceptable, leading to decreased utilization
(and revenue). To analyze this central trade-off, we develop a novel framework
that integrates models for realistic user behavior into queueing dynamics to
locate the optimal penalty from the points of view of utilization and revenue,
for different values of the external charging demand. Next, when the model
parameters are unknown, we show how an online learning algorithm, such as UCB,
can be adapted to learn the optimal penalty. Our experimental validation, based
on charging data from London, shows that an appropriate penalty can increase
both utilization and revenue while significantly reducing overstaying.
</summary>
    <author>
      <name>Arpita Biswas</name>
    </author>
    <author>
      <name>Ragavendran Gopalakrishnan</name>
    </author>
    <author>
      <name>Partha Dutta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of the 25th International Joint Conference
  on Artificial Intelligence IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05471v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05471v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05468v1</id>
    <updated>2016-04-19T08:31:23Z</updated>
    <published>2016-04-19T08:31:23Z</published>
    <title>Understanding Rating Behaviour and Predicting Ratings by Identifying
  Representative Users</title>
    <summary>  Online user reviews describing various products and services are now abundant
on the web. While the information conveyed through review texts and ratings is
easily comprehensible, there is a wealth of hidden information in them that is
not immediately obvious. In this study, we unlock this hidden value behind user
reviews to understand the various dimensions along which users rate products.
We learn a set of users that represent each of these dimensions and use their
ratings to predict product ratings. Specifically, we work with restaurant
reviews to identify users whose ratings are influenced by dimensions like
'Service', 'Atmosphere' etc. in order to predict restaurant ratings and
understand the variation in rating behaviour across different cuisines. While
previous approaches to obtaining product ratings require either a large number
of user ratings or a few review texts, we show that it is possible to predict
ratings with few user ratings and no review text. Our experiments show that our
approach outperforms other conventional methods by 16-27% in terms of RMSE.
</summary>
    <author>
      <name>Rahul Kamath</name>
    </author>
    <author>
      <name>Masanao Ochi</name>
    </author>
    <author>
      <name>Yutaka Matsuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 29th Pacific Asia Conference on Language, Information and
  Computation (PACLIC-29)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08390v1</id>
    <updated>2016-04-19T07:52:30Z</updated>
    <published>2016-04-19T07:52:30Z</published>
    <title>Estimation of Passenger Route Choice Pattern Using Smart Card Data for
  Complex Metro Systems</title>
    <summary>  Nowadays, metro systems play an important role in meeting the urban
transportation demand in large cities. The understanding of passenger route
choice is critical for public transit management. The wide deployment of
Automated Fare Collection(AFC) systems opens up a new opportunity. However,
only each trip's tap-in and tap-out timestamp and stations can be directly
obtained from AFC system records; the train and route chosen by a passenger are
unknown, which are necessary to solve our problem. While existing methods work
well in some specific situations, they don't work for complicated situations.
In this paper, we propose a solution that needs no additional equipment or
human involvement than the AFC systems. We develop a probabilistic model that
can estimate from empirical analysis how the passenger flows are dispatched to
different routes and trains. We validate our approach using a large scale data
set collected from the Shenzhen metro system. The measured results provide us
with useful inputs when building the passenger path choice model.
</summary>
    <author>
      <name>Juanjuan Zhao</name>
    </author>
    <author>
      <name>Fan Zhang</name>
    </author>
    <author>
      <name>Lai Tu</name>
    </author>
    <author>
      <name>Chengzhong Xu</name>
    </author>
    <author>
      <name>Dayong Shen</name>
    </author>
    <author>
      <name>Chen Tian</name>
    </author>
    <author>
      <name>Xiang-Yang Li</name>
    </author>
    <author>
      <name>Zhengxi Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.08390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05419v1</id>
    <updated>2016-04-19T03:36:20Z</updated>
    <published>2016-04-19T03:36:20Z</published>
    <title>Extending the Harper Identity to Iterated Belief Change</title>
    <summary>  The field of iterated belief change has focused mainly on revision, with the
other main operator of AGM belief change theory, i.e. contraction, receiving
relatively little attention. In this paper we extend the Harper Identity from
single-step change to define iterated contraction in terms of iterated
revision. Specifically, just as the Harper Identity provides a recipe for
defining the belief set resulting from contracting A in terms of (i) the
initial belief set and (ii) the belief set resulting from revision by not-A, we
look at ways to define the plausibility ordering over worlds resulting from
contracting A in terms of (iii) the initial plausibility ordering, and (iv) the
plausibility ordering resulting from revision by not-A. After noting that the
most straightforward such extension leads to a trivialisation of the space of
permissible orderings, we provide a family of operators for combining
plausibility orderings that avoid such a result. These operators are
characterised in our domain of interest by a pair of intuitively compelling
properties, which turn out to enable the derivation of a number of iterated
contraction postulates from postulates for iterated revision. We finish by
observing that a salient member of this family allows for the derivation of
counterparts for contraction of some well known iterated revision operators, as
well as for defining new iterated contraction operators.
</summary>
    <author>
      <name>Jake Chandler</name>
    </author>
    <author>
      <name>Richard Booth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper accepted to IJCAI16. 23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05358v1</id>
    <updated>2016-04-18T21:43:44Z</updated>
    <published>2016-04-18T21:43:44Z</published>
    <title>Text-based LSTM networks for Automatic Music Composition</title>
    <summary>  In this paper, we introduce new methods and discuss results of text-based
LSTM (Long Short-Term Memory) networks for automatic music composition. The
proposed network is designed to learn relationships within text documents that
represent chord progressions and drum tracks in two case studies. In the
experiments, word-RNNs (Recurrent Neural Networks) show good results for both
cases, while character-based RNNs (char-RNNs) only succeed to learn chord
progressions. The proposed system can be used for fully automatic composition
or as semi-automatic systems that help humans to compose music by controlling a
diversity parameter of the model.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the 1st Conference on Computer Simulation of Musical
  Creativity, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05288v2</id>
    <updated>2016-07-15T00:14:04Z</updated>
    <published>2016-04-18T19:37:46Z</published>
    <title>Inductive Coherence</title>
    <summary>  While probability theory is normally applied to external environments, there
has been some recent interest in probabilistic modeling of the outputs of
computations that are too expensive to run. Since mathematical logic is a
powerful tool for reasoning about computer programs, we consider this problem
from the perspective of integrating probability and logic. Recent work on
assigning probabilities to mathematical statements has used the concept of
coherent distributions, which satisfy logical constraints such as the
probability of a sentence and its negation summing to one. Although there are
algorithms which converge to a coherent probability distribution in the limit,
this yields only weak guarantees about finite approximations of these
distributions. In our setting, this is a significant limitation: Coherent
distributions assign probability one to all statements provable in a specific
logical theory, such as Peano Arithmetic, which can prove what the output of
any terminating computation is; thus, a coherent distribution must assign
probability one to the output of any terminating computation. To model
uncertainty about computations, we propose to work with approximations to
coherent distributions. We introduce inductive coherence, a strengthening of
coherence that provides appropriate constraints on finite approximations, and
propose an algorithm which satisfies this criterion.
</summary>
    <author>
      <name>Scott Garrabrant</name>
    </author>
    <author>
      <name>Benya Fallenstein</name>
    </author>
    <author>
      <name>Abram Demski</name>
    </author>
    <author>
      <name>Nate Soares</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Changed "Uniform" to "Inductive, updated bibliography references</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05288v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05288v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05280v4</id>
    <updated>2016-09-07T18:43:24Z</updated>
    <published>2016-04-18T19:04:59Z</published>
    <title>Asymptotic Convergence in Online Learning with Unbounded Delays</title>
    <summary>  We study the problem of predicting the results of computations that are too
expensive to run, via the observation of the results of smaller computations.
We model this as an online learning problem with delayed feedback, where the
length of the delay is unbounded, which we study mainly in a stochastic
setting. We show that in this setting, consistency is not possible in general,
and that optimal forecasters might not have average regret going to zero.
However, it is still possible to give algorithms that converge asymptotically
to Bayes-optimal predictions, by evaluating forecasters on specific sparse
independent subsequences of their predictions. We give an algorithm that does
this, which converges asymptotically on good behavior, and give very weak
bounds on how long it takes to converge. We then relate our results back to the
problem of predicting large computations in a deterministic setting.
</summary>
    <author>
      <name>Scott Garrabrant</name>
    </author>
    <author>
      <name>Nate Soares</name>
    </author>
    <author>
      <name>Jessica Taylor</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05280v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05280v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05273v1</id>
    <updated>2016-04-18T18:35:38Z</updated>
    <published>2016-04-18T18:35:38Z</published>
    <title>Learning Possibilistic Logic Theories from Default Rules</title>
    <summary>  We introduce a setting for learning possibilistic logic theories from
defaults of the form "if alpha then typically beta". We first analyse this
problem from the point of view of machine learning theory, determining the VC
dimension of possibilistic stratifications as well as the complexity of the
associated learning problems, after which we present a heuristic learning
algorithm that can easily scale to thousands of defaults. An important property
of our approach is that it is inherently able to handle noisy and conflicting
sets of defaults. Among others, this allows us to learn possibilistic logic
theories from crowdsourced data and to approximate propositional Markov logic
networks using heuristic MAP solvers. We present experimental results that
demonstrate the effectiveness of this approach.
</summary>
    <author>
      <name>Ondrej Kuzelka</name>
    </author>
    <author>
      <name>Jesse Davis</name>
    </author>
    <author>
      <name>Steven Schockaert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Long version of a paper accepted at IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05170v3</id>
    <updated>2016-09-07T17:17:54Z</updated>
    <published>2016-04-18T14:13:52Z</published>
    <title>A Repeated Signal Difference for Recognising Patterns</title>
    <summary>  This paper describes a new mechanism that might help with defining pattern
sequences, by the fact that it can produce an upper bound on the ensemble value
that can persistently oscillate with the actual values produced from each
pattern. With every firing event, a node also receives an on/off feedback
switch. If the node fires, then it sends a feedback result depending on the
input signal strength. If the input signal is positive or larger, it can store
an 'on' switch feedback for the next iteration. If the signal is negative or
smaller, it can store an 'off' switch feedback for the next iteration. If the
node does not fire, then it does not affect the current feedback situation and
receives the switch command produced by the last active pattern event for the
same neuron. The upper bound therefore also represents the largest or most
enclosing pattern set and the lower value is for the actual set of firing
patterns. If the pattern sequence repeats, it will oscillate between the two
values, allowing them to be recognised and measured more easily, over time.
Tests show that changing the sequence ordering produces different value sets,
which can also be measured.
</summary>
    <author>
      <name>Kieran Greer</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05170v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05170v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05129v2</id>
    <updated>2016-05-29T18:39:52Z</updated>
    <published>2016-04-18T13:17:55Z</published>
    <title>Memory shapes time perception and intertemporal choices</title>
    <summary>  There is a consensus that human and non-human subjects experience temporal
distortions in many stages of their perceptual and decision-making systems.
Similarly, intertemporal choice research has shown that decision-makers
undervalue future outcomes relative to immediate ones. Here we combine
techniques from information theory and artificial intelligence to show how both
temporal distortions and intertemporal choice preferences can be explained as a
consequence of the coding efficiency of sensorimotor representation. In
particular, the model implies that interactions that constrain future behavior
are perceived as being both longer in duration and more valuable. Furthermore,
using simulations of artificial agents, we investigate how memory constraints
enforce a renormalization of the perceived timescales. Our results show that
qualitatively different discount functions, such as exponential and hyperbolic
discounting, arise as a consequence of an agent's probabilistic model of the
world.
</summary>
    <author>
      <name>Pedro A. Ortega</name>
    </author>
    <author>
      <name>Naftali Tishby</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 4 figures, 2 tables. Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05129v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05129v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05091v2</id>
    <updated>2016-04-19T14:09:26Z</updated>
    <published>2016-04-18T11:15:56Z</published>
    <title>End-to-End Tracking and Semantic Segmentation Using Recurrent Neural
  Networks</title>
    <summary>  In this work we present a novel end-to-end framework for tracking and
classifying a robot's surroundings in complex, dynamic and only partially
observable real-world environments. The approach deploys a recurrent neural
network to filter an input stream of raw laser measurements in order to
directly infer object locations, along with their identity in both visible and
occluded areas. To achieve this we first train the network using unsupervised
Deep Tracking, a recently proposed theoretical framework for end-to-end space
occupancy prediction. We show that by learning to track on a large amount of
unsupervised data, the network creates a rich internal representation of its
environment which we in turn exploit through the principle of inductive
transfer of knowledge to perform the task of it's semantic classification. As a
result, we show that only a small amount of labelled data suffices to steer the
network towards mastering this additional task. Furthermore we propose a novel
recurrent neural network architecture specifically tailored to tracking and
semantic classification in real-world robotics applications. We demonstrate the
tracking and classification performance of the method on real-world data
collected at a busy road junction. Our evaluation shows that the proposed
end-to-end framework compares favourably to a state-of-the-art, model-free
tracking solution and that it outperforms a conventional one-shot training
scheme for semantic classification.
</summary>
    <author>
      <name>Peter Ondruska</name>
    </author>
    <author>
      <name>Julie Dequaire</name>
    </author>
    <author>
      <name>Dominic Zeng Wang</name>
    </author>
    <author>
      <name>Ingmar Posner</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05091v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05091v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05086v1</id>
    <updated>2016-04-18T11:07:02Z</updated>
    <published>2016-04-18T11:07:02Z</published>
    <title>Normative Multiagent Systems: A Dynamic Generalization</title>
    <summary>  Social norms are powerful formalism in coordinating autonomous agents'
behaviour to achieve certain objectives. In this paper, we propose a dynamic
normative system to enable the reasoning of the changes of norms under
different circumstances, which cannot be done in the existing static normative
systems. We study two important problems (norm synthesis and norm recognition)
related to the autonomy of the entire system and the agents, and characterise
the computational complexities of solving these problems.
</summary>
    <author>
      <name>Xiaowei Huang</name>
    </author>
    <author>
      <name>Ji Ruan</name>
    </author>
    <author>
      <name>Qingliang Chen</name>
    </author>
    <author>
      <name>Kaile Su</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages. A conference version of this work is accepted by the 25th
  International Joint Conference on Artificial Intelligence (IJCAI-16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05085v1</id>
    <updated>2016-04-18T11:06:32Z</updated>
    <published>2016-04-18T11:06:32Z</published>
    <title>Mastering $2048$ with Delayed Temporal Coherence Learning, Multi-State
  Weight Promotion, Redundant Encoding and Carousel Shaping</title>
    <summary>  $2048$ is an engaging single-player, nondeterministic video puzzle game,
which, thanks to the simple rules and hard-to-master gameplay, has gained
massive popularity in recent years. As $2048$ can be conveniently embedded into
the discrete-state Markov decision processes framework, we treat it as a
testbed for evaluating existing and new methods in reinforcement learning. With
the aim to develop a strong $2048$ playing program, we employ temporal
difference learning with systematic n-tuple networks. We show that this basic
method can be significantly improved with temporal coherence learning,
multi-stage function approximator with weight promotion, carousel shaping, and
redundant encoding. In addition, we demonstrate how to take advantage of the
characteristics of the n-tuple network, to improve the algorithmic
effectiveness of the learning process by i) delaying the (decayed) update and
applying lock-free optimistic parallelism to effortlessly make advantage of
multiple CPU cores. This way, we were able to develop the best known $2048$
playing program to date, which confirms the effectiveness of the introduced
methods for discrete-state Markov decision problems.
</summary>
    <author>
      <name>Wojciech Jaśkowski</name>
    </author>
    <link href="http://arxiv.org/abs/1604.05085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05024v1</id>
    <updated>2016-04-18T08:01:02Z</updated>
    <published>2016-04-18T08:01:02Z</published>
    <title>Empirical study of PROXTONE and PROXTONE$^+$ for Fast Learning of Large
  Scale Sparse Models</title>
    <summary>  PROXTONE is a novel and fast method for optimization of large scale
non-smooth convex problem \cite{shi2015large}. In this work, we try to use
PROXTONE method in solving large scale \emph{non-smooth non-convex} problems,
for example training of sparse deep neural network (sparse DNN) or sparse
convolutional neural network (sparse CNN) for embedded or mobile device.
PROXTONE converges much faster than first order methods, while first order
method is easy in deriving and controlling the sparseness of the solutions.
Thus in some applications, in order to train sparse models fast, we propose to
combine the merits of both methods, that is we use PROXTONE in the first
several epochs to reach the neighborhood of an optimal solution, and then use
the first order method to explore the possibility of sparsity in the following
training. We call such method PROXTONE plus (PROXTONE$^+$). Both PROXTONE and
PROXTONE$^+$ are tested in our experiments, and which demonstrate both methods
improved convergence speed twice as fast at least on diverse sparse model
learning problems, and at the same time reduce the size to 0.5\% for DNN
models. The source of all the algorithms is available upon request.
</summary>
    <author>
      <name>Ziqiang Shi</name>
    </author>
    <author>
      <name>Rujie Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1311.2115 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05006v2</id>
    <updated>2016-04-27T13:22:52Z</updated>
    <published>2016-04-18T06:16:49Z</published>
    <title>Expressive Completeness of Existential Rule Languages for Ontology-based
  Query Answering</title>
    <summary>  Existential rules, also known as data dependencies in Databases, have been
recently rediscovered as a promising family of languages for Ontology-based
Query Answering. In this paper, we prove that disjunctive embedded dependencies
exactly capture the class of recursively enumerable ontologies in
Ontology-based Conjunctive Query Answering (OCQA). Our expressive completeness
result does not rely on any built-in linear order on the database. To establish
the expressive completeness, we introduce a novel semantic definition for OCQA
ontologies. We also show that neither the class of disjunctive tuple-generating
dependencies nor the class of embedded dependencies is expressively complete
for recursively enumerable OCQA ontologies.
</summary>
    <author>
      <name>Heng Zhang</name>
    </author>
    <author>
      <name>Yan Zhang</name>
    </author>
    <author>
      <name>Jia-Huai You</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; the full version of a paper to appear in IJCAI 2016.
  Changes (regarding to v1): a new reference has been added, and some typos
  have been corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05006v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05006v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04928v1</id>
    <updated>2016-04-17T21:07:02Z</updated>
    <published>2016-04-17T21:07:02Z</published>
    <title>Learning to Incentivize: Eliciting Effort via Output Agreement</title>
    <summary>  In crowdsourcing when there is a lack of verification for contributed
answers, output agreement mechanisms are often used to incentivize participants
to provide truthful answers when the correct answer is hold by the majority. In
this paper, we focus on using output agreement mechanisms to elicit effort, in
addition to eliciting truthful answers, from a population of workers. We
consider a setting where workers have heterogeneous cost of effort exertion and
examine the data requester's problem of deciding the reward level in output
agreement for optimal elicitation. In particular, when the requester knows the
cost distribution, we derive the optimal reward level for output agreement
mechanisms. This is achieved by first characterizing Bayesian Nash equilibria
of output agreement mechanisms for a given reward level. When the requester
does not know the cost distribution, we develop sequential mechanisms that
combine learning the cost distribution with incentivizing effort exertion to
approximately determine the optimal reward level.
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Yiling Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages; short version is accepted to IJCAI 16</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04894v1</id>
    <updated>2016-04-17T16:32:27Z</updated>
    <published>2016-04-17T16:32:27Z</published>
    <title>A global constraint for closed itemset mining</title>
    <summary>  Discovering the set of closed frequent patterns is one of the fundamental
problems in Data Mining. Recent Constraint Programming (CP) approaches for
declarative itemset mining have proven their usefulness and flexibility. But
the wide use of reified constraints in current CP approaches raises many
difficulties to cope with high dimensional datasets. This paper proposes CLOSED
PATTERN global constraint which does not require any reified constraints nor
any extra variables to encode efficiently the Closed Frequent Pattern Mining
(CFPM) constraint. CLOSED-PATTERN captures the particular semantics of the CFPM
problem in order to ensure a polynomial pruning algorithm ensuring domain
consistency. The computational properties of our constraint are analyzed and
their practical effectiveness is experimentally evaluated.
</summary>
    <author>
      <name>Mehdi Maamar</name>
    </author>
    <author>
      <name>Nadjib Lazaar</name>
    </author>
    <author>
      <name>Samir Loudni</name>
    </author>
    <author>
      <name>Yahia Lebbah</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04795v2</id>
    <updated>2016-07-10T16:24:37Z</updated>
    <published>2016-04-16T20:54:12Z</published>
    <title>KOGNAC: Efficient Encoding of Large Knowledge Graphs</title>
    <summary>  Many Web applications require efficient querying of large Knowledge Graphs
(KGs). We propose KOGNAC, a dictionary-encoding algorithm designed to improve
SPARQL querying with a judicious combination of statistical and semantic
techniques. In KOGNAC, frequent terms are detected with a frequency
approximation algorithm and encoded to maximise compression. Infrequent terms
are semantically grouped into ontological classes and encoded to increase data
locality. We evaluated KOGNAC in combination with state-of-the-art RDF engines,
and observed that it significantly improves SPARQL querying on KGs with up to
1B edges.
</summary>
    <author>
      <name>Jacopo Urbani</name>
    </author>
    <author>
      <name>Sourav Dutta</name>
    </author>
    <author>
      <name>Sairam Gurajada</name>
    </author>
    <author>
      <name>Gerhard Weikum</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04795v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04795v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04789v1</id>
    <updated>2016-04-16T19:38:21Z</updated>
    <published>2016-04-16T19:38:21Z</published>
    <title>A Hierarchical Genetic Optimization of a Fuzzy Logic System for Flow
  Control in Micro Grids</title>
    <summary>  Computational Intelligence techniques are today widely used to solve complex
engineering problems. Bio-inspired algorithms like Genetic Algorithms and Fuzzy
Inference Systems are nowadays adopted as hybrids techniques in the commercial
and industrial environment. In this paper, we present an interesting
application of the FUZZY-GA paradigm to Smart Grids. In particular, this study
focuses on the possibility of tuning a Fuzzy Rule Base trying to discover, by
means of a GA, a minimal fuzzy rules set in a Fuzzy Logic Controller (FLC)
adopted to perform decision making for the power flow management task in a
microgrid. The RB optimization is obtained through Hierarchical Genetic
Algorithm, based on an encoding scheme inspired by Nature, applied to the
optimization of the FIS parameters. Tests show how the proposed controller
scheme is effective in maximizing the economic return when dealing with the
problem of power flows management in a microgrid, equipped with an energy
storage system.
</summary>
    <author>
      <name>Enrico De Santis</name>
    </author>
    <author>
      <name>Alireza Sadeghian</name>
    </author>
    <author>
      <name>Antonello Rizzi</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04737v1</id>
    <updated>2016-04-16T12:13:02Z</updated>
    <published>2016-04-16T12:13:02Z</published>
    <title>Studying the impact of negotiation environments on negotiation teams'
  performance</title>
    <summary>  In this article we study the impact of the negotiation environment on the
performance of several intra-team strategies (team dynamics) for agent-based
negotiation teams that negotiate with an opponent. An agent-based negotiation
team is a group of agents that joins together as a party because they share
common interests in the negotiation at hand. It is experimentally shown how
negotiation environment conditions like the deadline of both parties, the
concession speed of the opponent, similarity among team members, and team size
affect performance metrics like the minimum utility of team members, the
average utility of team members, and the number of negotiation rounds. Our goal
is identifying which intra-team strategies work better in different
environmental conditions in order to provide useful knowledge for team members
to select appropriate intra-team strategies according to environmental
conditions.
</summary>
    <author>
      <name>Victor Sanchez-Anguix</name>
    </author>
    <author>
      <name>Vicente Julian</name>
    </author>
    <author>
      <name>Vicente Botti</name>
    </author>
    <author>
      <name>Ana Garcia-Fornes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2012.07.017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2012.07.017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11, I.2.8, K.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04736v1</id>
    <updated>2016-04-16T12:11:02Z</updated>
    <published>2016-04-16T12:11:02Z</published>
    <title>Intra-Team Strategies for Teams Negotiating Against Competitor,
  Matchers, and Conceders</title>
    <summary>  Under some circumstances, a group of individuals may need to negotiate
together as a negotiation team against another party. Unlike bilateral
negotiation between two individuals, this type of negotiations entails to adopt
an intra-team strategy for negotiation teams in order to make team decisions
and accordingly negotiate with the opponent. It is crucial to be able to
negotiate successfully with heterogeneous opponents since opponents'
negotiation strategy and behavior may vary in an open environment. While one
opponent might collaborate and concede over time, another may not be inclined
to concede. This paper analyzes the performance of recently proposed intra-team
strategies for negotiation teams against different categories of opponents:
competitors, matchers, and conceders. Furthermore, it provides an extension of
the negotiation tool Genius for negotiation teams in bilateral settings.
Consequently, this work facilitates research in negotiation teams.
</summary>
    <author>
      <name>Victor Sanchez-Anguix</name>
    </author>
    <author>
      <name>Reyhan Aydogan</name>
    </author>
    <author>
      <name>Vicente Julian</name>
    </author>
    <author>
      <name>Catholijn Jonker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-4-431-54758-7_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-4-431-54758-7_1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Novel Insights in Agent-based Complex Automated Negotiation, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11, I.2.8, K.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04730v1</id>
    <updated>2016-04-16T11:53:46Z</updated>
    <published>2016-04-16T11:53:46Z</published>
    <title>Evolutionary-aided negotiation model for bilateral bargaining in Ambient
  Intelligence domains with complex utility functions</title>
    <summary>  Ambient Intelligence aims to offer personalized services and easier ways of
interaction between people and systems. Since several users and systems may
coexist in these environments, it is quite possible that entities with opposing
preferences need to cooperate to reach their respective goals. Automated
negotiation is pointed as one of the mechanisms that may provide a solution to
this kind of problems. In this article, a multi-issue bilateral bargaining
model for Ambient Intelligence domains is presented where it is assumed that
agents have computational bounded resources and do not know their opponents'
preferences. The main goal of this work is to provide negotiation models that
obtain efficient agreements while maintaining the computational cost low. A
niching genetic algorithm is used before the negotiation process to sample
one's own utility function (self-sampling). During the negotiation process,
genetic operators are applied over the opponent's and one's own offers in order
to sample new offers that are interesting for both parties. Results show that
the proposed model is capable of outperforming similarity heuristics which only
sample before the negotiation process and of obtaining similar results to
similarity heuristics which have access to all of the possible offers.
</summary>
    <author>
      <name>Victor Sanchez-Anguix</name>
    </author>
    <author>
      <name>Soledad Valero</name>
    </author>
    <author>
      <name>Vicente Julian</name>
    </author>
    <author>
      <name>Vicente Botti</name>
    </author>
    <author>
      <name>Ana Garcia-Fornes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2010.11.018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2010.11.018" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11, I.2.8, K.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04728v1</id>
    <updated>2016-04-16T11:42:38Z</updated>
    <published>2016-04-16T11:42:38Z</published>
    <title>Reaching Unanimous Agreements Within Agent-Based Negotiation Teams With
  Linear and Monotonic Utility Functions</title>
    <summary>  In this article, an agent-based negotiation model for negotiation teams that
negotiate a deal with an opponent is presented. Agent-based negotiation teams
are groups of agents that join together as a single negotiation party because
they share an interest that is related to the negotiation process. The model
relies on a trusted mediator that coordinates and helps team members in the
decisions that they have to take during the negotiation process: which offer is
sent to the opponent, and whether the offers received from the opponent are
accepted. The main strength of the proposed negotiation model is the fact that
it guarantees unanimity within team decisions since decisions report a utility
to team members that is greater than or equal to their aspiration levels at
each negotiation round. This work analyzes how unanimous decisions are taken
within the team and the robustness of the model against different types of
manipulations. An empirical evaluation is also performed to study the impact of
the different parameters of the model.
</summary>
    <author>
      <name>Victor Sanchez-Anguix</name>
    </author>
    <author>
      <name>Vicente Julian</name>
    </author>
    <author>
      <name>Vicente Botti</name>
    </author>
    <author>
      <name>Ana Garcia-Fornes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSMCB.2011.2177658</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSMCB.2011.2177658" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics), 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11, I.2.8, K.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04727v1</id>
    <updated>2016-04-16T11:37:50Z</updated>
    <published>2016-04-16T11:37:50Z</published>
    <title>Tasks for agent-based negotiation teams: Analysis, review, and
  challenges</title>
    <summary>  An agent-based negotiation team is a group of interdependent agents that join
together as a single negotiation party due to their shared interests in the
negotiation at hand. The reasons to employ an agent-based negotiation team may
vary: (i) more computation and parallelization capabilities, (ii) unite agents
with different expertise and skills whose joint work makes it possible to
tackle complex negotiation domains, (iii) the necessity to represent different
stakeholders or different preferences in the same party (e.g., organizations,
countries, and married couple). The topic of agent-based negotiation teams has
been recently introduced in multi-agent research. Therefore, it is necessary to
identify good practices, challenges, and related research that may help in
advancing the state-of-the-art in agent-based negotiation teams. For that
reason, in this article we review the tasks to be carried out by agent-based
negotiation teams. Each task is analyzed and related with current advances in
different research areas. The analysis aims to identify special challenges that
may arise due to the particularities of agent-based negotiation teams.
</summary>
    <author>
      <name>Victor Sanchez-Anguix</name>
    </author>
    <author>
      <name>Vicente Julian</name>
    </author>
    <author>
      <name>Vicente Botti</name>
    </author>
    <author>
      <name>Ana Garcia-Fornes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.engappai.2013.07.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.engappai.2013.07.006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Engineering Applications of Artificial Intelligence, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11, I.2.8, K.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04725v1</id>
    <updated>2016-04-16T11:01:44Z</updated>
    <published>2016-04-16T11:01:44Z</published>
    <title>Unanimously acceptable agreements for negotiation teams in unpredictable
  domains</title>
    <summary>  A negotiation team is a set of agents with common and possibly also
conflicting preferences that forms one of the parties of a negotiation. A
negotiation team is involved in two decision making processes simultaneously, a
negotiation with the opponents, and an intra-team process to decide on the
moves to make in the negotiation. This article focuses on negotiation team
decision making for circumstances that require unanimity of team decisions.
Existing agent-based approaches only guarantee unanimity in teams negotiating
in domains exclusively composed of predictable and compatible issues. This
article presents a model for negotiation teams that guarantees unanimous team
decisions in domains consisting of predictable and compatible, and also
unpredictable issues. Moreover, the article explores the influence of using
opponent, and team member models in the proposing strategies that team members
use. Experimental results show that the team benefits if team members employ
Bayesian learning to model their teammates' preferences.
</summary>
    <author>
      <name>Victor Sanchez-Anguix</name>
    </author>
    <author>
      <name>Reyhan Aydogan</name>
    </author>
    <author>
      <name>Vicente Julian</name>
    </author>
    <author>
      <name>Catholijn Jonker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.elerap.2014.05.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.elerap.2014.05.002" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Commerce Research and Applications, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11, I.2.8, K.4.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04721v1</id>
    <updated>2016-04-16T10:50:02Z</updated>
    <published>2016-04-16T10:50:02Z</published>
    <title>An artificial intelligence tool for heterogeneous team formation in the
  classroom</title>
    <summary>  Nowadays, there is increasing interest in the development of teamwork skills
in the educational context. This growing interest is motivated by its
pedagogical effectiveness and the fact that, in labour contexts, enterprises
organize their employees in teams to carry out complex projects. Despite its
crucial importance in the classroom and industry, there is a lack of support
for the team formation process. Not only do many factors influence team
performance, but the problem becomes exponentially costly if teams are to be
optimized. In this article, we propose a tool whose aim it is to cover such a
gap. It combines artificial intelligence techniques such as coalition structure
generation, Bayesian learning, and Belbin's role theory to facilitate the
generation of working groups in an educational context. This tool improves
current state of the art proposals in three ways: i) it takes into account the
feedback of other teammates in order to establish the most predominant role of
a student instead of self-perception questionnaires; ii) it handles uncertainty
with regard to each student's predominant team role; iii) it is iterative since
it considers information from several interactions in order to improve the
estimation of role assignments. We tested the performance of the proposed tool
in an experiment involving students that took part in three different team
activities. The experiments suggest that the proposed tool is able to improve
different teamwork aspects such as team dynamics and student satisfaction.
</summary>
    <author>
      <name>Juan M. Alberola</name>
    </author>
    <author>
      <name>Elena Del Val</name>
    </author>
    <author>
      <name>Victor Sanchez-Anguix</name>
    </author>
    <author>
      <name>Alberto Palomares</name>
    </author>
    <author>
      <name>Maria Dolores Teruel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.knosys.2016.02.010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.knosys.2016.02.010" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Knowledge-Based Systems, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.04721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; K.3.1; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04660v2</id>
    <updated>2016-05-12T17:16:25Z</updated>
    <published>2016-04-15T23:36:44Z</published>
    <title>Why Artificial Intelligence Needs a Task Theory --- And What It Might
  Look Like</title>
    <summary>  The concept of "task" is at the core of artificial intelligence (AI): Tasks
are used for training and evaluating AI systems, which are built in order to
perform and automatize tasks we deem useful. In other fields of engineering
theoretical foundations allow thorough evaluation of designs by methodical
manipulation of well understood parameters with a known role and importance;
this allows an aeronautics engineer, for instance, to systematically assess the
effects of wind speed on an airplane's performance and stability. No framework
exists in AI that allows this kind of methodical manipulation: Performance
results on the few tasks in current use (cf. board games, question-answering)
cannot be easily compared, however similar or different. The issue is even more
acute with respect to artificial *general* intelligence systems, which must
handle unanticipated tasks whose specifics cannot be known beforehand. A *task
theory* would enable addressing tasks at the *class* level, bypassing their
specifics, providing the appropriate formalization and classification of tasks,
environments, and their parameters, resulting in more rigorous ways of
measuring, comparing, and evaluating intelligent behavior. Even modest
improvements in this direction would surpass the current ad-hoc nature of
machine learning and AI evaluation. Here we discuss the main elements of the
argument for a task theory and present an outline of what it might look like
for physical tasks.
</summary>
    <author>
      <name>Kristinn R. Thórisson</name>
    </author>
    <author>
      <name>Jordi Bieger</name>
    </author>
    <author>
      <name>Thröstur Thorarensen</name>
    </author>
    <author>
      <name>Jóna S. Sigurðardóttir</name>
    </author>
    <author>
      <name>Bas R. Steunebrink</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to the Ninth Conference on Artificial General Intelligence
  (AGI-16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04660v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04660v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05557v3</id>
    <updated>2016-04-28T18:49:12Z</updated>
    <published>2016-04-15T19:39:54Z</published>
    <title>AGI and Reflexivity</title>
    <summary>  We define a property of intelligent systems, which we call Reflexivity. In
human beings, it is one aspect of consciousness, and an element of
deliberation. We propose a conjecture, that this property is conditioned by a
topological property of the processes which implement this reflexivity. These
processes may be symbolic, or non symbolic e.g. connexionnist. An architecture
which implements reflexivity may be based on the interaction of one or several
modules of deep learning, which may be specialized or not, and interconnected
in a relevant way. A necessary condition of reflexivity is the existence of
recurrence in its processes, we will examine in which cases this condition may
be sufficient. We will then examine how this topology and this property make
possible the expression of a second property, the deliberation. In a final
paragraph, we propose an evaluation of intelligent systems, based on the
fulfillment of all or some of these properties.
</summary>
    <author>
      <name>Pascal Faudemay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ECAI-2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05557v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05557v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04562v2</id>
    <updated>2016-05-20T14:03:58Z</updated>
    <published>2016-04-15T16:40:49Z</published>
    <title>A Network-based End-to-End Trainable Task-oriented Dialogue System</title>
    <summary>  Teaching machines to accomplish tasks by conversing naturally with humans is
challenging. Currently, developing task-oriented dialogue systems requires
creating multiple components and typically this involves either a large amount
of handcrafting, or acquiring labelled datasets and solving a statistical
learning problem for each component. In this work we introduce a neural
network-based text-in, text-out end-to-end trainable dialogue system along with
a new way of collecting task-oriented dialogue data based on a novel pipe-lined
Wizard-of-Oz framework. This approach allows us to develop dialogue systems
easily and without making too many assumptions about the task at hand. The
results show that the model can converse with human subjects naturally whilst
helping them to accomplish tasks in a restaurant search domain.
</summary>
    <author>
      <name>Tsung-Hsien Wen</name>
    </author>
    <author>
      <name>David Vandyke</name>
    </author>
    <author>
      <name>Nikola Mrksic</name>
    </author>
    <author>
      <name>Milica Gasic</name>
    </author>
    <author>
      <name>Lina M. Rojas-Barahona</name>
    </author>
    <author>
      <name>Pei-Hao Su</name>
    </author>
    <author>
      <name>Stefan Ultes</name>
    </author>
    <author>
      <name>Steve Young</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04562v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04562v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04558v1</id>
    <updated>2016-04-15T16:27:38Z</updated>
    <published>2016-04-15T16:27:38Z</published>
    <title>Accessing accurate documents by mining auxiliary document information</title>
    <summary>  Earlier techniques of text mining included algorithms like k-means, Naive
Bayes, SVM which classify and cluster the text document for mining relevant
information about the documents. The need for improving the mining techniques
has us searching for techniques using the available algorithms. This paper
proposes one technique which uses the auxiliary information that is present
inside the text documents to improve the mining. This auxiliary information can
be a description to the content. This information can be either useful or
completely useless for mining. The user should assess the worth of the
auxiliary information before considering this technique for text mining. In
this paper, a combination of classical clustering algorithms is used to mine
the datasets. The algorithm runs in two stages which carry out mining at
different levels of abstraction. The clustered documents would then be
classified based on the necessary groups. The proposed technique is aimed at
improved results of document clustering.
</summary>
    <author>
      <name>Jinju Joby</name>
    </author>
    <author>
      <name>Jyothi Korra</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04506v1</id>
    <updated>2016-04-15T13:52:12Z</updated>
    <published>2016-04-15T13:52:12Z</published>
    <title>Integrating Know-How into the Linked Data Cloud</title>
    <summary>  This paper presents the first framework for integrating procedural knowledge,
or "know-how", into the Linked Data Cloud. Know-how available on the Web, such
as step-by-step instructions, is largely unstructured and isolated from other
sources of online knowledge. To overcome these limitations, we propose
extending to procedural knowledge the benefits that Linked Data has already
brought to representing, retrieving and reusing declarative knowledge. We
describe a framework for representing generic know-how as Linked Data and for
automatically acquiring this representation from existing resources on the Web.
This system also allows the automatic generation of links between different
know-how resources, and between those resources and other online knowledge
bases, such as DBpedia. We discuss the results of applying this framework to a
real-world scenario and we show how it outperforms existing manual
community-driven integration efforts.
</summary>
    <author>
      <name>Paolo Pareti</name>
    </author>
    <author>
      <name>Benoit Testu</name>
    </author>
    <author>
      <name>Ryutaro Ichise</name>
    </author>
    <author>
      <name>Ewan Klein</name>
    </author>
    <author>
      <name>Adam Barker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-13704-9_30</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-13704-9_30" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 19th International Conference on Knowledge Engineering and
  Knowledge Management (EKAW 2014), 24-28 November 2014, Link\"oping, Sweden</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Knowledge Engineering and Knowledge Management, volume 8876 of
  Lecture Notes in Computer Science, pages 385-396. Springer International
  Publishing (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.04506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05194v1</id>
    <updated>2016-04-15T08:47:18Z</updated>
    <published>2016-04-15T08:47:18Z</published>
    <title>Preference Elicitation For Single Crossing Domain</title>
    <summary>  Eliciting the preferences of a set of agents over a set of alternatives is a
problem of fundamental importance in social choice theory. Prior work on this
problem has studied the query complexity of preference elicitation for the
unrestricted domain and for the domain of single peaked preferences. In this
paper, we consider the domain of single crossing preference profiles and study
the query complexity of preference elicitation under various settings. We
consider two distinct situations: when an ordering of the voters with respect
to which the profile is single crossing is known versus when it is unknown. We
also consider different access models: when the votes can be accessed at
random, as opposed to when they are coming in a pre-defined sequence. In the
sequential access model, we distinguish two cases when the ordering is known:
the first is that sequence in which the votes appear is also a single-crossing
order, versus when it is not.
  The main contribution of our work is to provide polynomial time algorithms
with low query complexity for preference elicitation in all the above six
cases. Further, we show that the query complexities of our algorithms are
optimal up to constant factors for all but one of the above six cases. We then
present preference elicitation algorithms for profiles which are close to being
single crossing under various notions of closeness, for example, single
crossing width, minimum number of candidates | voters whose deletion makes a
profile single crossing.
</summary>
    <author>
      <name>Palash Dey</name>
    </author>
    <author>
      <name>Neeldhara Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04403v1</id>
    <updated>2016-04-15T08:40:39Z</updated>
    <published>2016-04-15T08:40:39Z</published>
    <title>Elicitation for Preferences Single Peaked on Trees</title>
    <summary>  In multiagent systems, we often have a set of agents each of which have a
preference ordering over a set of items and one would like to know these
preference orderings for various tasks, for example, data analysis, preference
aggregation, voting etc. However, we often have a large number of items which
makes it impractical to ask the agents for their complete preference ordering.
In such scenarios, we usually elicit these agents' preferences by asking (a
hopefully small number of) comparison queries --- asking an agent to compare
two items. Prior works on preference elicitation focus on unrestricted domain
and the domain of single peaked preferences and show that the preferences in
single peaked domain can be elicited by much less number of queries compared to
unrestricted domain. We extend this line of research and study preference
elicitation for single peaked preferences on trees which is a strict superset
of the domain of single peaked preferences. We show that the query complexity
crucially depends on the number of leaves, the path cover number, and the
distance from path of the underlying single peaked tree, whereas the other
natural parameters like maximum degree, diameter, pathwidth do not play any
direct role in determining query complexity. We then investigate the query
complexity for finding a weak Condorcet winner for preferences single peaked on
a tree and show that this task has much less query complexity than preference
elicitation. Here again we observe that the number of leaves in the underlying
single peaked tree and the path cover number of the tree influence the query
complexity of the problem.
</summary>
    <author>
      <name>Palash Dey</name>
    </author>
    <author>
      <name>Neeldhara Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IJCAI 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04378v1</id>
    <updated>2016-04-15T07:23:53Z</updated>
    <published>2016-04-15T07:23:53Z</published>
    <title>Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN</title>
    <summary>  Semantic matching, which aims to determine the matching degree between two
texts, is a fundamental problem for many NLP applications. Recently, deep
learning approach has been applied to this problem and significant improvements
have been achieved. In this paper, we propose to view the generation of the
global interaction between two texts as a recursive process: i.e. the
interaction of two texts at each position is a composition of the interactions
between their prefixes as well as the word level interaction at the current
position. Based on this idea, we propose a novel deep architecture, namely
Match-SRNN, to model the recursive matching structure. Firstly, a tensor is
constructed to capture the word level interactions. Then a spatial RNN is
applied to integrate the local interactions recursively, with importance
determined by four types of gates. Finally, the matching score is calculated
based on the global interaction. We show that, after degenerated to the exact
matching scenario, Match-SRNN can approximate the dynamic programming process
of longest common subsequence. Thus, there exists a clear interpretation for
Match-SRNN. Our experiments on two semantic matching tasks showed the
effectiveness of Match-SRNN, and its ability of visualizing the learned
matching structure.
</summary>
    <author>
      <name>Shengxian Wan</name>
    </author>
    <author>
      <name>Yanyan Lan</name>
    </author>
    <author>
      <name>Jun Xu</name>
    </author>
    <author>
      <name>Jiafeng Guo</name>
    </author>
    <author>
      <name>Liang Pang</name>
    </author>
    <author>
      <name>Xueqi Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCAI-2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04359v1</id>
    <updated>2016-04-15T05:55:07Z</updated>
    <published>2016-04-15T05:55:07Z</published>
    <title>Complexity of Manipulation with Partial Information in Voting</title>
    <summary>  The Coalitional Manipulation problem has been studied extensively in the
literature for many voting rules. However, most studies have focused on the
complete information setting, wherein the manipulators know the votes of the
non-manipulators. While this assumption is reasonable for purposes of showing
intractability, it is unrealistic for algorithmic considerations. In most
real-world scenarios, it is impractical for the manipulators to have accurate
knowledge of all the other votes. In this paper, we investigate manipulation
with incomplete information. In our framework, the manipulators know a partial
order for each voter that is consistent with the true preference of that voter.
In this setting, we formulate three natural computational notions of
manipulation, namely weak, opportunistic, and strong manipulation. We say that
an extension of a partial order is if there exists a manipulative vote for that
extension.
  1. Weak Manipulation (WM): the manipulators seek to vote in a way that makes
their preferred candidate win in at least one extension of the partial votes of
the non-manipulators.
  2. Opportunistic Manipulation (OM): the manipulators seek to vote in a way
that makes their preferred candidate win in every viable extension of the
partial votes of the non-manipulators.
  3. Strong Manipulation (SM): the manipulators seek to vote in a way that
makes their preferred candidate win in every extension of the partial votes of
the non-manipulators.
  We consider several scenarios for which the traditional manipulation problems
are easy (for instance, Borda with a single manipulator). For many of them, the
corresponding manipulative questions that we propose turn out to be
computationally intractable. Our hardness results often hold even when very
little information is missing, or in other words, even when the instances are
quite close to the complete information setting.
</summary>
    <author>
      <name>Palash Dey</name>
    </author>
    <author>
      <name>Neeldhara Misra</name>
    </author>
    <author>
      <name>Y. Narahari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To Appear in IJCAI 2016. arXiv admin note: substantial text overlap
  with arXiv:1504.08256</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04358v1</id>
    <updated>2016-04-15T05:51:29Z</updated>
    <published>2016-04-15T05:51:29Z</published>
    <title>StalemateBreaker: A Proactive Content-Introducing Approach to Automatic
  Human-Computer Conversation</title>
    <summary>  Existing open-domain human-computer conversation systems are typically
passive: they either synthesize or retrieve a reply provided a human-issued
utterance. It is generally presumed that humans should take the role to lead
the conversation and introduce new content when a stalemate occurs, and that
the computer only needs to "respond." In this paper, we propose
StalemateBreaker, a conversation system that can proactively introduce new
content when appropriate. We design a pipeline to determine when, what, and how
to introduce new content during human-computer conversation. We further propose
a novel reranking algorithm Bi-PageRank-HITS to enable rich interaction between
conversation context and candidate replies. Experiments show that both the
content-introducing approach and the reranking algorithm are effective. Our
full StalemateBreaker model outperforms a state-of-the-practice conversation
system by +14.4% p@1 when a stalemate occurs.
</summary>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Lili Mou</name>
    </author>
    <author>
      <name>Rui Yan</name>
    </author>
    <author>
      <name>Ming Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCAI-16</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04315v2</id>
    <updated>2016-05-17T18:47:00Z</updated>
    <published>2016-04-14T22:43:30Z</published>
    <title>Moving Beyond the Turing Test with the Allen AI Science Challenge</title>
    <summary>  Given recent successes in AI (e.g., AlphaGo's victory against Lee Sedol in
the game of GO), it's become increasingly important to assess: how close are AI
systems to human-level intelligence? This paper describes the Allen AI Science
Challenge---an approach towards that goal which led to a unique Kaggle
Competition, its results, the lessons learned, and our next steps.
</summary>
    <author>
      <name>Carissa Schoenick</name>
    </author>
    <author>
      <name>Peter Clark</name>
    </author>
    <author>
      <name>Oyvind Tafjord</name>
    </author>
    <author>
      <name>Peter Turney</name>
    </author>
    <author>
      <name>Oren Etzioni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04315v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04315v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04169v1</id>
    <updated>2016-04-14T14:39:56Z</updated>
    <published>2016-04-14T14:39:56Z</published>
    <title>A Deterministic Annealing Approach to the Multiple Traveling Salesmen
  and Related Problems</title>
    <summary>  This paper presents a novel and efficient heuristic framework for
approximating the solutions to the multiple traveling salesmen problem (m-TSP)
and other variants on the TSP. The approach adopted in this paper is an
extension of the Maximum-Entropy-Principle (MEP) and the Deterministic
Annealing (DA) algorithm. The framework is presented as a general tool that can
be suitably adapted to a number of variants on the basic TSP. Additionally,
unlike most other heuristics for the TSP, the framework presented in this paper
is independent of the edges defined between any two pairs of nodes. This makes
the algorithm particularly suited for variants such as the close-enough
traveling salesman problem (CETSP) which are challenging due to added
computational complexity. The examples presented in this paper illustrate the
effectiveness of this new framework for use in TSP and many variants thereof.
</summary>
    <author>
      <name>Mayank Baranwal</name>
    </author>
    <author>
      <name>Brian Roehl</name>
    </author>
    <author>
      <name>Srinivasa M. Salapaka</name>
    </author>
    <link href="http://arxiv.org/abs/1604.04169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04146v1</id>
    <updated>2016-04-14T13:25:42Z</updated>
    <published>2016-04-14T13:25:42Z</published>
    <title>A Discrete Firefly Algorithm to Solve a Rich Vehicle Routing Problem
  Modelling a Newspaper Distribution System with Recycling Policy</title>
    <summary>  A real-world newspaper distribution problem with recycling policy is tackled
in this work. In order to meet all the complex restrictions contained in such a
problem, it has been modeled as a rich vehicle routing problem, which can be
more specifically considered as an asymmetric and clustered vehicle routing
problem with simultaneous pickup and deliveries, variable costs and forbidden
paths (AC-VRP-SPDVCFP). This is the first study of such a problem in the
literature. For this reason, a benchmark composed by 15 instances has been also
proposed. In the design of this benchmark, real geographical positions have
been used, located in the province of Bizkaia, Spain. For the proper treatment
of this AC-VRP-SPDVCFP, a discrete firefly algorithm (DFA) has been developed.
This application is the first application of the firefly algorithm to any rich
vehicle routing problem. To prove that the proposed DFA is a promising
technique, its performance has been compared with two other well-known
techniques: an evolutionary algorithm and an evolutionary simulated annealing.
Our results have shown that the DFA has outperformed these two classic
meta-heuristics.
</summary>
    <author>
      <name>E. Osaba</name>
    </author>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <author>
      <name>F. Diaz</name>
    </author>
    <author>
      <name>E. Onieva</name>
    </author>
    <author>
      <name>A. D. Masegosa</name>
    </author>
    <author>
      <name>A. Perallos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00500-016-2114-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00500-016-2114-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 tables and 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="78M32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04138v1</id>
    <updated>2016-04-14T12:52:20Z</updated>
    <published>2016-04-14T12:52:20Z</published>
    <title>An Improved Discrete Bat Algorithm for Symmetric and Asymmetric
  Traveling Salesman Problems</title>
    <summary>  Bat algorithm is a population metaheuristic proposed in 2010 which is based
on the echolocation or bio-sonar characteristics of microbats. Since its first
implementation, the bat algorithm has been used in a wide range of fields. In
this paper, we present a discrete version of the bat algorithm to solve the
well-known symmetric and asymmetric traveling salesman problems. In addition,
we propose an improvement in the basic structure of the classic bat algorithm.
To prove that our proposal is a promising approximation method, we have
compared its performance in 37 instances with the results obtained by five
different techniques: evolutionary simulated annealing, genetic algorithm, an
island based distributed genetic algorithm, a discrete firefly algorithm and an
imperialist competitive algorithm. In order to obtain fair and rigorous
comparisons, we have conducted three different statistical tests along the
paper: the Student's $t$-test, the Holm's test, and the Friedman test. We have
also compared the convergence behaviour shown by our proposal with the ones
shown by the evolutionary simulated annealing, and the discrete firefly
algorithm. The experimentation carried out in this study has shown that the
presented improved bat algorithm outperforms significantly all the other
alternatives in most of the cases.
</summary>
    <author>
      <name>Eneko Osaba</name>
    </author>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <author>
      <name>Fernando Diaz</name>
    </author>
    <author>
      <name>Pedro Lopez-Garcia</name>
    </author>
    <author>
      <name>Roberto Carballedo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.engappai.2015.10.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.engappai.2015.10.006" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 figure, 8 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Engineering Applications of Artificial Intelligence, 48 (1), 59-71
  (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.04138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="78M32" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04324v1</id>
    <updated>2016-04-14T12:40:50Z</updated>
    <published>2016-04-14T12:40:50Z</published>
    <title>Random-Key Cuckoo Search for the Travelling Salesman Problem</title>
    <summary>  Combinatorial optimization problems are typically NP-hard, and thus very
challenging to solve. In this paper, we present the random key cuckoo search
(RKCS) algorithm for solving the famous Travelling Salesman Problem (TSP). We
used a simplified random-key encoding scheme to pass from a continuous space
(real numbers) to a combinatorial space. We also consider the displacement of a
solution in both spaces using Levy flights. The performance of the proposed
RKCS is tested against a set of benchmarks of symmetric TSP from the well-known
TSPLIB library. The results of the tests show that RKCS is superior to some
other metaheuristic algorithms.
</summary>
    <author>
      <name>Aziz Ouaarab</name>
    </author>
    <author>
      <name>B. Ahiod</name>
    </author>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00500-014-1322-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00500-014-1322-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Soft Computing, 19(4), 1099-1106(2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.04324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04096v1</id>
    <updated>2016-04-14T10:13:43Z</updated>
    <published>2016-04-14T10:13:43Z</published>
    <title>A General Framework for Describing Creative Agents</title>
    <summary>  Computational creativity is a subfield of AI focused on developing and
studying creative systems. Few academic studies analysing the behaviour of
creative agents from a theoretical viewpoint have been proposed. The proposed
frameworks are vague and hard to exploit; moreover, such works are focused on a
notion of creativity tailored for humans.
  In this paper we introduce General Creativity, which extends that traditional
notion. General Creativity provides the basis for a formalised theoretical
framework, that allows one to univocally describe any creative agent, and their
behaviour within societies of creative systems. Given the growing number of AI
creative systems developed over recent years, it is of fundamental importance
to understand how they could influence each other as well as how to gauge their
impact on human society. In particular, in this paper we exploit the proposed
framework for (i) identifying different forms of creativity; (ii) describing
some typical creative agents behaviour, and (iii) analysing the dynamics of
societies in which both human and non-human creative systems coexist.
</summary>
    <author>
      <name>Valerio Velardo</name>
    </author>
    <author>
      <name>Mauro Vallati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03968v1</id>
    <updated>2016-04-13T20:27:43Z</updated>
    <published>2016-04-13T20:27:43Z</published>
    <title>Visual Storytelling</title>
    <summary>  We introduce the first dataset for sequential vision-to-language, and explore
how this data may be used for the task of visual storytelling. The first
release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211
sequences, aligned to both descriptive (caption) and story language. We
establish several strong baselines for the storytelling task, and motivate an
automatic metric to benchmark progress. Modelling concrete description as well
as figurative and social language, as provided in this dataset and the
storytelling task, has the potential to move artificial intelligence from basic
understandings of typical visual scenes towards more and more human-like
understanding of grounded event structure and subjective expression.
</summary>
    <author>
      <name> Ting-Hao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kenneth</arxiv:affiliation>
    </author>
    <author>
      <name> Huang</name>
    </author>
    <author>
      <name>Francis Ferraro</name>
    </author>
    <author>
      <name>Nasrin Mostafazadeh</name>
    </author>
    <author>
      <name>Ishan Misra</name>
    </author>
    <author>
      <name>Aishwarya Agrawal</name>
    </author>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Ross Girshick</name>
    </author>
    <author>
      <name>Xiaodong He</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>Dhruv Batra</name>
    </author>
    <author>
      <name>C. Lawrence Zitnick</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <author>
      <name>Lucy Vanderwende</name>
    </author>
    <author>
      <name>Michel Galley</name>
    </author>
    <author>
      <name>Margaret Mitchell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in NAACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03912v1</id>
    <updated>2016-04-13T19:06:41Z</updated>
    <published>2016-04-13T19:06:41Z</published>
    <title>Inverse Reinforcement Learning with Simultaneous Estimation of Rewards
  and Dynamics</title>
    <summary>  Inverse Reinforcement Learning (IRL) describes the problem of learning an
unknown reward function of a Markov Decision Process (MDP) from observed
behavior of an agent. Since the agent's behavior originates in its policy and
MDP policies depend on both the stochastic system dynamics as well as the
reward function, the solution of the inverse problem is significantly
influenced by both. Current IRL approaches assume that if the transition model
is unknown, additional samples from the system's dynamics are accessible, or
the observed behavior provides enough samples of the system's dynamics to solve
the inverse problem accurately. These assumptions are often not satisfied. To
overcome this, we present a gradient-based IRL approach that simultaneously
estimates the system's dynamics. By solving the combined optimization problem,
our approach takes into account the bias of the demonstrations, which stems
from the generating policy. The evaluation on a synthetic MDP and a transfer
learning task shows improvements regarding the sample efficiency as well as the
accuracy of the estimated reward functions and transition models.
</summary>
    <author>
      <name>Michael Herman</name>
    </author>
    <author>
      <name>Tobias Gindele</name>
    </author>
    <author>
      <name>Jörg Wagner</name>
    </author>
    <author>
      <name>Felix Schmitt</name>
    </author>
    <author>
      <name>Wolfram Burgard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to appear in AISTATS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03901v1</id>
    <updated>2016-04-13T18:19:35Z</updated>
    <published>2016-04-13T18:19:35Z</published>
    <title>Single-Image Depth Perception in the Wild</title>
    <summary>  This paper studies single-image depth perception in the wild, i.e.,
recovering depth from a single image taken in unconstrained settings. We
introduce a new dataset "Depth in the Wild" consisting of images in the wild
annotated with relative depth between pairs of random points. We also propose a
new algorithm that learns to estimate metric depth using annotations of
relative depth. Compared to the state of the art, our algorithm is simpler and
performs better. Experiments show that our algorithm, combined with existing
RGB-D data and our new relative depth annotations, significantly improves
single-image depth perception in the wild.
</summary>
    <author>
      <name>Weifeng Chen</name>
    </author>
    <author>
      <name>Zhao Fu</name>
    </author>
    <author>
      <name>Dawei Yang</name>
    </author>
    <author>
      <name>Jia Deng</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03853v2</id>
    <updated>2016-05-26T11:09:19Z</updated>
    <published>2016-04-13T16:12:01Z</published>
    <title>Hierarchical Compound Poisson Factorization</title>
    <summary>  Non-negative matrix factorization models based on a hierarchical
Gamma-Poisson structure capture user and item behavior effectively in extremely
sparse data sets, making them the ideal choice for collaborative filtering
applications. Hierarchical Poisson factorization (HPF) in particular has proved
successful for scalable recommendation systems with extreme sparsity. HPF,
however, suffers from a tight coupling of sparsity model (absence of a rating)
and response model (the value of the rating), which limits the expressiveness
of the latter. Here, we introduce hierarchical compound Poisson factorization
(HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to
high-dimensional extremely sparse matrices. More importantly, HCPF decouples
the sparsity model from the response model, allowing us to choose the most
suitable distribution for the response. HCPF can capture binary, non-negative
discrete, non-negative continuous, and zero-inflated continuous responses. We
compare HCPF with HPF on nine discrete and three continuous data sets and
conclude that HCPF captures the relationship between sparsity and response
better than HPF.
</summary>
    <author>
      <name>Mehmet E. Basbug</name>
    </author>
    <author>
      <name>Barbara E. Engelhardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Will appear on Proceedings of the 33 rd International Conference on
  Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03853v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03853v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03793v1</id>
    <updated>2016-04-13T14:21:34Z</updated>
    <published>2016-04-13T14:21:34Z</published>
    <title>HordeQBF: A Modular and Massively Parallel QBF Solver</title>
    <summary>  The recently developed massively parallel satisfiability (SAT) solver
HordeSAT was designed in a modular way to allow the integration of any
sequential CDCL-based SAT solver in its core. We integrated the QCDCL-based
quantified Boolean formula (QBF) solver DepQBF in HordeSAT to obtain a
massively parallel QBF solver---HordeQBF. In this paper we describe the details
of this integration and report on results of the experimental evaluation of
HordeQBF's performance. HordeQBF achieves superlinear average and median
speedup on the hard application instances of the 2014 QBF Gallery.
</summary>
    <author>
      <name>Tomas Balyo</name>
    </author>
    <author>
      <name>Florian Lonsing</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-40970-2_33</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-40970-2_33" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">camera-ready version, 6-page tool paper, to appear in the proceedings
  of SAT 2016, LNCS, Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03692v2</id>
    <updated>2016-04-20T21:02:02Z</updated>
    <published>2016-04-13T08:40:06Z</published>
    <title>Learning Social Affordance for Human-Robot Interaction</title>
    <summary>  In this paper, we present an approach for robot learning of social affordance
from human activity videos. We consider the problem in the context of
human-robot interaction: Our approach learns structural representations of
human-human (and human-object-human) interactions, describing how body-parts of
each agent move with respect to each other and what spatial relations they
should maintain to complete each sub-event (i.e., sub-goal). This enables the
robot to infer its own movement in reaction to the human body motion, allowing
it to naturally replicate such interactions.
  We introduce the representation of social affordance and propose a generative
model for its weakly supervised learning from human demonstration videos. Our
approach discovers critical steps (i.e., latent sub-events) in an interaction
and the typical motion associated with them, learning what body-parts should be
involved and how. The experimental results demonstrate that our Markov Chain
Monte Carlo (MCMC) based learning algorithm automatically discovers
semantically meaningful interactive affordance from RGB-D videos, which allows
us to generate appropriate full body motion for an agent.
</summary>
    <author>
      <name>Tianmin Shu</name>
    </author>
    <author>
      <name>M. S. Ryoo</name>
    </author>
    <author>
      <name>Song-Chun Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Joint Conference on Artificial Intelligence (IJCAI),
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03692v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03692v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03655v9</id>
    <updated>2016-09-15T11:07:30Z</updated>
    <published>2016-04-13T05:06:29Z</published>
    <title>A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of
  Agents</title>
    <summary>  We consider the well-studied cake cutting problem in which the goal is to
find an envy-free allocation based on queries from $n$ agents. The problem has
received attention in computer science, mathematics, and economics. It has been
a major open problem whether there exists a discrete and bounded envy-free
protocol. We resolve the problem by proposing a discrete and bounded envy-free
protocol for any number of agents. The maximum number of queries required by
the protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do
not run our protocol to completion, it can find in at most $n^{n+1}$ queries a
partial allocation of the cake that achieves proportionality (each agent gets
at least $1/n$ of the value of the whole cake) and envy-freeness. Finally we
show that an envy-free partial allocation can be computed in $n^{n+1}$ queries
such that each agent gets a connected piece that gives the agent at least
$1/(3n)$ of the value of the whole cake.
</summary>
    <author>
      <name>Haris Aziz</name>
    </author>
    <author>
      <name>Simon Mackenzie</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03655v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03655v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A12, 68Q15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03632v1</id>
    <updated>2016-04-13T02:28:15Z</updated>
    <published>2016-04-13T02:28:15Z</published>
    <title>Strategyproof Peer Selection</title>
    <summary>  Peer review, evaluation, and selection is the foundation on which modern
science is built. Funding bodies the world over employ experts to study and
select the best proposals of those submitted for funding. The problem of peer
selection, however, is much more universal: a professional society may want
give a subset of its members awards based on the opinions of all the members;
an instructor for a MOOC or online course may want to crowdsource grading; or a
marketing company may select ideas from group brainstorming sessions based on
peer evaluation. We make three fundamental contributions to the study of
procedures or mechanisms for peer selection, a specific type of group decision
making problem studied in computer science, economics, political science, and
beyond. First, we detail a novel mechanism that is strategyproof, i.e., agents
cannot benefit themselves by reporting insincere valuations, in addition to
other desirable normative properties. Second, we demonstrate the effectiveness
of our mechanism through a comprehensive simulation based comparison of our
mechanism with a suite of mechanisms found in the computer science and
economics literature. Finally, our mechanism employs a randomized rounding
technique that is of independent interest, as it can be used as a randomized
method to addresses the ubiquitous apportionment problem that arises in various
settings where discrete resources such as parliamentary representation slots
need to be divided fairly.
</summary>
    <author>
      <name>Haris Aziz</name>
    </author>
    <author>
      <name>Omer Lev</name>
    </author>
    <author>
      <name>Nicholas Mattei</name>
    </author>
    <author>
      <name>Jeffrey S. Rosenschein</name>
    </author>
    <author>
      <name>Toby Walsh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, 1 Figure, Source Code available at
  https://github.com/nmattei/peerselection</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A80, 91B10, 91B12, 91B14, 68Q25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; F.2; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03526v1</id>
    <updated>2016-04-12T19:00:48Z</updated>
    <published>2016-04-12T19:00:48Z</published>
    <title>Spatiotemporal Articulated Models for Dynamic SLAM</title>
    <summary>  We propose an online spatiotemporal articulation model estimation framework
that estimates both articulated structure as well as a temporal prediction
model solely using passive observations. The resulting model can predict future
mo- tions of an articulated object with high confidence because of the spatial
and temporal structure. We demonstrate the effectiveness of the predictive
model by incorporating it within a standard simultaneous localization and
mapping (SLAM) pipeline for mapping and robot localization in previously
unexplored dynamic environments. Our method is able to localize the robot and
map a dynamic scene by explaining the observed motion in the world. We
demonstrate the effectiveness of the proposed framework for both simulated and
real-world dynamic environments.
</summary>
    <author>
      <name>Suren Kumar</name>
    </author>
    <author>
      <name>Vikas Dhiman</name>
    </author>
    <author>
      <name>Madan Ravi Ganesh</name>
    </author>
    <author>
      <name>Jason J. Corso</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03468v1</id>
    <updated>2016-04-12T16:22:29Z</updated>
    <published>2016-04-12T16:22:29Z</published>
    <title>Backward-Forward Search for Manipulation Planning</title>
    <summary>  In this paper we address planning problems in high-dimensional hybrid
configuration spaces, with a particular focus on manipulation planning problems
involving many objects. We present the hybrid backward-forward (HBF) planning
algorithm that uses a backward identification of constraints to direct the
sampling of the infinite action space in a forward search from the initial
state towards a goal configuration. The resulting planner is probabilistically
complete and can effectively construct long manipulation plans requiring both
prehensile and nonprehensile actions in cluttered environments.
</summary>
    <author>
      <name>Caelan Reed Garrett</name>
    </author>
    <author>
      <name>Tomas Lozano-Perez</name>
    </author>
    <author>
      <name>Leslie Pack Kaelbling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages in IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS), 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03458v1</id>
    <updated>2016-04-12T15:53:25Z</updated>
    <published>2016-04-12T15:53:25Z</published>
    <title>Resource Allocation with Population Dynamics</title>
    <summary>  Many analyses of resource-allocation problems employ simplistic models of the
population. Using the example of a resource-allocation problem of Marecek et
al. [arXiv:1406.7639], we introduce rather a general behavioural model, where
the evolution of a heterogeneous population of agents is governed by a Markov
chain. Still, we are able to show that the distribution of agents across
resources converges in distribution, for suitable means of information
provision, under certain assumptions. The model and proof techniques may have
wider applicability.
</summary>
    <author>
      <name>Jonathan Epperlein</name>
    </author>
    <author>
      <name>Jakub Marecek</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03318v1</id>
    <updated>2016-04-12T09:27:00Z</updated>
    <published>2016-04-12T09:27:00Z</published>
    <title>Applying Ontological Modeling on Quranic Nature Domain</title>
    <summary>  The holy Quran is the holy book of the Muslims. It contains information about
many domains. Often people search for particular concepts of holy Quran based
on the relations among concepts. An ontological modeling of holy Quran can be
useful in such a scenario. In this paper, we have modeled nature related
concepts of holy Quran using OWL (Web Ontology Language) / RDF (Resource
Description Framework). Our methodology involves identifying nature related
concepts mentioned in holy Quran and identifying relations among those
concepts. These concepts and relations are represented as classes/instances and
properties of an OWL ontology. Later, in the result section it is shown that,
using the Ontological model, SPARQL queries can retrieve verses and concepts of
interest. Thus, this modeling helps semantic search and query on the holy
Quran. In this work, we have used English translation of the holy Quran by
Sahih International, Protege OWL Editor and for querying we have used SPARQL.
</summary>
    <author>
      <name>A. B. M. Shamsuzzaman Sadi</name>
    </author>
    <author>
      <name>Towfique Anam</name>
    </author>
    <author>
      <name>Mohamed Abdirazak</name>
    </author>
    <author>
      <name>Abdillahi Hasan Adnan</name>
    </author>
    <author>
      <name>Sazid Zaman Khan</name>
    </author>
    <author>
      <name>Mohamed Mahmudur Rahman</name>
    </author>
    <author>
      <name>Ghassan Samara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2016 7th International Conference on Information and Communication
  Systems (ICICS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03303v1</id>
    <updated>2016-04-12T08:45:51Z</updated>
    <published>2016-04-12T08:45:51Z</published>
    <title>Optimal Route Planning with Prioritized Task Scheduling for AUV Missions</title>
    <summary>  This paper presents a solution to Autonomous Underwater Vehicles (AUVs) large
scale route planning and task assignment joint problem. Given a set of
constraints (e.g., time) and a set of task priority values, the goal is to find
the optimal route for underwater mission that maximizes the sum of the
priorities and minimizes the total risk percentage while meeting the given
constraints. Making use of the heuristic nature of genetic and swarm
intelligence algorithms in solving NP-hard graph problems, Particle Swarm
Optimization (PSO) and Genetic Algorithm (GA) are employed to find the optimum
solution, where each individual in the population is a candidate solution
(route). To evaluate the robustness of the proposed methods, the performance of
the all PS and GA algorithms are examined and compared for a number of Monte
Carlo runs. Simulation results suggest that the routes generated by both
algorithms are feasible and reliable enough, and applicable for underwater
motion planning. However, the GA-based route planner produces superior results
comparing to the results obtained from the PSO based route planner.
</summary>
    <author>
      <name>S. Mahmoud Zadeh</name>
    </author>
    <author>
      <name>D. Powers</name>
    </author>
    <author>
      <name>K. Sammut</name>
    </author>
    <author>
      <name>A. Lammas</name>
    </author>
    <author>
      <name>A. M. Yazdani</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Symposium on Robotics and Intelligent Sensors,
  pp 7-15, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.03303v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03303v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03265v2</id>
    <updated>2016-04-29T06:21:09Z</updated>
    <published>2016-04-12T07:10:43Z</published>
    <title>Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
    <summary>  3D shape models are becoming widely available and easier to capture, making
available 3D information crucial for progress in object classification. Current
state-of-the-art methods rely on CNNs to address this problem. Recently, we
witness two types of CNNs being developed: CNNs based upon volumetric
representations versus CNNs based upon multi-view representations. Empirical
results from these two types of CNNs exhibit a large gap, indicating that
existing volumetric CNN architectures and approaches are unable to fully
exploit the power of 3D representations. In this paper, we aim to improve both
volumetric CNNs and multi-view CNNs according to extensive analysis of existing
approaches. To this end, we introduce two distinct network architectures of
volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce
multi-resolution filtering in 3D. Overall, we are able to outperform current
state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We
provide extensive experiments designed to evaluate underlying design choices,
thus providing a better understanding of the space of methods available for
object classification on 3D data.
</summary>
    <author>
      <name>Charles R. Qi</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Matthias Niessner</name>
    </author>
    <author>
      <name>Angela Dai</name>
    </author>
    <author>
      <name>Mengyuan Yan</name>
    </author>
    <author>
      <name>Leonidas J. Guibas</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03265v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03265v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03243v1</id>
    <updated>2016-04-12T04:37:35Z</updated>
    <published>2016-04-12T04:37:35Z</published>
    <title>Finding Patterns is Almost Always Hard</title>
    <summary>  We study the complexity of the problem of searching for a set of patterns
that separate two given sets of strings. This problem has applications in a
wide variety of areas, most notably in data mining, computational biology, and
in understanding the complexity of genetic algorithms. We show that the basic
problem of finding a small set of patterns that match one set of strings but do
not match any string in a second set is difficult (NP-complete, W[2]-hard when
parameterized by the size of the pattern set, and APX-hard). We then perform a
detailed parameterized analysis of the problem, separating tractable and
intractable variants. In particular we show that parameterizing by the size of
pattern set and the number of strings, and the size of the alphabet and the
number of strings give FPT results, amongst others.
</summary>
    <author>
      <name>Giuseppe Lancia</name>
    </author>
    <author>
      <name>Luke Mathieson</name>
    </author>
    <author>
      <name>Pablo Moscato</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03200v1</id>
    <updated>2016-04-12T01:52:38Z</updated>
    <published>2016-04-12T01:52:38Z</published>
    <title>Efficient Classification of Multi-Labelled Text Streams by Clashing</title>
    <summary>  We present a method for the classification of multi-labelled text documents
explicitly designed for data stream applications that require to process a
virtually infinite sequence of data using constant memory and constant
processing time. Our method is composed of an online procedure used to
efficiently map text into a low-dimensional feature space and a partition of
this space into a set of regions for which the system extracts and keeps
statistics used to predict multi-label text annotations. Documents are fed into
the system as a sequence of words, mapped to a region of the partition, and
annotated using the statistics computed from the labelled instances colliding
in the same region. This approach is referred to as clashing. We illustrate the
method in real-world text data, comparing the results with those obtained using
other text classifiers. In addition, we provide an analysis about the effect of
the representation space dimensionality on the predictive performance of the
system. Our results show that the online embedding indeed approximates the
geometry of the full corpus-wise TF and TF-IDF space. The model obtains
competitive F measures with respect to the most accurate methods, using
significantly fewer computational resources. In addition, the method achieves a
higher macro-averaged F measure than methods with similar running time.
Furthermore, the system is able to learn faster than the other methods from
partially labelled streams.
</summary>
    <author>
      <name>Ricardo Ñanculef</name>
    </author>
    <author>
      <name>Ilias Flaounas</name>
    </author>
    <author>
      <name>Nello Cristianini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.eswa.2014.02.017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.eswa.2014.02.017" rel="related"/>
    <link href="http://arxiv.org/abs/1604.03200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03114v1</id>
    <updated>2016-04-11T20:00:04Z</updated>
    <published>2016-04-11T20:00:04Z</published>
    <title>Conversational flow in Oxford-style debates</title>
    <summary>  Public debates are a common platform for presenting and juxtaposing diverging
views on important issues. In this work we propose a methodology for tracking
how ideas flow between participants throughout a debate. We use this approach
in a case study of Oxford-style debates---a competitive format where the winner
is determined by audience votes---and show how the outcome of a debate depends
on aspects of conversational flow. In particular, we find that winners tend to
make better use of a debate's interactive component than losers, by actively
pursuing their opponents' points rather than promoting their own ideas over the
course of the conversation.
</summary>
    <author>
      <name>Justine Zhang</name>
    </author>
    <author>
      <name>Ravi Kumar</name>
    </author>
    <author>
      <name>Sujith Ravi</name>
    </author>
    <author>
      <name>Cristian Danescu-Niculescu-Mizil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at NAACL 2016. 5 pp, 1 fig. Data and other info available
  at http://www.cs.cornell.edu/~cristian/debates</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03099v1</id>
    <updated>2016-04-11T05:17:09Z</updated>
    <published>2016-04-11T05:17:09Z</published>
    <title>Symbolic Knowledge Extraction using Łukasiewicz Logics</title>
    <summary>  This work describes a methodology that combines logic-based systems and
connectionist systems. Our approach uses finite truth-valued {\L}ukasiewicz
logic, wherein every connective can be defined by a neuron in an artificial
network. This allowed the injection of first-order formulas into a network
architecture, and also simplified symbolic rule extraction. For that we trained
a neural networks using the Levenderg-Marquardt algorithm, where we restricted
the knowledge dissemination in the network structure. This procedure reduces
neural network plasticity without drastically damaging the learning
performance, thus making the descriptive power of produced neural networks
similar to the descriptive power of {\L}ukasiewicz logic language and
simplifying the translation between symbolic and connectionist structures. We
used this method for reverse engineering truth table and in extraction of
formulas from real data sets.
</summary>
    <author>
      <name>Carlos Leandro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. arXiv admin note: substantial text overlap with
  arXiv:1604.02780, arXiv:1604.02774</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.03099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03B52, 92B20, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02780v1</id>
    <updated>2016-04-11T03:23:21Z</updated>
    <published>2016-04-11T03:23:21Z</published>
    <title>Knowledge Extraction and Knowledge Integration governed by
  Łukasiewicz Logics</title>
    <summary>  The development of machine learning in particular and artificial intelligent
in general has been strongly conditioned by the lack of an appropriate
interface layer between deduction, abduction and induction. In this work we
extend traditional algebraic specification methods in this direction. Here we
assume that such interface for AI emerges from an adequate Neural-Symbolic
integration. This integration is made for universe of discourse described on a
Topos governed by a many-valued {\L}ukasiewicz logic. Sentences are integrated
in a symbolic knowledge base describing the problem domain, codified using a
graphic-based language, wherein every logic connective is defined by a neuron
in an artificial network. This allows the integration of first-order formulas
into a network architecture as background knowledge, and simplifies symbolic
rule extraction from trained networks. For the train of such neural networks we
changed the Levenderg-Marquardt algorithm, restricting the knowledge
dissemination in the network structure using soft crystallization. This
procedure reduces neural network plasticity without drastically damaging the
learning performance, allowing the emergence of symbolic patterns. This makes
the descriptive power of produced neural networks similar to the descriptive
power of {\L}ukasiewicz logic language, reducing the information lost on
translation between symbolic and connectionist structures. We tested this
method on the extraction of knowledge from specified structures. For it, we
present the notion of fuzzy state automata, and we use automata behaviour to
infer its structure. We use this type of automata on the generation of models
for relations specified as symbolic background knowledge.
</summary>
    <author>
      <name>Carlos Leandro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="03D05, 03B52, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02774v1</id>
    <updated>2016-04-11T02:05:21Z</updated>
    <published>2016-04-11T02:05:21Z</published>
    <title>Reverse Engineering and Symbolic Knowledge Extraction on Łukasiewicz
  Fuzzy Logics using Linear Neural Networks</title>
    <summary>  This work describes a methodology to combine logic-based systems and
connectionist systems. Our approach uses finite truth valued {\L}ukasiewicz
logic, where we take advantage of fact what in this type of logics every
connective can be define by a neuron in an artificial network having by
activation function the identity truncated to zero and one. This allowed the
injection of first-order formulas in a network architecture, and also
simplified symbolic rule extraction.
  Our method trains a neural network using Levenderg-Marquardt algorithm, where
we restrict the knowledge dissemination in the network structure. We show how
this reduces neural networks plasticity without damage drastically the learning
performance. Making the descriptive power of produced neural networks similar
to the descriptive power of {\L}ukasiewicz logic language, simplifying the
translation between symbolic and connectionist structures.
  This method is used in the reverse engineering problem of finding the formula
used on generation of a truth table for a multi-valued {\L}ukasiewicz logic.
For real data sets the method is particularly useful for attribute selection,
on binary classification problems defined using nominal attribute. After
attribute selection and possible data set completion in the resulting
connectionist model: neurons are directly representable using a disjunctive or
conjunctive formulas, in the {\L}ukasiewicz logic, or neurons are
interpretations which can be approximated by symbolic rules. This fact is
exemplified, extracting symbolic knowledge from connectionist models generated
for the data set Mushroom from UCI Machine Learning Repository.
</summary>
    <author>
      <name>Carlos Leandro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94D04" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02737v1</id>
    <updated>2016-04-10T21:21:00Z</updated>
    <published>2016-04-10T21:21:00Z</published>
    <title>Correlated Equilibria for Approximate Variational Inference in MRFs</title>
    <summary>  Almost all of the work in graphical models for game theory has mirrored
previous work in probabilistic graphical models. Our work considers the
opposite direction: Taking advantage of recent advances in equilibrium
computation for belief inference. In particular, we present formulations of
inference problems in Markov random fields (MRFs) as computation of equilibria
in a certain class of game-theoretic graphical models. While some previous work
explores this direction, none of that work concretely establishes the precise
connection between variational probabilistic inference in MRFs and correlated
equilibria. There is no work that exploits recent theoretical and empirical
results from the literature on algorithmic and computational game theory on the
tractable, polynomial-time computation of exact or approximate correlated
equilibria in graphical games with arbitrary, loopy graph structure. Our work
discusses how to design new algorithms with equally tractable guarantees for
the computation of approximate variational inference in MRFs. In addition,
inspired by a previously stated game-theoretic view of state-of-the-art
tree-reweighed (TRW) message-passing techniques for belief inference as
zero-sum game, we propose a different, general-sum potential game to design
approximate fictitious-play techniques. We perform synthetic experiments
evaluating our proposed approximation algorithms with standard methods and TRW
on several classes of classical Ising models. Our experiments show that our
global approach is competitive, particularly shinning in a class of Ising
models with constant, "highly attractive" edge-weights, in which it is often
better than all other alternatives we evaluated. While our local approach was
not as effective as our global approach or TRW, almost all of the alternatives
are often no better than a simple baseline: estimate the marginal probability
to be 0.5.
</summary>
    <author>
      <name>Luis E. Ortiz</name>
    </author>
    <author>
      <name>Ze Gong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 3 figures, 5 plots, Extension of Section of Section 4
  ("Probabilistic Inference and Equilibria") of an earlier manuscript by the
  first author entitled, "Correlated Equilibria and Probabilistic Inference in
  Graphical Models," first drafted on August 25, 2009.
  (http://www-personal.umd.umich.edu/~leortiz/papers/infeq.pdf)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02523v3</id>
    <updated>2016-06-15T23:01:23Z</updated>
    <published>2016-04-09T05:04:29Z</published>
    <title>Differential Evolution for Efficient AUV Path Planning in Time Variant
  Uncertain Underwater Environment</title>
    <summary>  The AUV three-dimension path planning in complex turbulent underwater
environment is investigated in this research, in which static current map data
and uncertain static-moving time variant obstacles are taken into account.
Robustness of AUVs path planning to this strong variability is known as a
complex NP-hard problem and is considered a critical issue to ensure vehicles
safe deployment. Efficient evolutionary techniques have substantial potential
of handling NP hard complexity of path planning problem as more powerful and
fast algorithms among other approaches for mentioned problem. For the purpose
of this research Differential Evolution (DE) technique is conducted to solve
the AUV path planning problem in a realistic underwater environment. The path
planners designed in this paper are capable of extracting feasible areas of a
real map to determine the allowed spaces for deployment, where coastal area,
islands, static/dynamic obstacles and ocean current is taken into account and
provides the efficient path with a small computation time. The results obtained
from analyze of experimental demonstrate the inherent robustness and drastic
efficiency of the proposed scheme in enhancement of the vehicles path planning
capability in coping undesired current, using useful current flow, and avoid
colliding collision boundaries in a real-time manner. The proposed approach is
also flexible and strictly respects to vehicle's kinematic constraints
resisting current instabilities.
</summary>
    <author>
      <name>S. Mahmoud Zadeh</name>
    </author>
    <author>
      <name>D. M. W. Powers</name>
    </author>
    <author>
      <name>K. Sammut</name>
    </author>
    <author>
      <name>A. Yazdani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages,6 figures, conference paper, 55th IEEE Conference on Decision
  and Control (CDC), USA, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02523v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02523v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02509v1</id>
    <updated>2016-04-09T01:57:13Z</updated>
    <published>2016-04-09T01:57:13Z</published>
    <title>Towards an Indexical Model of Situated Language Comprehension for
  Cognitive Agents in Physical Worlds</title>
    <summary>  We propose a computational model of situated language comprehension based on
the Indexical Hypothesis that generates meaning representations by translating
amodal linguistic symbols to modal representations of beliefs, knowledge, and
experience external to the linguistic system. This Indexical Model incorporates
multiple information sources, including perceptions, domain knowledge, and
short-term and long-term experiences during comprehension. We show that
exploiting diverse information sources can alleviate ambiguities that arise
from contextual use of underspecific referring expressions and unexpressed
argument alternations of verbs. The model is being used to support linguistic
interactions in Rosie, an agent implemented in Soar that learns from
instruction.
</summary>
    <author>
      <name>Shiwali Mohan</name>
    </author>
    <author>
      <name>Aaron Mininger</name>
    </author>
    <author>
      <name>John Laird</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Cognitive Systems 3 (2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02336v2</id>
    <updated>2016-05-21T18:26:21Z</updated>
    <published>2016-04-08T12:54:18Z</published>
    <title>Back to the Basics: Bayesian extensions of IRT outperform neural
  networks for proficiency estimation</title>
    <summary>  Estimating student proficiency is an important task for computer based
learning systems. We compare a family of IRT-based proficiency estimation
methods to Deep Knowledge Tracing (DKT), a recently proposed recurrent neural
network model with promising initial results. We evaluate how well each model
predicts a student's future response given previous responses using two
publicly available and one proprietary data set. We find that IRT-based methods
consistently matched or outperformed DKT across all data sets at the finest
level of content granularity that was tractable for them to be trained on. A
hierarchical extension of IRT that captured item grouping structure performed
best overall. When data sets included non-trivial autocorrelations in student
response patterns, a temporal extension of IRT improved performance over
standard IRT while the RNN-based method did not. We conclude that IRT-based
models provide a simpler, better-performing alternative to existing RNN-based
models of student interaction data while also affording more interpretability
and guarantees due to their formulation as Bayesian probabilistic models.
</summary>
    <author>
      <name>Kevin H. Wilson</name>
    </author>
    <author>
      <name>Yan Karklin</name>
    </author>
    <author>
      <name>Bojian Han</name>
    </author>
    <author>
      <name>Chaitanya Ekanadham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, Educational Data Mining 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02336v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02336v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02323v1</id>
    <updated>2016-04-08T12:12:17Z</updated>
    <published>2016-04-08T12:12:17Z</published>
    <title>A system of serial computation for classified rules prediction in
  non-regular ontology trees</title>
    <summary>  Objects or structures that are regular take uniform dimensions. Based on the
concepts of regular models, our previous research work has developed a system
of a regular ontology that models learning structures in a multiagent system
for uniform pre-assessments in a learning environment. This regular ontology
has led to the modelling of a classified rules learning algorithm that predicts
the actual number of rules needed for inductive learning processes and decision
making in a multiagent system. But not all processes or models are regular.
Thus this paper presents a system of polynomial equation that can estimate and
predict the required number of rules of a non-regular ontology model given some
defined parameters.
</summary>
    <author>
      <name>Kennedy E. Ehimwenma</name>
    </author>
    <author>
      <name>Paul Crowther</name>
    </author>
    <author>
      <name>Martin Beer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 15 figures, International Journal article, PhD research
  work</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence and Applications
  (IJAIA) March 2016, Vol 7(2), pp. 21-33</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.02323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02133v1</id>
    <updated>2016-04-07T19:41:35Z</updated>
    <published>2016-04-07T19:41:35Z</published>
    <title>Revising Incompletely Specified Convex Probabilistic Belief Bases</title>
    <summary>  We propose a method for an agent to revise its incomplete probabilistic
beliefs when a new piece of propositional information is observed. In this
work, an agent's beliefs are represented by a set of probabilistic formulae --
a belief base. The method involves determining a representative set of
'boundary' probability distributions consistent with the current belief base,
revising each of these probability distributions and then translating the
revised information into a new belief base. We use a version of Lewis Imaging
as the revision operation. The correctness of the approach is proved. The
expressivity of the belief bases under consideration are rather restricted, but
has some applications. We also discuss methods of belief base revision
employing the notion of optimum entropy, and point out some of the benefits and
difficulties in those methods. Both the boundary distribution method and the
optimum entropy method are reasonable, yet yield different results.
</summary>
    <author>
      <name>Gavin Rens</name>
    </author>
    <author>
      <name>Thomas Meyer</name>
    </author>
    <author>
      <name>Giovanni Casini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Sixteenth International Workshop on Non-Monotonic
  Reasoning, 22-24 April 2016, Cape Town, South Africa. 9.25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02126v1</id>
    <updated>2016-04-07T19:28:00Z</updated>
    <published>2016-04-07T19:28:00Z</published>
    <title>On Stochastic Belief Revision and Update and their Combination</title>
    <summary>  I propose a framework for an agent to change its probabilistic beliefs when a
new piece of propositional information $\alpha$ is observed. Traditionally,
belief change occurs by either a revision process or by an update process,
depending on whether the agent is informed with $\alpha$ in a static world or,
respectively, whether $\alpha$ is a 'signal' from the environment due to an
event occurring. Boutilier suggested a unified model of qualitative belief
change, which "combines aspects of revision and update, providing a more
realistic characterization of belief change." In this paper, I propose a
unified model of quantitative belief change, where an agent's beliefs are
represented as a probability distribution over possible worlds. As does
Boutilier, I take a dynamical systems perspective. The proposed approach is
evaluated against several rationality postulated, and some properties of the
approach are worked out.
</summary>
    <author>
      <name>Gavin Rens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Sixteenth International Workshop on Non-Monotonic
  Reasoning, 22-24 April 2016, Cape Town, South Africa. 10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02080v1</id>
    <updated>2016-04-07T17:12:07Z</updated>
    <published>2016-04-07T17:12:07Z</published>
    <title>Planning with Information-Processing Constraints and Model Uncertainty
  in Markov Decision Processes</title>
    <summary>  Information-theoretic principles for learning and acting have been proposed
to solve particular classes of Markov Decision Problems. Mathematically, such
approaches are governed by a variational free energy principle and allow
solving MDP planning problems with information-processing constraints expressed
in terms of a Kullback-Leibler divergence with respect to a reference
distribution. Here we consider a generalization of such MDP planners by taking
model uncertainty into account. As model uncertainty can also be formalized as
an information-processing constraint, we can derive a unified solution from a
single generalized variational principle. We provide a generalized value
iteration scheme together with a convergence proof. As limit cases, this
generalized scheme includes standard value iteration with a known model,
Bayesian MDP planning, and robust planning. We demonstrate the benefits of this
approach in a grid world simulation.
</summary>
    <author>
      <name>Jordi Grau-Moya</name>
    </author>
    <author>
      <name>Felix Leibfried</name>
    </author>
    <author>
      <name>Tim Genewein</name>
    </author>
    <author>
      <name>Daniel A. Braun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03210v1</id>
    <updated>2016-04-07T13:05:10Z</updated>
    <published>2016-04-07T13:05:10Z</published>
    <title>An Analysis of General Fuzzy Logic and Fuzzy Reasoning Method</title>
    <summary>  In this article, we describe the fuzzy logic, fuzzy language and algorithms
as the basis of fuzzy reasoning, one of the intelligent information processing
method, and then describe the general fuzzy reasoning method.
</summary>
    <author>
      <name>Kwak Son Il</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01802v2</id>
    <updated>2016-08-16T02:34:48Z</updated>
    <published>2016-04-06T20:39:34Z</published>
    <title>Learning to Track at 100 FPS with Deep Regression Networks</title>
    <summary>  Machine learning techniques are often used in computer vision due to their
ability to leverage large amounts of training data to improve performance.
Unfortunately, most generic object trackers are still trained from scratch
online and do not benefit from the large number of videos that are readily
available for offline training. We propose a method for offline training of
neural networks that can track novel objects at test-time at 100 fps. Our
tracker is significantly faster than previous methods that use neural networks
for tracking, which are typically very slow to run and not practical for
real-time applications. Our tracker uses a simple feed-forward network with no
online training required. The tracker learns a generic relationship between
object motion and appearance and can be used to track novel objects that do not
appear in the training set. We test our network on a standard tracking
benchmark to demonstrate our tracker's state-of-the-art performance. Further,
our performance improves as we add more videos to our offline training set. To
the best of our knowledge, our tracker is the first neural-network tracker that
learns to track generic objects at 100 fps.
</summary>
    <author>
      <name>David Held</name>
    </author>
    <author>
      <name>Sebastian Thrun</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in European Conference on Computer Vision (ECCV) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01802v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01802v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01785v1</id>
    <updated>2016-04-06T20:01:28Z</updated>
    <published>2016-04-06T20:01:28Z</published>
    <title>Safe Probability</title>
    <summary>  We formalize the idea of probability distributions that lead to reliable
predictions about some, but not all aspects of a domain. The resulting notion
of `safety' provides a fresh perspective on foundational issues in statistics,
providing a middle ground between imprecise probability and multiple-prior
models on the one hand and strictly Bayesian approaches on the other. It also
allows us to formalize fiducial distributions in terms of the set of random
variables that they can safely predict, thus taking some of the sting out of
the fiducial idea. By restricting probabilistic inference to safe uses, one
also automatically avoids paradoxes such as the Monty Hall problem. Safety
comes in a variety of degrees, such as "validity" (the strongest notion),
"calibration", "confidence safety" and "unbiasedness" (almost the weakest
notion).
</summary>
    <author>
      <name>Peter Grünwald</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62A01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01734v1</id>
    <updated>2016-04-06T19:08:34Z</updated>
    <published>2016-04-06T19:08:34Z</published>
    <title>Efficiency and Sequenceability in Fair Division of Indivisible Goods
  with Additive Preferences</title>
    <summary>  In fair division of indivisible goods, using sequences of sincere choices (or
picking sequences) is a natural way to allocate the objects. The idea is the
following: at each stage, a designated agent picks one object among those that
remain. This paper, restricted to the case where the agents have numerical
additive preferences over objects, revisits to some extent the seminal paper by
Brams and King [9] which was specific to ordinal and linear order preferences
over items. We point out similarities and differences with this latter context.
In particular, we show that any Pareto-optimal allocation (under additive
preferences) is sequenceable, but that the converse is not true anymore. This
asymmetry leads naturally to the definition of a "scale of efficiency" having
three steps: Pareto-optimality, sequenceability without Pareto-optimality, and
non-sequenceability. Finally, we investigate the links between these efficiency
properties and the "scale of fairness" we have described in an earlier work
[7]: we first show that an allocation can be envy-free and non-sequenceable,
but that every competitive equilibrium with equal incomes is sequenceable. Then
we experimentally explore the links between the scales of efficiency and
fairness.
</summary>
    <author>
      <name>Sylvain Bouveret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Toulouse</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Lemaître</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Toulouse</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">COMSOC-2016, Jun 2016, Toulouse, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01696v1</id>
    <updated>2016-04-06T17:15:10Z</updated>
    <published>2016-04-06T17:15:10Z</published>
    <title>A Corpus and Evaluation Framework for Deeper Understanding of
  Commonsense Stories</title>
    <summary>  Representation and learning of commonsense knowledge is one of the
foundational problems in the quest to enable deep language understanding. This
issue is particularly challenging for understanding casual and correlational
relationships between events. While this topic has received a lot of interest
in the NLP community, research has been hindered by the lack of a proper
evaluation framework. This paper attempts to address this problem with a new
framework for evaluating story understanding and script learning: the 'Story
Cloze Test'. This test requires a system to choose the correct ending to a
four-sentence story. We created a new corpus of ~50k five-sentence commonsense
stories, ROCStories, to enable this evaluation. This corpus is unique in two
ways: (1) it captures a rich set of causal and temporal commonsense relations
between daily events, and (2) it is a high quality collection of everyday life
stories that can also be used for story generation. Experimental evaluation
shows that a host of baselines and state-of-the-art models based on shallow
language understanding struggle to achieve a high score on the Story Cloze
Test. We discuss these implications for script and story learning, and offer
suggestions for deeper language understanding.
</summary>
    <author>
      <name>Nasrin Mostafazadeh</name>
    </author>
    <author>
      <name>Nathanael Chambers</name>
    </author>
    <author>
      <name>Xiaodong He</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <author>
      <name>Dhruv Batra</name>
    </author>
    <author>
      <name>Lucy Vanderwende</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>James Allen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 2016 North American Chapter of the ACL (NAACL
  HLT), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01673v2</id>
    <updated>2016-04-07T15:09:02Z</updated>
    <published>2016-04-06T16:03:42Z</published>
    <title>On the uniform one-dimensional fragment</title>
    <summary>  The uniform one-dimensional fragment of first-order logic, U1, is a recently
introduced formalism that extends two-variable logic in a natural way to
contexts with relations of all arities. We survey properties of U1 and
investigate its relationship to description logics designed to accommodate
higher arity relations, with particular attention given to DLR_reg. We also
define a description logic version of a variant of U1 and prove a range of new
results concerning the expressivity of U1 and related logics.
</summary>
    <author>
      <name>Antti Kuusisto</name>
    </author>
    <link href="http://arxiv.org/abs/1604.01673v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01673v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01662v2</id>
    <updated>2016-04-07T06:17:44Z</updated>
    <published>2016-04-06T15:35:08Z</published>
    <title>Towards Bayesian Deep Learning: A Survey</title>
    <summary>  While perception tasks such as visual object recognition and text
understanding play an important role in human intelligence, the subsequent
tasks that involve inference, reasoning and planning require an even higher
level of intelligence. The past few years have seen major advances in many
perception tasks using deep learning models. For higher-level inference,
however, probabilistic graphical models with their Bayesian nature are still
more powerful and flexible. To achieve integrated intelligence that involves
both perception and inference, it is naturally desirable to tightly integrate
deep learning and Bayesian models within a principled probabilistic framework,
which we call Bayesian deep learning. In this unified framework, the perception
of text or images using deep learning can boost the performance of higher-level
inference and in return, the feedback from the inference process is able to
enhance the perception of text or images. This survey provides a general
introduction to Bayesian deep learning and reviews its recent applications on
recommender systems, topic models, and control. In this survey, we also discuss
the relationship and differences between Bayesian deep learning and other
related topics like Bayesian treatment of neural networks.
</summary>
    <author>
      <name>Hao Wang</name>
    </author>
    <author>
      <name>Dit-Yan Yeung</name>
    </author>
    <link href="http://arxiv.org/abs/1604.01662v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01662v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01360v2</id>
    <updated>2016-07-26T03:30:44Z</updated>
    <published>2016-04-05T18:47:15Z</published>
    <title>The Curious Robot: Learning Visual Representations via Physical
  Interactions</title>
    <summary>  What is the right supervisory signal to train visual representations? Current
approaches in computer vision use category labels from datasets such as
ImageNet to train ConvNets. However, in case of biological agents, visual
representation learning does not require millions of semantic labels. We argue
that biological agents use physical interactions with the world to learn visual
representations unlike current vision systems which just use passive
observations (images and videos downloaded from web). For example, babies push
objects, poke them, put them in their mouth and throw them to learn
representations. Towards this goal, we build one of the first systems on a
Baxter platform that pushes, pokes, grasps and observes objects in a tabletop
environment. It uses four different types of physical interactions to collect
more than 130K datapoints, with each datapoint providing supervision to a
shared ConvNet architecture allowing us to learn visual representations. We
show the quality of learned representations by observing neuron activations and
performing nearest neighbor retrieval on this learned representation.
Quantitatively, we evaluate our learned ConvNet on image classification tasks
and show improvements compared to learning without external data. Finally, on
the task of instance retrieval, our network outperforms the ImageNet network on
recall@1 by 3%
</summary>
    <author>
      <name>Lerrel Pinto</name>
    </author>
    <author>
      <name>Dhiraj Gandhi</name>
    </author>
    <author>
      <name>Yuanfeng Han</name>
    </author>
    <author>
      <name>Yong-Lae Park</name>
    </author>
    <author>
      <name>Abhinav Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1604.01360v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01360v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01350v1</id>
    <updated>2016-04-05T18:00:02Z</updated>
    <published>2016-04-05T18:00:02Z</published>
    <title>Bounded Optimal Exploration in MDP</title>
    <summary>  Within the framework of probably approximately correct Markov decision
processes (PAC-MDP), much theoretical work has focused on methods to attain
near optimality after a relatively long period of learning and exploration.
However, practical concerns require the attainment of satisfactory behavior
within a short period of time. In this paper, we relax the PAC-MDP conditions
to reconcile theoretically driven exploration methods and practical needs. We
propose simple algorithms for discrete and continuous state spaces, and
illustrate the benefits of our proposed relaxation via theoretical analyses and
numerical examples. Our algorithms also maintain anytime error bounds and
average loss bounds. Our approach accommodates both Bayesian and non-Bayesian
methods.
</summary>
    <author>
      <name>Kenji Kawaguchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 30th AAAI Conference on Artificial Intelligence
  (AAAI), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01335v2</id>
    <updated>2016-07-20T01:55:12Z</updated>
    <published>2016-04-05T17:08:14Z</published>
    <title>Deep Cross Residual Learning for Multitask Visual Recognition</title>
    <summary>  Residual learning has recently surfaced as an effective means of constructing
very deep neural networks for object recognition. However, current incarnations
of residual networks do not allow for the modeling and integration of complex
relations between closely coupled recognition tasks or across domains. Such
problems are often encountered in multimedia applications involving large-scale
content recognition. We propose a novel extension of residual learning for deep
networks that enables intuitive learning across multiple related tasks using
cross-connections called cross-residuals. These cross-residuals connections can
be viewed as a form of in-network regularization and enables greater network
generalization. We show how cross-residual learning (CRL) can be integrated in
multitask networks to jointly train and detect visual concepts across several
tasks. We present a single multitask cross-residual network with &gt;40% less
parameters that is able to achieve competitive, or even better, detection
performance on a visual sentiment concept detection problem normally requiring
multiple specialized single-task networks. The resulting multitask
cross-residual network also achieves better detection performance by about
10.4% over a standard multitask residual network without cross-residuals with
even a small amount of cross-task weighting.
</summary>
    <author>
      <name>Brendan Jou</name>
    </author>
    <author>
      <name>Shih-Fu Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, To appear in ACM Multimedia</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01335v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01335v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1; I.5.4; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01277v2</id>
    <updated>2016-06-28T17:56:47Z</updated>
    <published>2016-04-05T14:44:03Z</published>
    <title>Landmark-based Plan Recognition</title>
    <summary>  Recognition of goals and plans using incomplete evidence from action
execution can be done efficiently by using planning techniques. In many
applications it is important to recognize goals and plans not only accurately,
but also quickly. In this paper, we develop a heuristic approach for
recognizing plans based on planning techniques that rely on ordering
constraints to filter candidate goals from observations. These ordering
constraints are called landmarks in the planning literature, which are facts or
actions that cannot be avoided to achieve a goal. We show the applicability of
planning landmarks in two settings: first, we use it directly to develop a
heuristic-based plan recognition approach; second, we refine an existing
planning-based plan recognition approach by pre-filtering its candidate goals.
Our empirical evaluation shows that our approach is not only substantially more
accurate than the state-of-the-art in all available datasets, it is also an
order of magnitude faster.
</summary>
    <author>
      <name>Ramon Fraga Pereira</name>
    </author>
    <author>
      <name>Felipe Meneguzzi</name>
    </author>
    <link href="http://arxiv.org/abs/1604.01277v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01277v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01272v1</id>
    <updated>2016-04-05T14:32:48Z</updated>
    <published>2016-04-05T14:32:48Z</published>
    <title>Feature extraction using Latent Dirichlet Allocation and Neural
  Networks: A case study on movie synopses</title>
    <summary>  Feature extraction has gained increasing attention in the field of machine
learning, as in order to detect patterns, extract information, or predict
future observations from big data, the urge of informative features is crucial.
The process of extracting features is highly linked to dimensionality reduction
as it implies the transformation of the data from a sparse high-dimensional
space, to higher level meaningful abstractions. This dissertation employs
Neural Networks for distributed paragraph representations, and Latent Dirichlet
Allocation to capture higher level features of paragraph vectors. Although
Neural Networks for distributed paragraph representations are considered the
state of the art for extracting paragraph vectors, we show that a quick topic
analysis model such as Latent Dirichlet Allocation can provide meaningful
features too. We evaluate the two methods on the CMU Movie Summary Corpus, a
collection of 25,203 movie plot summaries extracted from Wikipedia. Finally,
for both approaches, we use K-Nearest Neighbors to discover similar movies, and
plot the projected representations using T-Distributed Stochastic Neighbor
Embedding to depict the context similarities. These similarities, expressed as
movie distances, can be used for movies recommendation. The recommended movies
of this approach are compared with the recommended movies from IMDB, which use
a collaborative filtering recommendation approach, to show that our two models
could constitute either an alternative or a supplementary recommendation
approach.
</summary>
    <author>
      <name>Despoina Christou</name>
    </author>
    <link href="http://arxiv.org/abs/1604.01272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01219v1</id>
    <updated>2016-04-05T11:18:04Z</updated>
    <published>2016-04-05T11:18:04Z</published>
    <title>Learning to Generate Posters of Scientific Papers</title>
    <summary>  Researchers often summarize their work in the form of posters. Posters
provide a coherent and efficient way to convey core ideas from scientific
papers. Generating a good scientific poster, however, is a complex and time
consuming cognitive task, since such posters need to be readable, informative,
and visually aesthetic. In this paper, for the first time, we study the
challenging problem of learning to generate posters from scientific papers. To
this end, a data-driven framework, that utilizes graphical models, is proposed.
Specifically, given content to display, the key elements of a good poster,
including panel layout and attributes of each panel, are learned and inferred
from data. Then, given inferred layout and attributes, composition of graphical
elements within each panel is synthesized. To learn and validate our model, we
collect and make public a Poster-Paper dataset, which consists of scientific
papers and corresponding posters with exhaustively labelled panels and
attributes. Qualitative and quantitative results indicate the effectiveness of
our approach.
</summary>
    <author>
      <name>Yuting Qiang</name>
    </author>
    <author>
      <name>Yanwei Fu</name>
    </author>
    <author>
      <name>Yanwen Guo</name>
    </author>
    <author>
      <name>Zhi-Hua Zhou</name>
    </author>
    <author>
      <name>Leonid Sigal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of the 30th AAAI Conference on Artificial Intelligence
  (AAAI'16), Phoenix, AZ, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01166v1</id>
    <updated>2016-04-05T08:15:24Z</updated>
    <published>2016-04-05T08:15:24Z</published>
    <title>An Efficient Algorithm for Mining Frequent Sequence with Constraint
  Programming</title>
    <summary>  The main advantage of Constraint Programming (CP) approaches for sequential
pattern mining (SPM) is their modularity, which includes the ability to add new
constraints (regular expressions, length restrictions, etc). The current best
CP approach for SPM uses a global constraint (module) that computes the
projected database and enforces the minimum frequency; it does this with a
filtering algorithm similar to the PrefixSpan method. However, the resulting
system is not as scalable as some of the most advanced mining systems like
Zaki's cSPADE. We show how, using techniques from both data mining and CP, one
can use a generic constraint solver and yet outperform existing specialized
systems. This is mainly due to two improvements in the module that computes the
projected frequencies: first, computing the projected database can be sped up
by pre-computing the positions at which an symbol can become unsupported by a
sequence, thereby avoiding to scan the full sequence each time; and second by
taking inspiration from the trailing used in CP solvers to devise a
backtracking-aware data structure that allows fast incremental storing and
restoring of the projected database. Detailed experiments show how this
approach outperforms existing CP as well as specialized systems for SPM, and
that the gain in efficiency translates directly into increased efficiency for
other settings such as mining with regular expressions.
</summary>
    <author>
      <name>John O. R. Aoga</name>
    </author>
    <author>
      <name>Tias Guns</name>
    </author>
    <author>
      <name>Pierre Schaus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">frequent sequence mining, constraint programming</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00932v1</id>
    <updated>2016-04-04T16:18:16Z</updated>
    <published>2016-04-04T16:18:16Z</published>
    <title>Asking the metaquestions in constraint tractability</title>
    <summary>  The constraint satisfaction problem (CSP) involves deciding, given a set of
variables and a set of constraints on the variables, whether or not there is an
assignment to the variables satisfying all of the constraints. One formulation
of the CSP is as the problem of deciding, given a pair (G,H) of relational
structures, whether or not there is a homomorphism from the first structure to
the second structure. The CSP is in general NP-hard; a common way to restrict
this problem is to fix the second structure H, so that each structure H gives
rise to a problem CSP(H). The problem family CSP(H) has been studied using an
algebraic approach, which links the algorithmic and complexity properties of
each problem CSP(H) to a set of operations, the so-called polymorphisms of H.
Certain types of polymorphisms are known to imply the polynomial-time
tractability of $CSP(H)$, and others are conjectured to do so. This article
systematically studies---for various classes of polymorphisms---the
computational complexity of deciding whether or not a given structure H admits
a polymorphism from the class. Among other results, we prove the
NP-completeness of deciding a condition conjectured to characterize the
tractable problems CSP(H), as well as the NP-completeness of deciding if CSP(H)
has bounded width.
</summary>
    <author>
      <name>Hubie Chen</name>
    </author>
    <author>
      <name>Benoit Larose</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00923v1</id>
    <updated>2016-04-04T15:56:52Z</updated>
    <published>2016-04-04T15:56:52Z</published>
    <title>Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning</title>
    <summary>  In this paper we present a new way of predicting the performance of a
reinforcement learning policy given historical data that may have been
generated by a different policy. The ability to evaluate a policy from
historical data is important for applications where the deployment of a bad
policy can be dangerous or costly. We show empirically that our algorithm
produces estimates that often have orders of magnitude lower mean squared error
than existing methods---it makes more efficient use of the available data. Our
new estimator is based on two advances: an extension of the doubly robust
estimator (Jiang and Li, 2015), and a new way to mix between model based
estimates and importance sampling based estimates.
</summary>
    <author>
      <name>Philip S. Thomas</name>
    </author>
    <author>
      <name>Emma Brunskill</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00869v1</id>
    <updated>2016-04-04T14:23:25Z</updated>
    <published>2016-04-04T14:23:25Z</published>
    <title>Automatic Knowledge Base Evolution by Learning Instances</title>
    <summary>  Knowledge base is the way to store structured and unstructured data
throughout the web. Since the size of the web is increasing rapidly, there are
huge needs to structure the knowledge in a fully automated way. However
fully-automated knowledge-base evolution on the Semantic Web is a major
challenges, although there are many ontology evolution techniques available.
Therefore learning ontology automatically can contribute to the semantic web
society significantly. In this paper, we propose full-automated ontology
learning algorithm to generate refined knowledge base from incomplete knowledge
base and rdf-triples. Our algorithm is data-driven approach which is based on
the property of each instance. Ontology class is being elaborated by
generalizing frequent property of its instances. By using that developed class
information, each instance can find its most relatively matching class. By
repeating these two steps, we achieve fully-automated ontology evolution from
incomplete basic knowledge base.
</summary>
    <author>
      <name>Sundong Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, submitted to International Semantic Web Conference 2014
  (Rejected), Revising(2016-04-04~)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00799v1</id>
    <updated>2016-04-04T10:11:52Z</updated>
    <published>2016-04-04T10:11:52Z</published>
    <title>Extending DLR with Labelled Tuples, Projections, Functional Dependencies
  and Objectification (full version)</title>
    <summary>  We introduce an extension of the n-ary description logic DLR to deal with
attribute-labelled tuples (generalising the positional notation), with
arbitrary projections of relations (inclusion dependencies), generic functional
dependencies and with global and local objectification (reifying relations or
their projections). We show how a simple syntactic condition on the appearance
of projections and functional dependencies in a knowledge base makes the
language decidable without increasing the computational complexity of the basic
DLR language.
</summary>
    <author>
      <name>Alessandro Artale</name>
    </author>
    <author>
      <name>Enrico Franconi</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00727v4</id>
    <updated>2016-06-05T02:02:10Z</updated>
    <published>2016-04-04T02:43:23Z</published>
    <title>Character-Level Question Answering with Attention</title>
    <summary>  We show that a character-level encoder-decoder framework can be successfully
applied to question answering with a structured knowledge base. We use our
model for single-relation question answering and demonstrate the effectiveness
of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we
improve state-of-the-art accuracy from 63.9% to 70.9%, without use of
ensembles. Importantly, our character-level model has 16x fewer parameters than
an equivalent word-level model, can be learned with significantly less data
compared to previous work, which relies on data augmentation, and is robust to
new entities in testing.
</summary>
    <author>
      <name>David Golub</name>
    </author>
    <author>
      <name>Xiaodong He</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00727v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00727v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00697v3</id>
    <updated>2016-07-16T05:09:04Z</updated>
    <published>2016-04-03T22:44:00Z</published>
    <title>A New Learning Method for Inference Accuracy, Core Occupation, and
  Performance Co-optimization on TrueNorth Chip</title>
    <summary>  IBM TrueNorth chip uses digital spikes to perform neuromorphic computing and
achieves ultrahigh execution parallelism and power efficiency. However, in
TrueNorth chip, low quantization resolution of the synaptic weights and spikes
significantly limits the inference (e.g., classification) accuracy of the
deployed neural network model. Existing workaround, i.e., averaging the results
over multiple copies instantiated in spatial and temporal domains, rapidly
exhausts the hardware resources and slows down the computation. In this work,
we propose a novel learning method on TrueNorth platform that constrains the
random variance of each computation copy and reduces the number of needed
copies. Compared to the existing learning method, our method can achieve up to
68.8% reduction of the required neuro-synaptic cores or 6.5X speedup, with even
slightly improved inference accuracy.
</summary>
    <author>
      <name>Wei Wen</name>
    </author>
    <author>
      <name>Chunpeng Wu</name>
    </author>
    <author>
      <name>Yandan Wang</name>
    </author>
    <author>
      <name>Kent Nixon</name>
    </author>
    <author>
      <name>Qing Wu</name>
    </author>
    <author>
      <name>Mark Barnell</name>
    </author>
    <author>
      <name>Hai Li</name>
    </author>
    <author>
      <name>Yiran Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2897937.2897968</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2897937.2897968" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; 9 figures; DAC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00697v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00697v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00693v1</id>
    <updated>2016-04-03T21:48:37Z</updated>
    <published>2016-04-03T21:48:37Z</published>
    <title>Pareto Optimality and Strategy Proofness in Group Argument Evaluation</title>
    <summary>  An inconsistent knowledge base can be abstracted as a set of arguments and a
defeat relation among them. There can be more than one consistent way to
evaluate such an argumentation graph. Collective argument evaluation is the
problem of aggregating the opinions of multiple agents on how a given set of
arguments should be evaluated. It is crucial not only to ensure that the
outcome is logically consistent, but also satisfies measures of social
optimality and immunity to strategic manipulation. This is because agents have
their individual preferences about what the outcome ought to be. In the current
paper, we analyze three previously introduced argument-based aggregation
operators with respect to Pareto optimality and strategy proofness under
different general classes of agent preferences. We highlight fundamental
trade-offs between strategic manipulability and social optimality on one hand,
and classical logical criteria on the other. Our results motivate further
investigation into the relationship between social choice and argumentation
theory. The results are also relevant for choosing an appropriate aggregation
operator given the criteria that are considered more important, as well as the
nature of agents' preferences.
</summary>
    <author>
      <name>Edmond Awad</name>
    </author>
    <author>
      <name>Martin Caminada</name>
    </author>
    <author>
      <name>Gabriella Pigozzi</name>
    </author>
    <author>
      <name>Mikołaj Podlaszewski</name>
    </author>
    <author>
      <name>Iyad Rahwan</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00681v1</id>
    <updated>2016-04-03T19:58:18Z</updated>
    <published>2016-04-03T19:58:18Z</published>
    <title>Experimental Assessment of Aggregation Rules in Argumentation-enabled
  Collective Intelligence</title>
    <summary>  On the Web, there is always a need to aggregate opinions from the crowd (as
in posts, social networks, forums, etc.). Different mechanisms have been
implemented to capture these opinions such as "Like" in Facebook, "Favorite" in
Twitter, thumbs-up/down, flagging, and so on. However, in more contested
domains (e.g. Wikipedia, political discussion, and climate change discussion)
these mechanisms are not sufficient since they only deal with each issue
independently without considering the relationships between different claims.
We can view a set of conflicting arguments as a graph in which the nodes
represent arguments and the arcs between these nodes represent the defeat
relation. A group of people can then collectively evaluate such graphs. To do
this, the group must use a rule to aggregate their individual opinions about
the entire argument graph. Here, we present the first experimental evaluation
of different aggregation rules presented in the literature. We use randomized
controlled experiments to investigate which rules people consider better at
aggregating opinions under different conditions. Our analysis reveals a number
of factors, not captured by traditional formal models, that play an important
role in determining the efficacy of aggregation. These results help bring
formal models of argumentation closer to real-world application.
</summary>
    <author>
      <name>Edmond Awad</name>
    </author>
    <author>
      <name>Jean-François Bonnefon</name>
    </author>
    <author>
      <name>Martin Caminada</name>
    </author>
    <author>
      <name>Thomas Malone</name>
    </author>
    <author>
      <name>Iyad Rahwan</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00664v1</id>
    <updated>2016-04-03T18:06:36Z</updated>
    <published>2016-04-03T18:06:36Z</published>
    <title>Bicycle-Sharing System Analysis and Trip Prediction</title>
    <summary>  Bicycle-sharing systems, which can provide shared bike usage services for the
public, have been launched in many big cities. In bicycle-sharing systems,
people can borrow and return bikes at any stations in the service region very
conveniently. Therefore, bicycle-sharing systems are normally used as a
short-distance trip supplement for private vehicles as well as regular public
transportation. Meanwhile, for stations located at different places in the
service region, the bike usages can be quite skewed and imbalanced. Some
stations have too many incoming bikes and get jammed without enough docks for
upcoming bikes, while some other stations get empty quickly and lack enough
bikes for people to check out. Therefore, inferring the potential destinations
and arriving time of each individual trip beforehand can effectively help the
service providers schedule manual bike re-dispatch in advance. In this paper,
we will study the individual trip prediction problem for bicycle-sharing
systems. To address the problem, we study a real-world bicycle-sharing system
and analyze individuals' bike usage behaviors first. Based on the analysis
results, a new trip destination prediction and trip duration inference model
will be introduced. Experiments conducted on a real-world bicycle-sharing
system demonstrate the effectiveness of the proposed model.
</summary>
    <author>
      <name>Jiawei Zhang</name>
    </author>
    <author>
      <name>Xiao Pan</name>
    </author>
    <author>
      <name>Moyin Li</name>
    </author>
    <author>
      <name>Philip S. Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures, accepted by 2016 IEEE MDM Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00647v1</id>
    <updated>2016-04-03T15:42:36Z</updated>
    <published>2016-04-03T15:42:36Z</published>
    <title>Multi-Relational Learning at Scale with ADMM</title>
    <summary>  Learning from multiple-relational data which contains noise, ambiguities, or
duplicate entities is essential to a wide range of applications such as
statistical inference based on Web Linked Data, recommender systems,
computational biology, and natural language processing. These tasks usually
require working with very large and complex datasets - e.g., the Web graph -
however, current approaches to multi-relational learning are not practical for
such scenarios due to their high computational complexity and poor scalability
on large data.
  In this paper, we propose a novel and scalable approach for multi-relational
factorization based on consensus optimization. Our model, called ConsMRF, is
based on the Alternating Direction Method of Multipliers (ADMM) framework,
which enables us to optimize each target relation using a smaller set of
parameters than the state-of-the-art competitors in this task.
  Due to ADMM's nature, ConsMRF can be easily parallelized which makes it
suitable for large multi-relational data. Experiments on large Web datasets -
derived from DBpedia, Wikipedia and YAGO - show the efficiency and performance
improvement of ConsMRF over strong competitors. In addition, ConsMRF
near-linear scalability indicates great potential to tackle Web-scale problem
sizes.
</summary>
    <author>
      <name>Lucas Drumond</name>
    </author>
    <author>
      <name>Ernesto Diaz-Aviles</name>
    </author>
    <author>
      <name>Lars Schmidt-Thieme</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Multi-Relational Learning, Distributed Learning,
  Factorization Models, ADMM</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00644v2</id>
    <updated>2016-04-11T18:35:29Z</updated>
    <published>2016-04-03T14:57:24Z</published>
    <title>An electronic-game framework for evaluating coevolutionary algorithms</title>
    <summary>  One of the common artificial intelligence applications in electronic games
consists of making an artificial agent learn how to execute some determined
task successfully in a game environment. One way to perform this task is
through machine learning algorithms capable of learning the sequence of actions
required to win in a given game environment. There are several supervised
learning techniques able to learn the correct answer for a problem through
examples. However, when learning how to play electronic games, the correct
answer might only be known by the end of the game, after all the actions were
already taken. Thus, not being possible to measure the accuracy of each
individual action to be taken at each time step. A way for dealing with this
problem is through Neuroevolution, a method which trains Artificial Neural
Networks using evolutionary algorithms. In this article, we introduce a
framework for testing optimization algorithms with artificial agent controllers
in electronic games, called EvoMan, which is inspired in the action-platformer
game Mega Man II. The environment can be configured to run in different
experiment modes, as single evolution, coevolution and others. To demonstrate
some challenges regarding the proposed platform, as initial experiments we
applied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in the
context of competitively coevolving two distinct agents in this game.
</summary>
    <author>
      <name>Karine da Silva Miras de Araújo</name>
    </author>
    <author>
      <name>Fabrício Olivetti de França</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is a translation of \cite{karine2015}, published in
  Portuguese at Brazilian Congress on Computational Intelligence, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00644v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00644v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00545v3</id>
    <updated>2016-07-13T14:54:16Z</updated>
    <published>2016-04-02T19:26:05Z</published>
    <title>The AGI Containment Problem</title>
    <summary>  There is considerable uncertainty about what properties, capabilities and
motivations future AGIs will have. In some plausible scenarios, AGIs may pose
security risks arising from accidents and defects. In order to mitigate these
risks, prudent early AGI research teams will perform significant testing on
their creations before use. Unfortunately, if an AGI has human-level or greater
intelligence, testing itself may not be safe; some natural AGI goal systems
create emergent incentives for AGIs to tamper with their test environments,
make copies of themselves on the internet, or convince developers and operators
to do dangerous things. In this paper, we survey the AGI containment problem -
the question of how to build a container in which tests can be conducted safely
and reliably, even on AGIs with unknown motivations and capabilities that could
be dangerous. We identify requirements for AGI containers, available
mechanisms, and weaknesses that need to be addressed.
</summary>
    <author>
      <name>James Babcock</name>
    </author>
    <author>
      <name>Janos Kramar</name>
    </author>
    <author>
      <name>Roman Yampolskiy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-41649-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-41649-6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Artificial Intelligence 9782 (AGI 2016,
  Proceedings) 53-63</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.00545v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00545v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00536v1</id>
    <updated>2016-04-02T17:50:32Z</updated>
    <published>2016-04-02T17:50:32Z</published>
    <title>Improving SAT Solvers via Blocked Clause Decomposition</title>
    <summary>  The decision variable selection policy used by the most competitive CDCL
(Conflict-Driven Clause Learning) SAT solvers is either VSIDS (Variable State
Independent Decaying Sum) or its variants such as exponential version EVSIDS.
The common characteristic of VSIDS and its variants is to make use of
statistical information in the solving process, but ignore structure
information of the problem. For this reason, this paper modifies the decision
variable selection policy, and presents a SAT solving technique based on BCD
(Blocked Clause Decomposition). Its basic idea is that a part of decision
variables are selected by VSIDS heuristic, while another part of decision
variables are selected by blocked sets that are obtained by BCD. Compared with
the existing BCD-based technique, our technique is simple, and need not to
reencode CNF formulas. SAT solvers for certified UNSAT track can apply also our
BCD-based technique. Our experiments on application benchmarks demonstrate that
the new variables selection policy based on BCD can increase the performance of
SAT solvers such as abcdSAT. The solver with BCD solved an instance from the
SAT Race 2015 that was not solved by any solver so far. This shows that in some
cases, the heuristic based on structure information is more efficient than that
based on statistical information.
</summary>
    <author>
      <name>Jingchao Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02239v1</id>
    <updated>2016-04-02T10:13:26Z</updated>
    <published>2016-04-02T10:13:26Z</published>
    <title>A Formal Calculus for International Relations Computation and Evaluation</title>
    <summary>  This publication presents a relation computation or calculus for
international relations using a mathematical modeling. It examined trust for
international relations and its calculus, which related to Bayesian inference,
Dempster-Shafer theory and subjective logic. Based on an observation in the
literature, we found no literature discussing the calculus method for the
international relations. To bridge this research gap, we propose a relation
algebra method for international relations computation. The proposed method
will allow a relation computation which is previously subjective and
incomputable. We also present three international relations as case studies to
demonstrate the proposed method is a real-world scenario. The method will
deliver the relation computation for the international relations that to
support decision makers in a government such as foreign ministry, defense
ministry, presidential or prime minister office. The Department of Defense
(DoD) may use our method to determine a nation that can be identified as a
friendly, neutral or hostile nation.
</summary>
    <author>
      <name>Mohd Anuar Mat Isa</name>
    </author>
    <author>
      <name>Ramlan Mahmod</name>
    </author>
    <author>
      <name>Nur Izura Udzir</name>
    </author>
    <author>
      <name>Jamalul-lail Ab Manan</name>
    </author>
    <author>
      <name>Audun Jøsang</name>
    </author>
    <author>
      <name>Ali Dehghan Tanha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.1.3796.6321</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.1.3796.6321" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">keywords: Relation Algebra, International Relations, Defense,
  Relation Calculus, Foreign Policy, Politics Economy, Dempster-Shafer,
  Subjective Logic, Common Criteria. arXiv admin note: text overlap with
  arXiv:1604.00980</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Current Research in Science, Issn 2322-5009, 4(2),
  2016: 177-194</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.02239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00461v1</id>
    <updated>2016-04-02T04:59:21Z</updated>
    <published>2016-04-02T04:59:21Z</published>
    <title>Embedding Lexical Features via Low-Rank Tensors</title>
    <summary>  Modern NLP models rely heavily on engineered features, which often combine
word and contextual information into complex lexical features. Such combination
results in large numbers of features, which can lead to over-fitting. We
present a new model that represents complex lexical features---comprised of
parts for words, contextual information and labels---in a tensor that captures
conjunction information among these parts. We apply low-rank tensor
approximations to the corresponding parameter tensors to reduce the parameter
space and improve prediction speed. Furthermore, we investigate two methods for
handling features that include $n$-grams of mixed lengths. Our model achieves
state-of-the-art results on tasks in relation extraction, PP-attachment, and
preposition disambiguation.
</summary>
    <author>
      <name>Mo Yu</name>
    </author>
    <author>
      <name>Mark Dredze</name>
    </author>
    <author>
      <name>Raman Arora</name>
    </author>
    <author>
      <name>Matthew Gormley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by NAACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00449v1</id>
    <updated>2016-04-02T01:28:27Z</updated>
    <published>2016-04-02T01:28:27Z</published>
    <title>3D-R2N2: A Unified Approach for Single and Multi-view 3D Object
  Reconstruction</title>
    <summary>  Inspired by the recent success of methods that employ shape priors to achieve
robust 3D reconstructions, we propose a novel recurrent neural network
architecture that we call the 3D Recurrent Reconstruction Neural Network
(3D-R2N2). The network learns a mapping from images of objects to their
underlying 3D shapes from a large collection of synthetic data. Our network
takes in one or more images of an object instance from arbitrary viewpoints and
outputs a reconstruction of the object in the form of a 3D occupancy grid.
Unlike most of the previous works, our network does not require any image
annotations or object class labels for training or testing. Our extensive
experimental analysis shows that our reconstruction framework i) outperforms
the state-of-the-art methods for single view reconstruction, and ii) enables
the 3D reconstruction of objects in situations when traditional SFM/SLAM
methods fail (because of lack of texture and/or wide baseline).
</summary>
    <author>
      <name>Christopher B. Choy</name>
    </author>
    <author>
      <name>Danfei Xu</name>
    </author>
    <author>
      <name>JunYoung Gwak</name>
    </author>
    <author>
      <name>Kevin Chen</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appendix can be found at
  http://cvgl.stanford.edu/papers/choy_16_appendix.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00377v1</id>
    <updated>2016-04-01T19:38:35Z</updated>
    <published>2016-04-01T19:38:35Z</published>
    <title>Reinforcement learning based local search for grouping problems: A case
  study on graph coloring</title>
    <summary>  Grouping problems aim to partition a set of items into multiple mutually
disjoint subsets according to some specific criterion and constraints. Grouping
problems cover a large class of important combinatorial optimization problems
that are generally computationally difficult. In this paper, we propose a
general solution approach for grouping problems, i.e., reinforcement learning
based local search (RLS), which combines reinforcement learning techniques with
descent-based local search. The viability of the proposed approach is verified
on a well-known representative grouping problem (graph coloring) where a very
simple descent-based coloring algorithm is applied. Experimental studies on
popular DIMACS and COLOR02 benchmark graphs indicate that RLS achieves
competitive performances compared to a number of well-known coloring
algorithms.
</summary>
    <author>
      <name>Yangming Zhou</name>
    </author>
    <author>
      <name>Jin-Kao Hao</name>
    </author>
    <author>
      <name>Béatrice Duval</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00359v2</id>
    <updated>2016-05-25T06:49:31Z</updated>
    <published>2016-04-01T18:55:05Z</published>
    <title>COCO: The Bi-objective Black Box Optimization Benchmarking (bbob-biobj)
  Test Suite</title>
    <summary>  The bbob-biobj test suite contains 55 bi-objective functions in continuous
domain which are derived from combining functions of the well-known
single-objective noiseless bbob test suite. Besides giving the actual function
definitions and presenting their (known) properties, this documentation also
aims at giving the rationale behind our approach in terms of function groups,
instances, and potential objective space normalization.
</summary>
    <author>
      <name>Tea Tusar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Dimo Brockhoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Nikolaus Hansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Anne Auger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1604.00359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00301v1</id>
    <updated>2016-04-01T15:50:24Z</updated>
    <published>2016-04-01T15:50:24Z</published>
    <title>A strengthening of rational closure in DLs: reasoning about multiple
  aspects</title>
    <summary>  We propose a logical analysis of the concept of typicality, central in human
cognition (Rosch,1978). We start from a previously proposed extension of the
basic Description Logic ALC (a computationally tractable fragment of First
Order Logic, used to represent concept inclusions and ontologies) with a
typicality operator T that allows to consistently represent the attribution to
classes of individuals of properties with exceptions (as in the classic example
(i) typical birds fly, (ii) penguins are birds but (iii) typical penguins don't
fly). We then strengthen this extension in order to separately reason about the
typicality with respect to different aspects (e.g., flying, having nice
feather: in the previous example, penguins may not inherit the property of
flying, for which they are exceptional, but can nonetheless inherit other
properties, such as having nice feather).
</summary>
    <author>
      <name>Valentina Gliozzi</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00300v1</id>
    <updated>2016-04-01T15:49:51Z</updated>
    <published>2016-04-01T15:49:51Z</published>
    <title>A SAT model to mine flexible sequences in transactional datasets</title>
    <summary>  Traditional pattern mining algorithms generally suffer from a lack of
flexibility. In this paper, we propose a SAT formulation of the problem to
successfully mine frequent flexible sequences occurring in transactional
datasets. Our SAT-based approach can easily be extended with extra constraints
to address a broad range of pattern mining applications. To demonstrate this
claim, we formulate and add several constraints, such as gap and span
constraints, to our model in order to extract more specific patterns. We also
use interactive solving to perform important derived tasks, such as closed
pattern mining or maximal pattern mining. Finally, we prove the practical
feasibility of our SAT model by running experiments on two real datasets.
</summary>
    <author>
      <name>Rémi Coletta</name>
    </author>
    <author>
      <name>Benjamin Negrevergne</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00289v2</id>
    <updated>2016-05-07T18:03:53Z</updated>
    <published>2016-04-01T15:37:57Z</published>
    <title>Building Machines That Learn and Think Like People</title>
    <summary>  Recent progress in artificial intelligence (AI) has renewed interest in
building systems that learn and think like people. Many advances have come from
using deep neural networks trained end-to-end in tasks such as object
recognition, video games, and board games, achieving performance that equals or
even beats humans in some respects. Despite their biological inspiration and
performance achievements, these systems differ from human intelligence in
crucial ways. We review progress in cognitive science suggesting that truly
human-like learning and thinking machines will have to reach beyond current
engineering trends in both what they learn, and how they learn it.
Specifically, we argue that these machines should (a) build causal models of
the world that support explanation and understanding, rather than merely
solving pattern recognition problems; (b) ground learning in intuitive theories
of physics and psychology, to support and enrich the knowledge that is learned;
and (c) harness compositionality and learning-to-learn to rapidly acquire and
generalize knowledge to new tasks and situations. We suggest concrete
challenges and promising routes towards these goals that can combine the
strengths of recent neural network advances with more structured cognitive
models.
</summary>
    <author>
      <name>Brenden M. Lake</name>
    </author>
    <author>
      <name>Tomer D. Ullman</name>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
    </author>
    <author>
      <name>Samuel J. Gershman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added references. Updated Figure 3</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00289v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00289v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00162v1</id>
    <updated>2016-04-01T08:14:30Z</updated>
    <published>2016-04-01T08:14:30Z</published>
    <title>Relations between assumption-based approaches in nonmonotonic logic and
  formal argumentation</title>
    <summary>  In this paper we make a contribution to the unification of formal models of
defeasible reasoning. We present several translations between formal
argumentation frameworks and nonmonotonic logics for reasoning with plausible
assumptions. More specifically, we translate adaptive logics into
assumption-based argumentation and ASPIC+, ASPIC+ into assumption-based
argumentation and a fragment of assumption-based argumentation into adaptive
logics. Adaptive logics are closely related to Makinson's default assumptions
and to a significant class of systems within the tradition of preferential
semantics in the vein of KLM and Shoham. Thus, our results also provide close
links between formal argumentation and the latter approaches.
</summary>
    <author>
      <name>Jesse Heyninck</name>
    </author>
    <author>
      <name>Christian Straßer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to the 16th International Workshop on Non-Monotonic
  Reasoning (NMR'16), Cape Town</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T27" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.3; I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00066v1</id>
    <updated>2016-03-31T21:53:32Z</updated>
    <published>2016-03-31T21:53:32Z</published>
    <title>To Fall Or Not To Fall: A Visual Approach to Physical Stability
  Prediction</title>
    <summary>  Understanding physical phenomena is a key competence that enables humans and
animals to act and interact under uncertain perception in previously unseen
environments containing novel object and their configurations. Developmental
psychology has shown that such skills are acquired by infants from observations
at a very early stage.
  In this paper, we contrast a more traditional approach of taking a
model-based route with explicit 3D representations and physical simulation by
an end-to-end approach that directly predicts stability and related quantities
from appearance. We ask the question if and to what extent and quality such a
skill can directly be acquired in a data-driven way bypassing the need for an
explicit simulation.
  We present a learning-based approach based on simulated data that predicts
stability of towers comprised of wooden blocks under different conditions and
quantities related to the potential fall of the towers. The evaluation is
carried out on synthetic data and compared to human judgments on the same
stimuli.
</summary>
    <author>
      <name>Wenbin Li</name>
    </author>
    <author>
      <name>Seyedmajid Azimi</name>
    </author>
    <author>
      <name>Aleš Leonardis</name>
    </author>
    <author>
      <name>Mario Fritz</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09727v1</id>
    <updated>2016-03-31T19:16:54Z</updated>
    <published>2016-03-31T19:16:54Z</published>
    <title>Neural Language Correction with Character-Based Attention</title>
    <summary>  Natural language correction has the potential to help language learners
improve their writing skills. While approaches with separate classifiers for
different error types have high precision, they do not flexibly handle errors
such as redundancy or non-idiomatic phrasing. On the other hand, word and
phrase-based machine translation methods are not designed to cope with
orthographic errors, and have recently been outpaced by neural models.
Motivated by these issues, we present a neural network-based approach to
language correction. The core component of our method is an encoder-decoder
recurrent neural network with an attention mechanism. By operating at the
character level, the network avoids the problem of out-of-vocabulary words. We
illustrate the flexibility of our approach on dataset of noisy, user-generated
text collected from an English learner forum. When combined with a language
model, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014
Shared Task. We further demonstrate that training the network on additional
data with synthesized errors can improve performance.
</summary>
    <author>
      <name>Ziang Xie</name>
    </author>
    <author>
      <name>Anand Avati</name>
    </author>
    <author>
      <name>Naveen Arivazhagan</name>
    </author>
    <author>
      <name>Dan Jurafsky</name>
    </author>
    <author>
      <name>Andrew Y. Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09545v1</id>
    <updated>2016-03-31T12:05:34Z</updated>
    <published>2016-03-31T12:05:34Z</published>
    <title>Characterizing Realizability in Abstract Argumentation</title>
    <summary>  Realizability for knowledge representation formalisms studies the following
question: given a semantics and a set of interpretations, is there a knowledge
base whose semantics coincides exactly with the given interpretation set? We
introduce a general framework for analyzing realizability in abstract
dialectical frameworks (ADFs) and various of its subclasses. In particular, the
framework applies to Dung argumentation frameworks, SETAFs by Nielsen and
Parsons, and bipolar ADFs. We present a uniform characterization method for the
admissible, complete, preferred and model/stable semantics. We employ this
method to devise an algorithm that decides realizability for the mentioned
formalisms and semantics; moreover the algorithm allows for constructing a
desired knowledge base whenever one exists. The algorithm is built in a modular
way and thus easily extensible to new formalisms and semantics. We have also
implemented our approach in answer set programming, and used the implementation
to obtain several novel results on the relative expressiveness of the
abovementioned formalisms.
</summary>
    <author>
      <name>Thomas Linsbichler</name>
    </author>
    <author>
      <name>Jörg Pührer</name>
    </author>
    <author>
      <name>Hannes Strass</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to the 16h International Workshop on Non-Monotonic
  Reasoning, 2016, Cape Town</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09511v1</id>
    <updated>2016-03-31T09:59:02Z</updated>
    <published>2016-03-31T09:59:02Z</published>
    <title>Distributing Knowledge into Simple Bases</title>
    <summary>  Understanding the behavior of belief change operators for fragments of
classical logic has received increasing interest over the last years. Results
in this direction are mainly concerned with adapting representation theorems.
However, fragment-driven belief change also leads to novel research questions.
In this paper we propose the concept of belief distribution, which can be
understood as the reverse task of merging. More specifically, we are interested
in the following question: given an arbitrary knowledge base $K$ and some
merging operator $\Delta$, can we find a profile $E$ and a constraint $\mu$,
both from a given fragment of classical logic, such that $\Delta_\mu(E)$ yields
a result equivalent to $K$? In other words, we are interested in seeing if $K$
can be distributed into knowledge bases of simpler structure, such that the
task of merging allows for a reconstruction of the original knowledge. Our
initial results show that merging based on drastic distance allows for an easy
distribution of knowledge, while the power of distribution for operators based
on Hamming distance relies heavily on the fragment of choice.
</summary>
    <author>
      <name>Adrian Haret</name>
    </author>
    <author>
      <name>Jean-Guy Mailly</name>
    </author>
    <author>
      <name>Stefan Woltran</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09502v1</id>
    <updated>2016-03-31T09:29:02Z</updated>
    <published>2016-03-31T09:29:02Z</published>
    <title>Verifiability of Argumentation Semantics</title>
    <summary>  Dung's abstract argumentation theory is a widely used formalism to model
conflicting information and to draw conclusions in such situations. Hereby, the
knowledge is represented by so-called argumentation frameworks (AFs) and the
reasoning is done via semantics extracting acceptable sets. All reasonable
semantics are based on the notion of conflict-freeness which means that
arguments are only jointly acceptable when they are not linked within the AF.
In this paper, we study the question which information on top of conflict-free
sets is needed to compute extensions of a semantics at hand. We introduce a
hierarchy of so-called verification classes specifying the required amount of
information. We show that well-known standard semantics are exactly verifiable
through a certain such class. Our framework also gives a means to study
semantics lying inbetween known semantics, thus contributing to a more abstract
understanding of the different features argumentation semantics offer.
</summary>
    <author>
      <name>Ringo Baumann</name>
    </author>
    <author>
      <name>Thomas Linsbichler</name>
    </author>
    <author>
      <name>Stefan Woltran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to the 16h International Workshop on Non-Monotonic
  Reasoning, 2016, Cape Town</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09495v1</id>
    <updated>2016-03-31T09:05:28Z</updated>
    <published>2016-03-31T09:05:28Z</published>
    <title>Reactive Policies with Planning for Action Languages</title>
    <summary>  We describe a representation in a high-level transition system for policies
that express a reactive behavior for the agent. We consider a target decision
component that figures out what to do next and an (online) planning capability
to compute the plans needed to reach these targets. Our representation allows
one to analyze the flow of executing the given reactive policy, and to
determine whether it works as expected. Additionally, the flexibility of the
representation opens a range of possibilities for designing behaviors.
</summary>
    <author>
      <name>Zeynep G. Saribatur</name>
    </author>
    <author>
      <name>Thomas Eiter</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09488v1</id>
    <updated>2016-03-31T08:41:22Z</updated>
    <published>2016-03-31T08:41:22Z</published>
    <title>Building the Signature of Set Theory Using the MathSem Program</title>
    <summary>  Knowledge representation is a popular research field in IT. As mathematical
knowledge is most formalized, its representation is important and interesting.
Mathematical knowledge consists of various mathematical theories. In this paper
we consider a deductive system that derives mathematical notions, axioms and
theorems. All these notions, axioms and theorems can be considered as the part
of elementary set theory. This theory will be represented as a semantic net.
</summary>
    <author>
      <name>Andrey Luxemburg</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09465v1</id>
    <updated>2016-03-31T06:27:16Z</updated>
    <published>2016-03-31T06:27:16Z</published>
    <title>A New Approach for Revising Logic Programs</title>
    <summary>  Belief revision has been studied mainly with respect to background logics
that are monotonic in character. In this paper we study belief revision when
the underlying logic is non-monotonic instead--an inherently interesting
problem that is under explored. In particular, we will focus on the revision of
a body of beliefs that is represented as a logic program under the answer set
semantics, while the new information is also similarly represented as a logic
program. Our approach is driven by the observation that unlike in a monotonic
setting where, when necessary, consistency in a revised body of beliefs is
maintained by jettisoning some old beliefs, in a non-monotonic setting
consistency can be restored by adding new beliefs as well. We will define a
syntactic revision function and subsequently provide representation theorem for
characterising it.
</summary>
    <author>
      <name>Zhiqiang Zhuang</name>
    </author>
    <author>
      <name>James Delgrande</name>
    </author>
    <author>
      <name>Abhaya Nayak</name>
    </author>
    <author>
      <name>Abdul Sattar</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09429v1</id>
    <updated>2016-03-31T00:48:40Z</updated>
    <published>2016-03-31T00:48:40Z</published>
    <title>Ordinal Conditional Functions for Nearly Counterfactual Revision</title>
    <summary>  We are interested in belief revision involving conditional statements where
the antecedent is almost certainly false. To represent such problems, we use
Ordinal Conditional Functions that may take infinite values. We model belief
change in this context through simple arithmetical operations that allow us to
capture the intuition that certain antecedents can not be validated by any
number of observations. We frame our approach as a form of finite belief
improvement, and we propose a model of conditional belief revision in which
only the "right" hypothetical levels of implausibility are revised.
</summary>
    <author>
      <name>Aaron Hunter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, presented at the International Workshop on
  Non-monotonic Reasoning 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09405v1</id>
    <updated>2016-03-30T22:39:59Z</updated>
    <published>2016-03-30T22:39:59Z</published>
    <title>Enhancing Sentence Relation Modeling with Auxiliary Character-level
  Embedding</title>
    <summary>  Neural network based approaches for sentence relation modeling automatically
generate hidden matching features from raw sentence pairs. However, the quality
of matching feature representation may not be satisfied due to complex semantic
relations such as entailment or contradiction. To address this challenge, we
propose a new deep neural network architecture that jointly leverage
pre-trained word embedding and auxiliary character embedding to learn sentence
meanings. The two kinds of word sequence representations as inputs into
multi-layer bidirectional LSTM to learn enhanced sentence representation. After
that, we construct matching features followed by another temporal CNN to learn
high-level hidden matching feature representations. Experimental results
demonstrate that our approach consistently outperforms the existing methods on
standard evaluation datasets.
</summary>
    <author>
      <name>Peng Li</name>
    </author>
    <author>
      <name>Heng Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09194v1</id>
    <updated>2016-03-30T13:50:13Z</updated>
    <published>2016-03-30T13:50:13Z</published>
    <title>Iterated Ontology Revision by Reinterpretation</title>
    <summary>  Iterated applications of belief change operators are essential for different
scenarios such as that of ontology evolution where new information is not
presented at once but only in piecemeal fashion within a sequence. I discuss
iterated applications of so called reinterpretation operators that trace
conflicts between ontologies back to the ambiguous of symbols and that provide
conflict resolution strategies with bridging axioms. The discussion centers on
adaptations of the classical iteration postulates according to Darwiche and
Pearl. The main result of the paper is that reinterpretation operators fulfill
the postulates for sequences containing only atomic triggers. For complex
triggers, a fulfillment is not guaranteed and indeed there are different
reasons for the different postulates why they should not be fulfilled in the
particular scenario of ontology revision with well developed ontologies.
</summary>
    <author>
      <name>Özgür Lütfü Özçep</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, to be published in Proceedings of the 16th
  International Workshop on Non-Monotonic Reasoning (NMR'16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09051v2</id>
    <updated>2016-09-02T04:53:19Z</updated>
    <published>2016-03-30T06:41:04Z</published>
    <title>Phoenix: A Self-Optimizing Chess Engine</title>
    <summary>  Since the advent of computers, many tasks which required humans to spend a
lot of time and energy have been trivialized by the computers' ability to
perform repetitive tasks extremely quickly. However there are still many areas
in which humans excel in comparison with the machines. One such area is chess.
Even with great advances in the speed and computational power of modern
machines, Grandmasters often beat the best chess programs in the world with
relative ease. This may be due to the fact that a game of chess cannot be won
by pure calculation. There is more to the goodness of a chess position than
some numerical value which apparently can be seen only by the human brain. Here
an effort has been made to improve current chess engines by letting themselves
evolve over a period of time. Firstly, the problem of learning is reduced into
an optimization problem by defining Position Evaluation in terms of Positional
Value Tables (PVTs). Next, the PVTs are optimized using Multi-Niche Crowding
which successfully identifies the optima in a multimodal function, thereby
arriving at distinctly different solutions which are close to the global
optimum.
</summary>
    <author>
      <name>Rahul A R</name>
    </author>
    <author>
      <name>G Srinivasaraghavan</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09051v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09051v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09029v1</id>
    <updated>2016-03-30T03:27:41Z</updated>
    <published>2016-03-30T03:27:41Z</published>
    <title>Maximize Pointwise Cost-sensitively Submodular Functions With Budget
  Constraint</title>
    <summary>  We study the worst-case adaptive optimization problem with budget constraint.
Unlike previous works, we consider the general setting where the cost is a set
function on sets of decisions. For this setting, we investigate the
near-optimality of greedy policies when the utility function satisfies a novel
property called pointwise cost-sensitive submodularity. This property is an
extension of cost-sensitive submodularity, which in turn is a generalization of
submodularity to general cost functions. We prove that two simple greedy
policies for the problem are not near-optimal but the best between them is
near-optimal. With this result, we propose a combined policy that is
near-optimal with respect to the optimal worst-case policy that uses half of
the budget. We discuss applications of our theoretical results and also report
experimental results comparing the greedy policies on the active learning
problem.
</summary>
    <author>
      <name>Nguyen Viet Cuong</name>
    </author>
    <author>
      <name>Huan Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08988v1</id>
    <updated>2016-03-29T22:41:17Z</updated>
    <published>2016-03-29T22:41:17Z</published>
    <title>Towards Practical Bayesian Parameter and State Estimation</title>
    <summary>  Joint state and parameter estimation is a core problem for dynamic Bayesian
networks. Although modern probabilistic inference toolkits make it relatively
easy to specify large and practically relevant probabilistic models, the silver
bullet---an efficient and general online inference algorithm for such
problems---remains elusive, forcing users to write special-purpose code for
each application. We propose a novel blackbox algorithm -- a hybrid of particle
filtering for state variables and assumed density filtering for parameter
variables. It has following advantages: (a) it is efficient due to its online
nature, and (b) it is applicable to both discrete and continuous parameter
spaces . On a variety of toy and real models, our system is able to generate
more accurate results within a fixed computation budget. This preliminary
evidence indicates that the proposed approach is likely to be of practical use.
</summary>
    <author>
      <name>Yusuf Bugra Erol</name>
    </author>
    <author>
      <name>Yi Wu</name>
    </author>
    <author>
      <name>Lei Li</name>
    </author>
    <author>
      <name>Stuart Russell</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08976v1</id>
    <updated>2016-03-29T21:41:55Z</updated>
    <published>2016-03-29T21:41:55Z</published>
    <title>Local Search Yields a PTAS for k-Means in Doubling Metrics</title>
    <summary>  The most well known and ubiquitous clustering problem encountered in nearly
every branch of science is undoubtedly $k$-means: given a set of data points
and a parameter $k$, select $k$ centres and partition the data points into $k$
clusters around these centres so that the sum of squares of distances of the
points to their cluster centre % (called the cost of the solution) is
minimized. Typically these data points lie in Euclidean space $\mathbb{R}^d$
for some $d\geq 2$.
  The most commonly used algorithm in practice is known as Lloyd-Forgy, which
is also referred to as "the" $k$-means algorithm, and various extensions of it
often work very well in practice. However, they may produce solutions whose
cost is arbitrarily large compared to the optimum solution. Kanungo et al.
[2004] analyzed a very simple local search heuristic to get a polynomial-time
algorithm with approximation ratio $9+\epsilon$ for any fixed $\epsilon&gt;0$ for
$k$-means in Euclidean space.
  Finding an algorithm with a better worst-case approximation guarantee has
remained one of the biggest open questions in this area, in particular whether
one can get a true PTAS for fixed dimension Euclidean space. We settle this
problem by showing that a simple local search algorithm provides a PTAS for
$k$-means for $\mathbb{R}^d$ for any fixed $d$.
  More precisely, for any error parameter $\epsilon&gt;0$, the local search
algorithm that considers swaps of up to
$\rho=d^{O(d)}\cdot{\epsilon}^{-O(d/\epsilon)}$ centres will produce a solution
whose cost is at most $1+\epsilon$ times greater than the optimum cost. Our
analysis extends very easily to the more general setting where the metric has
fixed doubling dimension and to where we are interested in minimizing the sum
of the $q$-th powers of the distances for fixed $q$.
</summary>
    <author>
      <name>Zachary Friggstad</name>
    </author>
    <author>
      <name>Mohsen Rezapour</name>
    </author>
    <author>
      <name>Mohammad R. Salavatipour</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08869v1</id>
    <updated>2016-03-29T18:17:17Z</updated>
    <published>2016-03-29T18:17:17Z</published>
    <title>Algorithms for Batch Hierarchical Reinforcement Learning</title>
    <summary>  Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to
solve large Markov Decision Processes (MDP) and provide transferable subtask
policies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical
Q-value Iteration (HQI). We show that it is possible to effectively learn
recursive optimal policies for any valid hierarchical decomposition of the
original MDP, given a fixed dataset collected from a flat stochastic behavioral
policy. We first formally prove the convergence of the algorithm for tabular
MDP. Then our experiments on the Taxi domain show that HQI converges faster
than a flat Q-value Iteration and enjoys easy state abstraction. Also, we
demonstrate that our algorithm is able to learn optimal policies for different
hierarchical structures from the same fixed dataset, which enables model
comparison without recollecting data.
</summary>
    <author>
      <name>Tiancheng Zhao</name>
    </author>
    <author>
      <name>Mohammad Gowayyed</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08789v1</id>
    <updated>2016-03-29T14:29:00Z</updated>
    <published>2016-03-29T14:29:00Z</published>
    <title>Using Enthymemes to Fill the Gap between Logical Argumentation and
  Revision of Abstract Argumentation Frameworks</title>
    <summary>  In this paper, we present a preliminary work on an approach to fill the gap
between logic-based argumentation and the numerous approaches to tackle the
dynamics of abstract argumentation frameworks. Our idea is that, even when
arguments and attacks are defined by means of a logical belief base, there may
be some uncertainty about how accurate is the content of an argument, and so
the presence (or absence) of attacks concerning it. We use enthymemes to
illustrate this notion of uncertainty of arguments and attacks. Indeed, as
argued in the literature, real arguments are often enthymemes instead of
completely specified deductive arguments. This means that some parts of the
pair (support, claim) may be missing because they are supposed to belong to
some "common knowledge", and then should be deduced by the agent which receives
the enthymeme. But the perception that agents have of the common knowledge may
be wrong, and then a first agent may state an enthymeme that her opponent is
not able to decode in an accurate way. It is likely that the decoding of the
enthymeme by the agent leads to mistaken attacks between this new argument and
the existing ones. In this case, the agent can receive some information about
attacks or arguments acceptance statuses which disagree with her argumentation
framework. We exemplify a way to incorporate this new piece of information by
means of existing works on the dynamics of abstract argumentation frameworks.
</summary>
    <author>
      <name>Jean-Guy Mailly</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08785v3</id>
    <updated>2016-08-01T15:19:31Z</updated>
    <published>2016-03-29T14:18:52Z</published>
    <title>COCO: A Platform for Comparing Continuous Optimizers in a Black-Box
  Setting</title>
    <summary>  COCO is a platform for Comparing Continuous Optimizers in a black-box
setting. It aims at automatizing the tedious and repetitive task of
benchmarking numerical optimization algorithms to the greatest possible extent.
We present the rationals behind the development of the platform as a general
proposition for a guideline towards better benchmarking. We detail underlying
fundamental concepts of COCO such as its definition of a problem, the idea of
instances, the relevance of target values, and runtime as central performance
measure. Finally, we give a quick overview of the basic code structure and the
available test suites.
</summary>
    <author>
      <name>Nikolaus Hansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAO</arxiv:affiliation>
    </author>
    <author>
      <name>Anne Auger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAO</arxiv:affiliation>
    </author>
    <author>
      <name>Olaf Mersmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DOLPHIN</arxiv:affiliation>
    </author>
    <author>
      <name>Tea Tusar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DOLPHIN</arxiv:affiliation>
    </author>
    <author>
      <name>Dimo Brockhoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DOLPHIN</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ArXiv e-prints, arXiv:1603.08785</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08785v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08785v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08776v2</id>
    <updated>2016-05-19T11:58:22Z</updated>
    <published>2016-03-29T14:10:14Z</published>
    <title>COCO: The Experimental Procedure</title>
    <summary>  We present a budget-free experimental setup and procedure for benchmarking
numericaloptimization algorithms in a black-box scenario. This procedure can be
applied with the COCO benchmarking platform. We describe initialization of and
input to the algorithm and touch upon therelevance of termination and restarts.
</summary>
    <author>
      <name>Nikolaus Hansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Tea Tusar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Olaf Mersmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Anne Auger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Dimo Brockhoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ArXiv e-prints, arXiv:1603.08776</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08776v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08776v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08714v1</id>
    <updated>2016-03-29T10:37:38Z</updated>
    <published>2016-03-29T10:37:38Z</published>
    <title>Properties of ABA+ for Non-Monotonic Reasoning</title>
    <summary>  We investigate properties of ABA+, a formalism that extends the well studied
structured argumentation formalism Assumption-Based Argumentation (ABA) with a
preference handling mechanism. In particular, we establish desirable properties
that ABA+ semantics exhibit. These pave way to the satisfaction by ABA+ of some
(arguably) desirable principles of preference handling in argumentation and
non-monotonic reasoning, as well as non-monotonic inference properties of ABA+
under various semantics.
</summary>
    <author>
      <name>Kristijonas Cyras</name>
    </author>
    <author>
      <name>Francesca Toni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 16th International Workshop on Non-Monotonic
  Reasoning (NMR'16)</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08561v2</id>
    <updated>2016-07-26T17:26:01Z</updated>
    <published>2016-03-28T21:00:43Z</published>
    <title>Shuffle and Learn: Unsupervised Learning using Temporal Order
  Verification</title>
    <summary>  In this paper, we present an approach for learning a visual representation
from the raw spatiotemporal signals in videos. Our representation is learned
without supervision from semantic labels. We formulate our method as an
unsupervised sequential verification task, i.e., we determine whether a
sequence of frames from a video is in the correct temporal order. With this
simple task and no semantic labels, we learn a powerful visual representation
using a Convolutional Neural Network (CNN). The representation contains
complementary information to that learned from supervised image datasets like
ImageNet. Qualitative results show that our method captures information that is
temporally varying, such as human pose. When used as pre-training for action
recognition, our method gives significant gains over learning without external
data on benchmark datasets like UCF101 and HMDB51. To demonstrate its
sensitivity to human pose, we show results for pose estimation on the FLIC and
MPII datasets that are competitive, or better than approaches using
significantly more supervision. Our method can be combined with supervised
representations to provide an additional boost in accuracy.
</summary>
    <author>
      <name>Ishan Misra</name>
    </author>
    <author>
      <name>C. Lawrence Zitnick</name>
    </author>
    <author>
      <name>Martial Hebert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08507v1</id>
    <updated>2016-03-28T19:54:12Z</updated>
    <published>2016-03-28T19:54:12Z</published>
    <title>Generating Visual Explanations</title>
    <summary>  Clearly explaining a rationale for a classification decision to an end-user
can be as important as the decision itself. Existing approaches for deep visual
recognition are generally opaque and do not output any justification text;
contemporary vision-language models can describe image content but fail to take
into account class-discriminative image aspects which justify visual
predictions. We propose a new model that focuses on the discriminating
properties of the visible object, jointly predicts a class label, and explains
why the predicted label is appropriate for the image. We propose a novel loss
function based on sampling and reinforcement learning that learns to generate
sentences that realize a global sentence property, such as class specificity.
Our results on a fine-grained bird species classification dataset show that our
model is able to generate explanations which are not only consistent with an
image but also more discriminative than descriptions produced by existing
captioning methods.
</summary>
    <author>
      <name>Lisa Anne Hendricks</name>
    </author>
    <author>
      <name>Zeynep Akata</name>
    </author>
    <author>
      <name>Marcus Rohrbach</name>
    </author>
    <author>
      <name>Jeff Donahue</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08262v1</id>
    <updated>2016-03-27T22:01:59Z</updated>
    <published>2016-03-27T22:01:59Z</published>
    <title>Towards Machine Intelligence</title>
    <summary>  There exists a theory of a single general-purpose learning algorithm which
could explain the principles of its operation. This theory assumes that the
brain has some initial rough architecture, a small library of simple innate
circuits which are prewired at birth and proposes that all significant mental
algorithms can be learned. Given current understanding and observations, this
paper reviews and lists the ingredients of such an algorithm from both
architectural and functional perspectives.
</summary>
    <author>
      <name>Kamil Rocki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, submitted to AGI-16. arXiv admin note: substantial text
  overlap with arXiv:1512.01926</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08253v2</id>
    <updated>2016-03-29T11:10:33Z</updated>
    <published>2016-03-27T20:02:13Z</published>
    <title>Negative Learning Rates and P-Learning</title>
    <summary>  We present a method of training a differentiable function approximator for a
regression task using negative examples. We effect this training using negative
learning rates. We also show how this method can be used to perform direct
policy learning in a reinforcement learning setting.
</summary>
    <author>
      <name>Devon Merrill</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08253v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08253v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08079v1</id>
    <updated>2016-03-26T06:49:33Z</updated>
    <published>2016-03-26T06:49:33Z</published>
    <title>Do You See What I Mean? Visual Resolution of Linguistic Ambiguities</title>
    <summary>  Understanding language goes hand in hand with the ability to integrate
complex contextual information obtained via perception. In this work, we
present a novel task for grounded language understanding: disambiguating a
sentence given a visual scene which depicts one of the possible interpretations
of that sentence. To this end, we introduce a new multimodal corpus containing
ambiguous sentences, representing a wide range of syntactic, semantic and
discourse ambiguities, coupled with videos that visualize the different
interpretations for each sentence. We address this task by extending a vision
model which determines if a sentence is depicted by a video. We demonstrate how
such a model can be adjusted to recognize different interpretations of the same
underlying sentence, allowing to disambiguate sentences in a unified fashion
across the different ambiguity types.
</summary>
    <author>
      <name>Yevgeni Berzak</name>
    </author>
    <author>
      <name>Andrei Barbu</name>
    </author>
    <author>
      <name>Daniel Harari</name>
    </author>
    <author>
      <name>Boris Katz</name>
    </author>
    <author>
      <name>Shimon Ullman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Empirical Methods in Natural Language Processing
  (EMNLP), 2015, pages 1477--1487</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.08079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08023v1</id>
    <updated>2016-03-25T20:32:21Z</updated>
    <published>2016-03-25T20:32:21Z</published>
    <title>How NOT To Evaluate Your Dialogue System: An Empirical Study of
  Unsupervised Evaluation Metrics for Dialogue Response Generation</title>
    <summary>  We investigate evaluation metrics for end-to-end dialogue systems where
supervised labels, such as task completion, are not available. Recent works in
end-to-end dialogue systems have adopted metrics from machine translation and
text summarization to compare a model's generated response to a single target
response. We show that these metrics correlate very weakly or not at all with
human judgements of the response quality in both technical and non-technical
domains. We provide quantitative and qualitative results highlighting specific
weaknesses in existing metrics, and provide recommendations for future
development of better automatic evaluation metrics for dialogue systems.
</summary>
    <author>
      <name>Chia-Wei Liu</name>
    </author>
    <author>
      <name>Ryan Lowe</name>
    </author>
    <author>
      <name>Iulian V. Serban</name>
    </author>
    <author>
      <name>Michael Noseworthy</name>
    </author>
    <author>
      <name>Laurent Charlin</name>
    </author>
    <author>
      <name>Joelle Pineau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First 4 authors had equal contribution. 13 pages, 5 tables, 6
  figures. Submitted to ACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07886v1</id>
    <updated>2016-03-25T11:47:16Z</updated>
    <published>2016-03-25T11:47:16Z</published>
    <title>A Novel Biologically Mechanism-Based Visual Cognition Model--Automatic
  Extraction of Semantics, Formation of Integrated Concepts and Re-selection
  Features for Ambiguity</title>
    <summary>  Integration between biology and information science benefits both fields.
Many related models have been proposed, such as computational visual cognition
models, computational motor control models, integrations of both and so on. In
general, the robustness and precision of recognition is one of the key problems
for object recognition models.
  In this paper, inspired by features of human recognition process and their
biological mechanisms, a new integrated and dynamic framework is proposed to
mimic the semantic extraction, concept formation and feature re-selection in
human visual processing. The main contributions of the proposed model are as
follows:
  (1) Semantic feature extraction: Local semantic features are learnt from
episodic features that are extracted from raw images through a deep neural
network;
  (2) Integrated concept formation: Concepts are formed with local semantic
information and structural information learnt through network.
  (3) Feature re-selection: When ambiguity is detected during recognition
process, distinctive features according to the difference between ambiguous
candidates are re-selected for recognition.
  Experimental results on hand-written digits and facial shape dataset show
that, compared with other methods, the new proposed model exhibits higher
robustness and precision for visual recognition, especially in the condition
when input samples are smantic ambiguous. Meanwhile, the introduced biological
mechanisms further strengthen the interaction between neuroscience and
information science.
</summary>
    <author>
      <name>Peijie Yin</name>
    </author>
    <author>
      <name>Hong Qiao</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Lu Qi</name>
    </author>
    <author>
      <name>YinLin Li</name>
    </author>
    <author>
      <name>Shanlin Zhong</name>
    </author>
    <author>
      <name>Bo Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07810v1</id>
    <updated>2016-03-25T02:52:02Z</updated>
    <published>2016-03-25T02:52:02Z</published>
    <title>Disentangling Nonlinear Perceptual Embeddings With Multi-Query Triplet
  Networks</title>
    <summary>  In typical perceptual tasks, higher-order concepts are inferred from visual
features to assist with perceptual decision making. However, there is a
multitude of visual concepts which can be inferred from a single stimulus. When
learning nonlinear embeddings with siamese or triplet networks from
similarities, we typically assume they are sourced from a single visual
concept. In this paper, we are concerned with the hypothesis that it can be
potentially harmful to ignore the heterogeneity of concepts affiliated with
observed similarities when learning these embedding networks. We demonstrate
empirically that this hypothesis holds and suggest an approach that deals with
these shortcomings, by combining multiple notions of similarities in one
compact system. We propose Multi-Query Networks (MQNs) that leverage recent
advances in representation learning on factorized triplet embeddings in
combination with Convolutional Networks in order to learn embeddings
differentiated into semantically distinct subspaces, which are learned with a
latent space attention mechanism. We show that the resulting model learns
visually relevant semantic subspaces with features that do not only outperform
single triplet networks, but even sets of concept specific networks.
</summary>
    <author>
      <name>Andreas Veit</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <author>
      <name>Theofanis Karaletsos</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07704v2</id>
    <updated>2016-08-03T14:31:17Z</updated>
    <published>2016-03-24T18:54:18Z</published>
    <title>Probabilistic Reasoning via Deep Learning: Neural Association Models</title>
    <summary>  In this paper, we propose a new deep learning approach, called neural
association model (NAM), for probabilistic reasoning in artificial
intelligence. We propose to use neural networks to model association between
any two events in a domain. Neural networks take one event as input and compute
a conditional probability of the other event to model how likely these two
events are to be associated. The actual meaning of the conditional
probabilities varies between applications and depends on how the models are
trained. In this work, as two case studies, we have investigated two NAM
structures, namely deep neural networks (DNN) and relation-modulated neural
nets (RMNN), on several probabilistic reasoning tasks in AI, including
recognizing textual entailment, triple classification in multi-relational
knowledge bases and commonsense reasoning. Experimental results on several
popular datasets derived from WordNet, FreeBase and ConceptNet have all
demonstrated that both DNNs and RMNNs perform equally well and they can
significantly outperform the conventional methods available for these reasoning
tasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer,
where a pre-trained model can be quickly extended to an unseen relation after
observing only a few training samples. To further prove the effectiveness of
the proposed models, in this work, we have applied NAMs to solving challenging
Winograd Schema (WS) problems. Experiments conducted on a set of WS problems
prove that the proposed models have the potential for commonsense reasoning.
</summary>
    <author>
      <name>Quan Liu</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <author>
      <name>Andrew Evdokimov</name>
    </author>
    <author>
      <name>Zhen-Hua Ling</name>
    </author>
    <author>
      <name>Xiaodan Zhu</name>
    </author>
    <author>
      <name>Si Wei</name>
    </author>
    <author>
      <name>Yu Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Probabilistic reasoning, Winograd Schema Challenge, Deep learning,
  Neural Networks, Distributed Representation</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07704v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07704v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07453v1</id>
    <updated>2016-03-24T07:00:33Z</updated>
    <published>2016-03-24T07:00:33Z</published>
    <title>An Expressive Probabilistic Temporal Logic</title>
    <summary>  This paper argues that a combined treatment of probabilities, time and
actions is essential for an appropriate logical account of the notion of
probability; and, based on this intuition, describes an expressive
probabilistic temporal logic for reasoning about actions with uncertain
outcomes. The logic is modal and higher-order: modalities annotated by actions
are used to express possibility and necessity of propositions in the next
states resulting from the actions, and a higher-order function is needed to
express the probability operator. The proposed logic is shown to be an adequate
extension of classical mathematical probability theory, and its expressiveness
is illustrated through the formalization of the Monty Hall problem.
</summary>
    <author>
      <name>Bruno Woltzenlogel Paleo</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07442v2</id>
    <updated>2016-08-29T01:20:33Z</updated>
    <published>2016-03-24T05:20:59Z</published>
    <title>Pixel-Level Domain Transfer</title>
    <summary>  We present an image-conditional image generation model. The model transfers
an input domain to a target domain in semantic level, and generates the target
image in pixel level. To generate realistic target images, we employ the
real/fake-discriminator as in Generative Adversarial Nets, but also introduce a
novel domain-discriminator to make the generated image relevant to the input
image. We verify our model through a challenging task of generating a piece of
clothing from an input image of a dressed person. We present a high quality
clothing dataset containing the two domains, and succeed in demonstrating
decent results.
</summary>
    <author>
      <name>Donggeun Yoo</name>
    </author>
    <author>
      <name>Namil Kim</name>
    </author>
    <author>
      <name>Sunggyun Park</name>
    </author>
    <author>
      <name>Anthony S. Paek</name>
    </author>
    <author>
      <name>In So Kweon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The code and the dataset will be available soon</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07417v3</id>
    <updated>2016-08-30T16:32:57Z</updated>
    <published>2016-03-24T02:54:45Z</published>
    <title>Load Disaggregation Based on Aided Linear Integer Programming</title>
    <summary>  Load disaggregation based on aided linear integer programming (ALIP) is
proposed. We start with a conventional linear integer programming (IP) based
disaggregation and enhance it in several ways. The enhancements include
additional constraints, correction based on a state diagram, median filtering,
and linear programming-based refinement. With the aid of these enhancements,
the performance of IP-based disaggregation is significantly improved. The
proposed ALIP system relies only on the instantaneous load samples instead of
waveform signatures, and hence does not crucially depend on high sampling
frequency. Experimental results show that the proposed ALIP system performs
better than the conventional IP-based load disaggregation system.
</summary>
    <author>
      <name>Md. Zulfiquar Ali Bhotto</name>
    </author>
    <author>
      <name>Stephen Makonin</name>
    </author>
    <author>
      <name>Ivan V. Bajic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSII.2016.2603479</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSII.2016.2603479" rel="related"/>
    <link href="http://arxiv.org/abs/1603.07417v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07417v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07396v1</id>
    <updated>2016-03-24T00:02:58Z</updated>
    <published>2016-03-24T00:02:58Z</published>
    <title>A Diagram Is Worth A Dozen Images</title>
    <summary>  Diagrams are common tools for representing complex concepts, relationships
and events, often when it would be difficult to portray the same information
with natural images. Understanding natural images has been extensively studied
in computer vision, while diagram understanding has received little attention.
In this paper, we study the problem of diagram interpretation and reasoning,
the challenging task of identifying the structure of a diagram and the
semantics of its constituents and their relationships. We introduce Diagram
Parse Graphs (DPG) as our representation to model the structure of diagrams. We
define syntactic parsing of diagrams as learning to infer DPGs for diagrams and
study semantic interpretation and reasoning of diagrams in the context of
diagram question answering. We devise an LSTM-based method for syntactic
parsing of diagrams and introduce a DPG-based attention model for diagram
question answering. We compile a new dataset of diagrams with exhaustive
annotations of constituents and relationships for over 5,000 diagrams and
15,000 questions and answers. Our results show the significance of our models
for syntactic parsing and question answering in diagrams using DPGs.
</summary>
    <author>
      <name>Aniruddha Kembhavi</name>
    </author>
    <author>
      <name>Mike Salvato</name>
    </author>
    <author>
      <name>Eric Kolve</name>
    </author>
    <author>
      <name>Minjoon Seo</name>
    </author>
    <author>
      <name>Hannaneh Hajishirzi</name>
    </author>
    <author>
      <name>Ali Farhadi</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07294v2</id>
    <updated>2016-06-09T00:00:10Z</updated>
    <published>2016-03-23T18:31:05Z</published>
    <title>On the Theory and Practice of Privacy-Preserving Bayesian Data Analysis</title>
    <summary>  Bayesian inference has great promise for the privacy-preserving analysis of
sensitive data, as posterior sampling automatically preserves differential
privacy, an algorithmic notion of data privacy, under certain conditions
(Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample
(OPS) approach elegantly provides privacy "for free," it is data inefficient in
the sense of asymptotic relative efficiency (ARE). We show that a simple
alternative based on the Laplace mechanism, the workhorse of differential
privacy, is as asymptotically efficient as non-private posterior inference,
under general assumptions. This technique also has practical advantages
including efficient use of the privacy budget for MCMC. We demonstrate the
practicality of our approach on a time-series analysis of sensitive military
records from the Afghanistan and Iraq wars disclosed by the Wikileaks
organization.
</summary>
    <author>
      <name>James Foulds</name>
    </author>
    <author>
      <name>Joseph Geumlek</name>
    </author>
    <author>
      <name>Max Welling</name>
    </author>
    <author>
      <name>Kamalika Chaudhuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated to match the accepted UAI version. Generalized the ARE result
  and included a more detailed proof. Improved some figures, etc</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 32nd Conference on Uncertainty in Artificial
  Intelligence (UAI), 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.07294v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07294v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07292v1</id>
    <updated>2016-03-23T18:30:37Z</updated>
    <published>2016-03-23T18:30:37Z</published>
    <title>Debugging Machine Learning Tasks</title>
    <summary>  Unlike traditional programs (such as operating systems or word processors)
which have large amounts of code, machine learning tasks use programs with
relatively small amounts of code (written in machine learning libraries), but
voluminous amounts of data. Just like developers of traditional programs debug
errors in their code, developers of machine learning tasks debug and fix errors
in their data. However, algorithms and tools for debugging and fixing errors in
data are less common, when compared to their counterparts for detecting and
fixing errors in code. In this paper, we consider classification tasks where
errors in training data lead to misclassifications in test points, and propose
an automated method to find the root causes of such misclassifications. Our
root cause analysis is based on Pearl's theory of causation, and uses Pearl's
PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,
encodes the computation of PS as a probabilistic program, and uses recent work
on probabilistic programs and transformations on probabilistic programs (along
with gray-box models of machine learning algorithms) to efficiently compute PS.
Psi is able to identify root causes of data errors in interesting data sets.
</summary>
    <author>
      <name>Aleksandar Chakarov</name>
    </author>
    <author>
      <name>Aditya Nori</name>
    </author>
    <author>
      <name>Sriram Rajamani</name>
    </author>
    <author>
      <name>Shayak Sen</name>
    </author>
    <author>
      <name>Deepak Vijaykeerthy</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.5; I.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07051v1</id>
    <updated>2016-03-23T02:30:35Z</updated>
    <published>2016-03-23T02:30:35Z</published>
    <title>Cosolver2B: An Efficient Local Search Heuristic for the Travelling Thief
  Problem</title>
    <summary>  Real-world problems are very difficult to optimize. However, many researchers
have been solving benchmark problems that have been extensively investigated
for the last decades even if they have very few direct applications. The
Traveling Thief Problem (TTP) is a NP-hard optimization problem that aims to
provide a more realistic model. TTP targets particularly routing problem under
packing/loading constraints which can be found in supply chain management and
transportation. In this paper, TTP is presented and formulated mathematically.
A combined local search algorithm is proposed and compared with Random Local
Search (RLS) and Evolutionary Algorithm (EA). The obtained results are quite
promising since new better solutions were found.
</summary>
    <author>
      <name>Mohamed El Yafrani</name>
    </author>
    <author>
      <name>Belaïd Ahiod</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12th ACS/IEEE International Conference on Computer Systems and
  Applications (AICCSA) 2015. November 17-20, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07029v1</id>
    <updated>2016-03-22T23:36:02Z</updated>
    <published>2016-03-22T23:36:02Z</published>
    <title>Comparing Human and Automated Evaluation of Open-Ended Student Responses
  to Questions of Evolution</title>
    <summary>  Written responses can provide a wealth of data in understanding student
reasoning on a topic. Yet they are time- and labor-intensive to score,
requiring many instructors to forego them except as limited parts of summative
assessments at the end of a unit or course. Recent developments in Machine
Learning (ML) have produced computational methods of scoring written responses
for the presence or absence of specific concepts. Here, we compare the scores
from one particular ML program -- EvoGrader -- to human scoring of responses to
structurally- and content-similar questions that are distinct from the ones the
program was trained on. We find that there is substantial inter-rater
reliability between the human and ML scoring. However, sufficient systematic
differences remain between the human and ML scoring that we advise only using
the ML scoring for formative, rather than summative, assessment of student
reasoning.
</summary>
    <author>
      <name>Michael J Wiser</name>
    </author>
    <author>
      <name>Louise S Mead</name>
    </author>
    <author>
      <name>James J Smith</name>
    </author>
    <author>
      <name>Robert T Pennock</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ALife 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life XV: Proceedings of the Fifteenth International
  Conference on Artificial life. pp. 116 - 122. MIT Press. 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.07029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06881v1</id>
    <updated>2016-03-22T17:28:08Z</updated>
    <published>2016-03-22T17:28:08Z</published>
    <title>Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of
  Pairwise Comparisons</title>
    <summary>  We study methods for aggregating pairwise comparison data in order to
estimate outcome probabilities for future comparisons among a collection of n
items. Working within a flexible framework that imposes only a form of strong
stochastic transitivity (SST), we introduce an adaptivity index defined by the
indifference sets of the pairwise comparison probabilities. In addition to
measuring the usual worst-case risk of an estimator, this adaptivity index also
captures the extent to which the estimator adapts to instance-specific
difficulty relative to an oracle estimator. We prove three main results that
involve this adaptivity index and different algorithms. First, we propose a
three-step estimator termed Count-Randomize-Least squares (CRL), and show that
it has adaptivity index upper bounded as $\sqrt{n}$ up to logarithmic factors.
We then show that that conditional on the hardness of planted clique, no
computationally efficient estimator can achieve an adaptivity index smaller
than $\sqrt{n}$. Second, we show that a regularized least squares estimator can
achieve a poly-logarithmic adaptivity index, thereby demonstrating a
$\sqrt{n}$-gap between optimal and computationally achievable adaptivity.
Finally, we prove that the standard least squares estimator, which is known to
be optimally adaptive in several closely related problems, fails to adapt in
the context of estimating pairwise probabilities.
</summary>
    <author>
      <name>Nihar B. Shah</name>
    </author>
    <author>
      <name>Sivaraman Balakrishnan</name>
    </author>
    <author>
      <name>Martin J. Wainwright</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06807v2</id>
    <updated>2016-05-29T20:00:20Z</updated>
    <published>2016-03-22T14:25:16Z</published>
    <title>Generating Factoid Questions With Recurrent Neural Networks: The 30M
  Factoid Question-Answer Corpus</title>
    <summary>  Over the past decade, large-scale supervised learning corpora have enabled
machine learning researchers to make substantial advances. However, to this
date, there are no large-scale question-answer corpora available. In this paper
we present the 30M Factoid Question-Answer Corpus, an enormous question answer
pair corpus produced by applying a novel neural network architecture on the
knowledge base Freebase to transduce facts into natural language questions. The
produced question answer pairs are evaluated both by human evaluators and using
automatic evaluation metrics, including well-established machine translation
and sentence similarity metrics. Across all evaluation criteria the
question-generation model outperforms the competing template-based baseline.
Furthermore, when presented to human evaluators, the generated questions appear
comparable in quality to real human-generated questions.
</summary>
    <author>
      <name>Iulian Vlad Serban</name>
    </author>
    <author>
      <name>Alberto García-Durán</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>Sungjin Ahn</name>
    </author>
    <author>
      <name>Sarath Chandar</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.4; I.5.1; I.2.6; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06677v1</id>
    <updated>2016-03-22T05:07:16Z</updated>
    <published>2016-03-22T05:07:16Z</published>
    <title>Learning Executable Semantic Parsers for Natural Language Understanding</title>
    <summary>  For building question answering systems and natural language interfaces,
semantic parsing has emerged as an important and powerful paradigm. Semantic
parsers map natural language into logical forms, the classic representation for
many important linguistic phenomena. The modern twist is that we are interested
in learning semantic parsers from data, which introduces a new layer of
statistical and computational issues. This article lays out the components of a
statistical semantic parser, highlighting the key challenges. We will see that
semantic parsing is a rich fusion of the logical and the statistical world, and
that this fusion will play an integral role in the future of natural language
understanding systems.
</summary>
    <author>
      <name>Percy Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the Communications of the ACM</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06554v1</id>
    <updated>2016-03-21T19:38:07Z</updated>
    <published>2016-03-21T19:38:07Z</published>
    <title>Action-Affect Classification and Morphing using Multi-Task
  Representation Learning</title>
    <summary>  Most recent work focused on affect from facial expressions, and not as much
on body. This work focuses on body affect analysis. Affect does not occur in
isolation. Humans usually couple affect with an action in natural interactions;
for example, a person could be talking and smiling. Recognizing body affect in
sequences requires efficient algorithms to capture both the micro movements
that differentiate between happy and sad and the macro variations between
different actions. We depart from traditional approaches for time-series data
analytics by proposing a multi-task learning model that learns a shared
representation that is well-suited for action-affect classification as well as
generation. For this paper we choose Conditional Restricted Boltzmann Machines
to be our building block. We propose a new model that enhances the CRBM model
with a factored multi-task component to become Multi-Task Conditional
Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two
publicly available datasets, the Body Affect dataset and the Tower Game
dataset, and show superior classification performance improvement over the
state-of-the-art, as well as the generative abilities of our model.
</summary>
    <author>
      <name>Timothy J. Shields</name>
    </author>
    <author>
      <name>Mohamed R. Amer</name>
    </author>
    <author>
      <name>Max Ehrlich</name>
    </author>
    <author>
      <name>Amir Tamrakar</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06485v1</id>
    <updated>2016-03-21T16:34:13Z</updated>
    <published>2016-03-21T16:34:13Z</published>
    <title>A System for Probabilistic Linking of Thesauri and Classification
  Systems</title>
    <summary>  This paper presents a system which creates and visualizes probabilistic
semantic links between concepts in a thesaurus and classes in a classification
system. For creating the links, we build on the Polylingual Labeled Topic Model
(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in
the classification system by using information from the natural language text
of documents, their assigned thesaurus descriptors and their designated
classes. The links are then presented to users of the system in an interactive
visualization, providing them with an automatically generated overview of the
relations between the thesaurus and the classification system.
</summary>
    <author>
      <name>Lisa Posch</name>
    </author>
    <author>
      <name>Philipp Schaer</name>
    </author>
    <author>
      <name>Arnim Bleier</name>
    </author>
    <author>
      <name>Markus Strohmaier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13218-015-0413-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13218-015-0413-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">KI - K\"unstliche Intelligenz, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.06485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06393v3</id>
    <updated>2016-06-08T13:53:21Z</updated>
    <published>2016-03-21T11:35:08Z</published>
    <title>Incorporating Copying Mechanism in Sequence-to-Sequence Learning</title>
    <summary>  We address an important problem in sequence-to-sequence (Seq2Seq) learning
referred to as copying, in which certain segments in the input sequence are
selectively replicated in the output sequence. A similar phenomenon is
observable in human language communication. For example, humans tend to repeat
entity names or even long phrases in conversation. The challenge with regard to
copying in Seq2Seq is that new machinery is needed to decide when to perform
the operation. In this paper, we incorporate copying into neural network-based
Seq2Seq learning and propose a new model called CopyNet with encoder-decoder
structure. CopyNet can nicely integrate the regular way of word generation in
the decoder with the new copying mechanism which can choose sub-sequences in
the input sequence and put them at proper places in the output sequence. Our
empirical study on both synthetic data sets and real world data sets
demonstrates the efficacy of CopyNet. For example, CopyNet can outperform
regular RNN-based model with remarkable margins on text summarization tasks.
</summary>
    <author>
      <name>Jiatao Gu</name>
    </author>
    <author>
      <name>Zhengdong Lu</name>
    </author>
    <author>
      <name>Hang Li</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, accepted by ACL2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06318v3</id>
    <updated>2016-07-19T23:30:48Z</updated>
    <published>2016-03-21T03:33:20Z</published>
    <title>Harnessing Deep Neural Networks with Logic Rules</title>
    <summary>  Combining deep neural networks with structured logic rules is desirable to
harness flexibility and reduce uninterpretability of the neural models. We
propose a general framework capable of enhancing various types of neural
networks (e.g., CNNs and RNNs) with declarative first-order logic rules.
Specifically, we develop an iterative distillation method that transfers the
structured information of logic rules into the weights of neural networks. We
deploy the framework on a CNN for sentiment analysis, and an RNN for named
entity recognition. With a few highly intuitive rules, we obtain substantial
improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.
</summary>
    <author>
      <name>Zhiting Hu</name>
    </author>
    <author>
      <name>Xuezhe Ma</name>
    </author>
    <author>
      <name>Zhengzhong Liu</name>
    </author>
    <author>
      <name>Eduard Hovy</name>
    </author>
    <author>
      <name>Eric Xing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACL2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06318v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06318v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06288v1</id>
    <updated>2016-03-20T22:58:43Z</updated>
    <published>2016-03-20T22:58:43Z</published>
    <title>Multi-fidelity Gaussian Process Bandit Optimisation</title>
    <summary>  In many scientific and engineering applications, we are tasked with the
optimisation of an expensive to evaluate black box function $f$. Traditional
methods for this problem assume just the availability of this single function.
However, in many cases, cheap approximations to $f$ may be obtainable. For
example, the expensive real world behaviour of a robot can be approximated by a
cheap computer simulation. We can use these approximations to eliminate low
function value regions and use the expensive evaluations to $f$ in a small
promising region and speedily identify the optimum. We formalise this task as a
\emph{multi-fidelity} bandit problem where the target function and its
approximations are sampled from a Gaussian process. We develop a method based
on upper confidence bound techniques and prove that it exhibits precisely the
above behaviour, hence achieving better regret than strategies which ignore
multi-fidelity information. Our method outperforms such naive strategies on
several synthetic and real experiments.
</summary>
    <author>
      <name>Kirthevasan Kandasamy</name>
    </author>
    <author>
      <name>Gautam Dasarathy</name>
    </author>
    <author>
      <name>Junier B. Oliva</name>
    </author>
    <author>
      <name>Jeff Schneider</name>
    </author>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06217v1</id>
    <updated>2016-03-20T14:22:26Z</updated>
    <published>2016-03-20T14:22:26Z</published>
    <title>An Approximation Approach for Solving the Subpath Planning Problem</title>
    <summary>  The subpath planning problem is a branch of the path planning problem, which
has widespread applications in automated manufacturing process as well as
vehicle and robot navigation. This problem is to find the shortest path or tour
subject for travelling a set of given subpaths. The current approaches for
dealing with the subpath planning problem are all based on meta-heuristic
approaches. It is well-known that meta-heuristic based approaches have several
deficiencies. To address them, we propose a novel approximation algorithm in
the O(n^3) time complexity class, which guarantees to solve any subpath
planning problem instance with the fixed ratio bound of 2. Also, the formal
proofs of the claims, our empirical evaluation shows that our approximation
method acts much better than a state-of-the-art method, both in result and
execution time.
</summary>
    <author>
      <name>Masoud Safilian</name>
    </author>
    <author>
      <name>S. Mehdi Tashakkori</name>
    </author>
    <author>
      <name>Sepehr Eghbali</name>
    </author>
    <author>
      <name>Aliakbar Safilian</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06212v1</id>
    <updated>2016-03-20T13:32:27Z</updated>
    <published>2016-03-20T13:32:27Z</published>
    <title>Evaluation of a Tree-based Pipeline Optimization Tool for Automating
  Data Science</title>
    <summary>  As the field of data science continues to grow, there will be an
ever-increasing demand for tools that make machine learning accessible to
non-experts. In this paper, we introduce the concept of tree-based pipeline
optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement an open source Tree-based Pipeline
Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a
series of simulated and real-world benchmark data sets. In particular, we show
that TPOT can design machine learning pipelines that provide a significant
improvement over a basic machine learning analysis while requiring little to no
input nor prior knowledge from the user. We also address the tendency for TPOT
to design overly complex pipelines by integrating Pareto optimization, which
produces compact pipelines without sacrificing classification accuracy. As
such, this work represents an important step toward fully automating machine
learning pipeline design.
</summary>
    <author>
      <name>Randal S. Olson</name>
    </author>
    <author>
      <name>Nathan Bartley</name>
    </author>
    <author>
      <name>Ryan J. Urbanowicz</name>
    </author>
    <author>
      <name>Jason H. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, preprint to appear in GECCO 2016, edits not yet
  made from reviewer comments</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06143v1</id>
    <updated>2016-03-19T20:58:47Z</updated>
    <published>2016-03-19T20:58:47Z</published>
    <title>Neurally-Guided Procedural Models: Learning to Guide Procedural Models
  with Deep Neural Networks</title>
    <summary>  We present a deep learning approach for speeding up constrained procedural
modeling. Probabilistic inference algorithms such as Sequential Monte Carlo
(SMC) provide powerful tools for constraining procedural models, but they
require many samples to produce desirable results. In this paper, we show how
to create procedural models which learn how to satisfy constraints. We augment
procedural models with neural networks: these networks control how the model
makes random choices based on what output it has generated thus far. We call
such a model a neurally-guided procedural model. As a pre-computation, we train
these models on constraint-satisfying example outputs generated via SMC. They
are then used as efficient importance samplers for SMC, generating high-quality
results with very few samples. We evaluate our method on L-system-like models
with image-based constraints. Given a desired quality threshold,
neurally-guided models can generate satisfactory results up to 10x faster than
unguided models.
</summary>
    <author>
      <name>Daniel Ritchie</name>
    </author>
    <author>
      <name>Anna Thomas</name>
    </author>
    <author>
      <name>Pat Hanrahan</name>
    </author>
    <author>
      <name>Noah D. Goodman</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06141v1</id>
    <updated>2016-03-19T20:36:44Z</updated>
    <published>2016-03-19T20:36:44Z</published>
    <title>Evolving Shepherding Behavior with Genetic Programming Algorithms</title>
    <summary>  We apply genetic programming techniques to the `shepherding' problem, in
which a group of one type of animal (sheep dogs) attempts to control the
movements of a second group of animals (sheep) obeying flocking behavior. Our
genetic programming algorithm evolves an expression tree that governs the
movements of each dog. The operands of the tree are hand-selected features of
the simulation environment that may allow the dogs to herd the sheep
effectively. The algorithm uses tournament-style selection, crossover
reproduction, and a point mutation. We find that the evolved solutions
generalize well and outperform a (naive) human-designed algorithm.
</summary>
    <author>
      <name>Joshua Brulé</name>
    </author>
    <author>
      <name>Kevin Engel</name>
    </author>
    <author>
      <name>Nick Fung</name>
    </author>
    <author>
      <name>Isaac Julien</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06129v1</id>
    <updated>2016-03-19T18:43:28Z</updated>
    <published>2016-03-19T18:43:28Z</published>
    <title>Automated Correction for Syntax Errors in Programming Assignments using
  Recurrent Neural Networks</title>
    <summary>  We present a method for automatically generating repair feedback for syntax
errors for introductory programming problems. Syntax errors constitute one of
the largest classes of errors (34%) in our dataset of student submissions
obtained from a MOOC course on edX. The previous techniques for generating
automated feed- back on programming assignments have focused on functional
correctness and style considerations of student programs. These techniques
analyze the program AST of the program and then perform some dynamic and
symbolic analyses to compute repair feedback. Unfortunately, it is not possible
to generate ASTs for student pro- grams with syntax errors and therefore the
previous feedback techniques are not applicable in repairing syntax errors.
  We present a technique for providing feedback on syntax errors that uses
Recurrent neural networks (RNNs) to model syntactically valid token sequences.
Our approach is inspired from the recent work on learning language models from
Big Code (large code corpus). For a given programming assignment, we first
learn an RNN to model all valid token sequences using the set of syntactically
correct student submissions. Then, for a student submission with syntax errors,
we query the learnt RNN model with the prefix to- ken sequence to predict token
sequences that can fix the error by either replacing or inserting the predicted
token sequence at the error location. We evaluate our technique on over 14, 000
student submissions with syntax errors. Our technique can completely re- pair
31.69% (4501/14203) of submissions with syntax errors and in addition partially
correct 6.39% (908/14203) of the submissions.
</summary>
    <author>
      <name>Sahil Bhatia</name>
    </author>
    <author>
      <name>Rishabh Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06127v4</id>
    <updated>2016-05-17T14:08:38Z</updated>
    <published>2016-03-19T18:35:26Z</published>
    <title>Sentence Pair Scoring: Towards Unified Framework for Text Comprehension</title>
    <summary>  We review the task of Sentence Pair Scoring, popular in the literature in
various forms - viewed as Answer Sentence Selection, Semantic Text Scoring,
Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a
component of Memory Networks.
  We argue that all such tasks are similar from the model perspective and
propose new baselines by comparing the performance of common IR metrics and
popular convolutional, recurrent and attention-based neural models across many
Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating
randomized models, propose a statistically grounded methodology, and attempt to
improve comparisons by releasing new datasets that are much harder than some of
the currently used well explored benchmarks. We introduce a unified open source
software framework with easily pluggable models and tasks, which enables us to
experiment with multi-task reusability of trained sentence model. We set a new
state-of-art in performance on the Ubuntu Dialogue dataset.
</summary>
    <author>
      <name>Petr Baudiš</name>
    </author>
    <author>
      <name>Jan Pichl</name>
    </author>
    <author>
      <name>Tomáš Vyskočil</name>
    </author>
    <author>
      <name>Jan Šedivý</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted as paper to CoNLL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06127v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06127v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06125v1</id>
    <updated>2016-03-19T18:30:02Z</updated>
    <published>2016-03-19T18:30:02Z</published>
    <title>The Computational Power of Dynamic Bayesian Networks</title>
    <summary>  This paper considers the computational power of constant size, dynamic
Bayesian networks. Although discrete dynamic Bayesian networks are no more
powerful than hidden Markov models, dynamic Bayesian networks with continuous
random variables and discrete children of continuous parents are capable of
performing Turing-complete computation. With modified versions of existing
algorithms for belief propagation, such a simulation can be carried out in real
time. This result suggests that dynamic Bayesian networks may be more powerful
than previously considered. Relationships to causal models and recurrent neural
networks are also discussed.
</summary>
    <author>
      <name>Joshua Brulé</name>
    </author>
    <link href="http://arxiv.org/abs/1603.06125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06059v3</id>
    <updated>2016-06-09T01:20:49Z</updated>
    <published>2016-03-19T07:27:15Z</published>
    <title>Generating Natural Questions About an Image</title>
    <summary>  There has been an explosion of work in the vision &amp; language community during
the past few years from image captioning to video transcription, and answering
questions about images. These tasks have focused on literal descriptions of the
image. To move beyond the literal, we choose to explore how questions about an
image are often directed at commonsense inference and the abstract events
evoked by objects in the image. In this paper, we introduce the novel task of
Visual Question Generation (VQG), where the system is tasked with asking a
natural and engaging question when shown an image. We provide three datasets
which cover a variety of images from object-centric to event-centric, with
considerably more abstract training data than provided to state-of-the-art
captioning systems thus far. We train and test several generative and retrieval
models to tackle the task of VQG. Evaluation results show that while such
models ask reasonable questions for a variety of images, there is still a wide
gap with human performance which motivates further work on connecting images
with commonsense knowledge and pragmatics. Our proposed task offers a new
challenge to the community which we hope furthers interest in exploring deeper
connections between vision &amp; language.
</summary>
    <author>
      <name>Nasrin Mostafazadeh</name>
    </author>
    <author>
      <name>Ishan Misra</name>
    </author>
    <author>
      <name>Jacob Devlin</name>
    </author>
    <author>
      <name>Margaret Mitchell</name>
    </author>
    <author>
      <name>Xiaodong He</name>
    </author>
    <author>
      <name>Lucy Vanderwende</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 54th Annual Meeting of the Association for
  Computational Linguistics</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06059v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06059v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06015v1</id>
    <updated>2016-03-18T23:17:01Z</updated>
    <published>2016-03-18T23:17:01Z</published>
    <title>A Comprehensive Performance Evaluation of Deformable Face Tracking
  "In-the-Wild"</title>
    <summary>  Recently, technologies such as face detection, facial landmark localisation
and face recognition and verification have matured enough to provide effective
and efficient solutions for imagery captured under arbitrary conditions
(referred to as "in-the-wild"). This is partially attributed to the fact that
comprehensive "in-the-wild" benchmarks have been developed for face detection,
landmark localisation and recognition/verification. A very important technology
that has not been thoroughly evaluated yet is deformable face tracking
"in-the-wild". Until now, the performance has mainly been assessed
qualitatively by visually assessing the result of a deformable face tracking
technology on short videos. In this paper, we perform the first, to the best of
our knowledge, thorough evaluation of state-of-the-art deformable face tracking
pipelines using the recently introduced 300VW benchmark. We evaluate many
different architectures focusing mainly on the task of on-line deformable face
tracking. In particular, we compare the following general strategies: (a)
generic face detection plus generic facial landmark localisation, (b) generic
model free tracking plus generic facial landmark localisation, as well as (c)
hybrid approaches using state-of-the-art face detection, model free tracking
and facial landmark localisation technologies. Our evaluation reveals future
avenues for further research on the topic.
</summary>
    <author>
      <name>Grigorios G. Chrysos</name>
    </author>
    <author>
      <name>Epameinondas Antonakos</name>
    </author>
    <author>
      <name>Patrick Snape</name>
    </author>
    <author>
      <name>Akshay Asthana</name>
    </author>
    <author>
      <name>Stefanos Zafeiriou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">E. Antonakos and P. Snape contributed equally and have joint second
  authorship</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05959v2</id>
    <updated>2016-04-01T19:59:22Z</updated>
    <published>2016-03-18T19:07:01Z</published>
    <title>Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain
  Lesion Segmentation</title>
    <summary>  We propose a dual pathway, 11-layers deep, three-dimensional Convolutional
Neural Network for the challenging task of brain lesion segmentation. The
devised architecture is the result of an in-depth analysis of the limitations
of current networks proposed for similar applications. To overcome the
computational burden of processing 3D medical scans, we have devised an
efficient and effective dense training scheme which joins the processing of
adjacent image patches into one pass through the network while automatically
adapting to the inherent class imbalance present in the data. Further, we
analyze the development of deeper, thus more discriminative 3D CNNs. In order
to incorporate both local and larger contextual information, we employ a dual
pathway architecture that processes the input images at multiple scales
simultaneously. For post-processing of the network's soft segmentation, we use
a 3D fully connected Conditional Random Field which effectively removes false
positives. Our pipeline is extensively evaluated on three challenging tasks of
lesion segmentation in multi-channel MRI patient data with traumatic brain
injuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art
for all three applications, with top ranking performance on the public
benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient,
which allows its adoption in a variety of research and clinical settings. The
source code of our implementation is made publicly available.
</summary>
    <author>
      <name>Konstantinos Kamnitsas</name>
    </author>
    <author>
      <name>Christian Ledig</name>
    </author>
    <author>
      <name>Virginia F. J. Newcombe</name>
    </author>
    <author>
      <name>Joanna P. Simpson</name>
    </author>
    <author>
      <name>Andrew D. Kane</name>
    </author>
    <author>
      <name>David K. Menon</name>
    </author>
    <author>
      <name>Daniel Rueckert</name>
    </author>
    <author>
      <name>Ben Glocker</name>
    </author>
    <link href="http://arxiv.org/abs/1603.05959v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05959v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05670v1</id>
    <updated>2016-03-17T20:06:27Z</updated>
    <published>2016-03-17T20:06:27Z</published>
    <title>Bank distress in the news: Describing events through deep learning</title>
    <summary>  While many models are purposed for detecting the occurrence of events in
complex systems, the task of providing qualitative detail on the developments
is not usually as well automated. We present a deep learning approach for
detecting relevant discussion in text and extracting natural language
descriptions of events. Supervised by only a small set of event information,
the model is leveraged by unsupervised learning of semantic vector
representations on extensive text data. We demonstrate applicability to the
study of financial risk based on news (6.6M articles), particularly bank
distress and government interventions (243 events), where indices can signal
the level of bank-stress-related reporting at the entity level, or aggregated
at country or European level, while being coupled with explanations. Thus, we
exemplify how text, as timely and widely available data, can serve as a useful
complementary source of information for financial risk analytics.
</summary>
    <author>
      <name>Samuel Rönnqvist</name>
    </author>
    <author>
      <name>Peter Sarlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1507.07870</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.05670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05594v1</id>
    <updated>2016-03-17T17:58:48Z</updated>
    <published>2016-03-17T17:58:48Z</published>
    <title>Mapping Temporal Variables into the NeuCube for Improved Pattern
  Recognition, Predictive Modelling and Understanding of Stream Data</title>
    <summary>  This paper proposes a new method for an optimized mapping of temporal
variables, describing a temporal stream data, into the recently proposed
NeuCube spiking neural network architecture. This optimized mapping extends the
use of the NeuCube, which was initially designed for spatiotemporal brain data,
to work on arbitrary stream data and to achieve a better accuracy of temporal
pattern recognition, a better and earlier event prediction and a better
understanding of complex temporal stream data through visualization of the
NeuCube connectivity. The effect of the new mapping is demonstrated on three
bench mark problems. The first one is early prediction of patient sleep stage
event from temporal physiological data. The second one is pattern recognition
of dynamic temporal patterns of traffic in the Bay Area of California and the
last one is the Challenge 2012 contest data set. In all cases the use of the
proposed mapping leads to an improved accuracy of pattern recognition and event
prediction and a better understanding of the data when compared to traditional
machine learning techniques or spiking neural network reservoirs with arbitrary
mapping of the variables.
</summary>
    <author>
      <name>Enmei Tu</name>
    </author>
    <author>
      <name>Nikola Kasabov</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE TNNLS</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.05594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05474v1</id>
    <updated>2016-03-17T13:30:45Z</updated>
    <published>2016-03-17T13:30:45Z</published>
    <title>Neural Aggregation Network for Video Face Recognition</title>
    <summary>  In this paper, we present a Neural Aggregation Network (NAN) for video face
recognition. The network takes a face video or face image set of a person with
variable number of face frames as its input, and produces a compact and
fixed-dimension visual representation of that person. The whole network is
composed of two modules. The feature embedding module is a CNN which maps each
face frame into a feature representation. The neural aggregation module is
composed of two content based attention blocks which is driven by a memory
storing all the features extracted from the face video through the feature
embedding module. The output of the first attention block adapts the second,
whose output is adopted as the aggregated representation of the video faces.
Due to the attention mechanism, this representation is invariant to the order
of the face frames. The experiments show that the proposed NAN consistently
outperforms hand-crafted aggregations such as average pooling, and achieves
state-of-the-art accuracy on three video face recognition datasets: the YouTube
Face, IJB-A and Celebrity-1000 datasets.
</summary>
    <author>
      <name>Jiaolong Yang</name>
    </author>
    <author>
      <name>Peiran Ren</name>
    </author>
    <author>
      <name>Dong Chen</name>
    </author>
    <author>
      <name>Fang Wen</name>
    </author>
    <author>
      <name>Hongdong Li</name>
    </author>
    <author>
      <name>Gang Hua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TR</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.05474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05314v1</id>
    <updated>2016-03-16T23:57:27Z</updated>
    <published>2016-03-16T23:57:27Z</published>
    <title>Hardware Acceleration for Boolean Satisfiability Solver by Applying
  Belief Propagation Algorithm</title>
    <summary>  Boolean satisfiability (SAT) has an extensive application domain in computer
science, especially in electronic design automation applications. Circuit
synthesis, optimization, and verification problems can be solved by
transforming original problems to SAT problems. However, the SAT problem is
known as NP-complete, which means there is no efficient method to solve it.
Therefore, an efficient SAT solver to enhance the performance is always
desired. We propose a hardware acceleration method for SAT problems. By
surveying the properties of SAT problems and the decoding of low-density
parity-check (LDPC) codes, a special class of error-correcting codes, we
discover that both of them are constraint satisfaction problems. The belief
propagation algorithm has been successfully applied to the decoding of LDPC,
and the corresponding decoder hardware designs are extensively studied.
Therefore, we proposed a belief propagation based algorithm to solve SAT
problems. With this algorithm, the SAT solver can be accelerated by hardware. A
software simulator is implemented to verify the proposed algorithm and the
performance improvement is estimated. Our experiment results show that time
complexity does not increase with the size of SAT problems and the proposed
method can achieve at least 30x speedup compared to MiniSat.
</summary>
    <author>
      <name>Te-Hsuan Chen</name>
    </author>
    <author>
      <name>Ju-Yi Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1603.05314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05145v1</id>
    <updated>2016-03-16T15:35:07Z</updated>
    <published>2016-03-16T15:35:07Z</published>
    <title>Suppressing the Unusual: towards Robust CNNs using Symmetric Activation
  Functions</title>
    <summary>  Many deep Convolutional Neural Networks (CNN) make incorrect predictions on
adversarial samples obtained by imperceptible perturbations of clean samples.
We hypothesize that this is caused by a failure to suppress unusual signals
within network layers. As remedy we propose the use of Symmetric Activation
Functions (SAF) in non-linear signal transducer units. These units suppress
signals of exceptional magnitude. We prove that SAF networks can perform
classification tasks to arbitrary precision in a simplified situation. In
practice, rather than use SAFs alone, we add them into CNNs to improve their
robustness. The modified CNNs can be easily trained using popular strategies
with the moderate training load. Our experiments on MNIST and CIFAR-10 show
that the modified CNNs perform similarly to plain ones on clean samples, and
are remarkably more robust against adversarial and nonsense samples.
</summary>
    <author>
      <name>Qiyang Zhao</name>
    </author>
    <author>
      <name>Lewis D Griffin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.05145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05106v2</id>
    <updated>2016-05-25T12:57:19Z</updated>
    <published>2016-03-16T14:10:00Z</published>
    <title>One-Shot Generalization in Deep Generative Models</title>
    <summary>  Humans have an impressive ability to reason about new concepts and
experiences from just a single example. In particular, humans have an ability
for one-shot generalization: an ability to encounter a new concept, understand
its structure, and then be able to generate compelling alternative variations
of the concept. We develop machine learning systems with this important
capacity by developing new deep generative models, models that combine the
representational power of deep learning with the inferential power of Bayesian
reasoning. We develop a class of sequential generative models that are built on
the principles of feedback and attention. These two characteristics lead to
generative models that are among the state-of-the art in density estimation and
image generation. We demonstrate the one-shot generalization ability of our
models using three tasks: unconditional sampling, generating new exemplars of a
given concept, and generating new exemplars of a family of concepts. In all
cases our models are able to generate compelling and diverse samples---having
seen new examples just once---providing an important class of general-purpose
models for one-shot machine learning.
</summary>
    <author>
      <name>Danilo Jimenez Rezende</name>
    </author>
    <author>
      <name>Shakir Mohamed</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8pgs, 1pg references, 1pg appendix, In Proceedings of the 33rd
  International Conference on Machine Learning, JMLR: W&amp;CP volume 48, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.05106v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05106v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00921v1</id>
    <updated>2016-03-16T11:00:23Z</updated>
    <published>2016-03-16T11:00:23Z</published>
    <title>A Review of Theoretical and Practical Challenges of Trusted Autonomy in
  Big Data</title>
    <summary>  Despite the advances made in artificial intelligence, software agents, and
robotics, there is little we see today that we can truly call a fully
autonomous system. We conjecture that the main inhibitor for advancing autonomy
is lack of trust. Trusted autonomy is the scientific and engineering field to
establish the foundations and ground work for developing trusted autonomous
systems (robotics and software agents) that can be used in our daily life, and
can be integrated with humans seamlessly, naturally and efficiently.
  In this paper, we review this literature to reveal opportunities for
researchers and practitioners to work on topics that can create a leap forward
in advancing the field of trusted autonomy. We focus the paper on the `trust'
component as the uniting technology between humans and machines. Our inquiry
into this topic revolves around three sub-topics: (1) reviewing and positioning
the trust modelling literature for the purpose of trusted autonomy; (2)
reviewing a critical subset of sensor technologies that allow a machine to
sense human states; and (3) distilling some critical questions for advancing
the field of trusted autonomy. The inquiry is augmented with conceptual models
that we propose along the way by recompiling and reshaping the literature into
forms that enables trusted autonomous systems to become a reality. The paper
offers a vision for a Trusted Cyborg Swarm, an extension of our previous
Cognitive Cyber Symbiosis concept, whereby humans and machines meld together in
a harmonious, seamless, and coordinated manner.
</summary>
    <author>
      <name>Hussein A. Abbass</name>
    </author>
    <author>
      <name>George Leu</name>
    </author>
    <author>
      <name>Kathryn Merrick</name>
    </author>
    <link href="http://arxiv.org/abs/1604.00921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04586v1</id>
    <updated>2016-03-15T08:12:52Z</updated>
    <published>2016-03-15T08:12:52Z</published>
    <title>Optimal Sensing via Multi-armed Bandit Relaxations in Mixed
  Observability Domains</title>
    <summary>  Sequential decision making under uncertainty is studied in a mixed
observability domain. The goal is to maximize the amount of information
obtained on a partially observable stochastic process under constraints imposed
by a fully observable internal state. An upper bound for the optimal value
function is derived by relaxing constraints. We identify conditions under which
the relaxed problem is a multi-armed bandit whose optimal policy is easily
computable. The upper bound is applied to prune the search space in the
original problem, and the effect on solution quality is assessed via simulation
experiments. Empirical results show effective pruning of the search space in a
target monitoring domain.
</summary>
    <author>
      <name>Mikko Lauri</name>
    </author>
    <author>
      <name>Risto Ritala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICRA.2015.7139867</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICRA.2015.7139867" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE Intl. Conf. on Robotics and Automation (ICRA), pp.
  4807-4812, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.04586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04535v1</id>
    <updated>2016-03-15T02:56:22Z</updated>
    <published>2016-03-15T02:56:22Z</published>
    <title>Domain Adaptation via Maximum Independence of Domain Features</title>
    <summary>  When the distributions of the source and the target domains are different,
domain adaptation techniques are needed. For example, in the field of sensors
and measurement, discrete and continuous distributional change often exist in
data because of instrumental variation and time-varying sensor drift. In this
paper, we propose maximum independence domain adaptation (MIDA) to address this
problem. Domain features are first defined to describe the background
information of a sample, such as the device label and acquisition time. Then,
MIDA learns features which have maximal independence with the domain features,
so as to reduce the inter-domain discrepancy in distributions. A feature
augmentation strategy is designed so that the learned projection is
background-specific. Semi-supervised MIDA (SMIDA) extends MIDA by exploiting
the label information. The proposed methods can handle not only discrete
domains in traditional domain adaptation problems but also continuous
distributional change such as the time-varying drift. In addition, they are
naturally applicable in supervised/semi-supervised/unsupervised classification
or regression problems with multiple domains. This flexibility brings potential
for a wide range of applications. The effectiveness of our approaches is
verified by experiments on synthetic datasets and four real-world ones on
sensors, measurement, and computer vision.
</summary>
    <author>
      <name>Ke Yan</name>
    </author>
    <author>
      <name>Lu Kou</name>
    </author>
    <author>
      <name>David Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 9 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04466v1</id>
    <updated>2016-03-14T20:48:43Z</updated>
    <published>2016-03-14T20:48:43Z</published>
    <title>Sequential Voting Promotes Collective Discovery in Social Recommendation
  Systems</title>
    <summary>  One goal of online social recommendation systems is to harness the wisdom of
crowds in order to identify high quality content. Yet the sequential voting
mechanisms that are commonly used by these systems are at odds with existing
theoretical and empirical literature on optimal aggregation. This literature
suggests that sequential voting will promote herding---the tendency for
individuals to copy the decisions of others around them---and hence lead to
suboptimal content recommendation. Is there a problem with our practice, or a
problem with our theory? Previous attempts at answering this question have been
limited by a lack of objective measurements of content quality. Quality is
typically defined endogenously as the popularity of content in absence of
social influence. The flaw of this metric is its presupposition that the
preferences of the crowd are aligned with underlying quality. Domains in which
content quality can be defined exogenously and measured objectively are thus
needed in order to better assess the design choices of social recommendation
systems. In this work, we look to the domain of education, where content
quality can be measured via how well students are able to learn from the
material presented to them. Through a behavioral experiment involving a
simulated massive open online course (MOOC) run on Amazon Mechanical Turk, we
show that sequential voting systems can surface better content than systems
that elicit independent votes.
</summary>
    <author>
      <name>L. Elisa Celis</name>
    </author>
    <author>
      <name>Peter M. Krafft</name>
    </author>
    <author>
      <name>Nathan Kobe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the 10th International AAAI Conference on Web and
  Social Media (ICWSM) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04402v1</id>
    <updated>2016-03-14T19:20:36Z</updated>
    <published>2016-03-14T19:20:36Z</published>
    <title>Controlling Search in Very large Commonsense Knowledge Bases: A Machine
  Learning Approach</title>
    <summary>  Very large commonsense knowledge bases (KBs) often have thousands to millions
of axioms, of which relatively few are relevant for answering any given query.
A large number of irrelevant axioms can easily overwhelm resolution-based
theorem provers. Therefore, methods that help the reasoner identify useful
inference paths form an essential part of large-scale reasoning systems. In
this paper, we describe two ordering heuristics for optimization of reasoning
in such systems. First, we discuss how decision trees can be used to select
inference steps that are more likely to succeed. Second, we identify a small
set of problem instance features that suffice to guide searches away from
intractable regions of the search space. We show the efficacy of these
techniques via experiments on thousands of queries from the Cyc KB. Results
show that these methods lead to an order of magnitude reduction in inference
time.
</summary>
    <author>
      <name>Abhishek Sharma</name>
    </author>
    <author>
      <name>Michael Witbrock</name>
    </author>
    <author>
      <name>Keith Goolsbey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04319v1</id>
    <updated>2016-03-14T16:08:26Z</updated>
    <published>2016-03-14T16:08:26Z</published>
    <title>Learning Network of Multivariate Hawkes Processes: A Time Series
  Approach</title>
    <summary>  Learning the influence structure of multiple time series data is of great
interest to many disciplines. This paper studies the problem of recovering the
causal structure in network of multivariate linear Hawkes processes. In such
processes, the occurrence of an event in one process affects the probability of
occurrence of new events in some other processes. Thus, a natural notion of
causality exists between such processes captured by the support of the
excitation matrix. We show that the resulting causal influence network is
equivalent to the Directed Information graph (DIG) of the processes, which
encodes the causal factorization of the joint distribution of the processes.
Furthermore, we present an algorithm for learning the support of excitation
matrix (or equivalently the DIG). The performance of the algorithm is evaluated
on synthesized multivariate Hawkes networks as well as a stock market and
MemeTracker real-world dataset.
</summary>
    <author>
      <name>Jalal Etesami</name>
    </author>
    <author>
      <name>Negar Kiyavash</name>
    </author>
    <author>
      <name>Kun Zhang</name>
    </author>
    <author>
      <name>Kushagra Singhal</name>
    </author>
    <link href="http://arxiv.org/abs/1603.04319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04259v2</id>
    <updated>2016-03-19T13:45:53Z</updated>
    <published>2016-03-14T13:37:03Z</published>
    <title>Item2Vec: Neural Item Embedding for Collaborative Filtering</title>
    <summary>  Many Collaborative Filtering (CF) algorithms are item-based in the sense that
they analyze item-item relations in order to produce item similarities.
Recently, several works in the field of Natural Language Processing suggested
to learn a latent representation of words using neural embedding algorithms.
Among them, the Skip-gram with Negative Sampling (SGNS), also known as
Word2Vec, was shown to provide state-of-the-art results on various linguistics
tasks. In this paper, we show that item-based CF can be cast in the same
framework of neural word embedding. Inspired by SGNS, we describe a method we
name Item2Vec for item-based CF that produces embedding for items in a latent
space. The method is capable of inferring item-to-item relations even when user
information is not available. We present experimental results on large scale
datasets that demonstrate the effectiveness of the Item2Vec method and show it
is competitive with SVD.
</summary>
    <author>
      <name>Oren Barkan</name>
    </author>
    <author>
      <name>Noam Koenigstein</name>
    </author>
    <link href="http://arxiv.org/abs/1603.04259v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04259v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02416v2</id>
    <updated>2016-06-21T04:51:22Z</updated>
    <published>2016-03-14T04:20:55Z</published>
    <title>How deep is knowledge tracing?</title>
    <summary>  In theoretical cognitive science, there is a tension between highly
structured models whose parameters have a direct psychological interpretation
and highly complex, general-purpose models whose parameters and representations
are difficult to interpret. The former typically provide more insight into
cognition but the latter often perform better. This tension has recently
surfaced in the realm of educational data mining, where a deep learning
approach to predicting students' performance as they work through a series of
exercises---termed deep knowledge tracing or DKT---has demonstrated a stunning
performance advantage over the mainstay of the field, Bayesian knowledge
tracing or BKT. In this article, we attempt to understand the basis for DKT's
advantage by considering the sources of statistical regularity in the data that
DKT can leverage but which BKT cannot. We hypothesize four forms of regularity
that BKT fails to exploit: recency effects, the contextualized trial sequence,
inter-skill similarity, and individual variation in ability. We demonstrate
that when BKT is extended to allow it more flexibility in modeling statistical
regularities---using extensions previously proposed in the literature---BKT
achieves a level of performance indistinguishable from that of DKT. We argue
that while DKT is a powerful, useful, general-purpose framework for modeling
student learning, its gains do not come from the discovery of novel
representations---the fundamental advantage of deep learning. To answer the
question posed in our title, knowledge tracing may be a domain that does not
require `depth'; shallow models like BKT can perform just as well and offer us
greater interpretability and explanatory power.
</summary>
    <author>
      <name>Mohammad Khajah</name>
    </author>
    <author>
      <name>Robert V. Lindsey</name>
    </author>
    <author>
      <name>Michael C. Mozer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02416v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02416v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04119v1</id>
    <updated>2016-03-14T03:16:25Z</updated>
    <published>2016-03-14T03:16:25Z</published>
    <title>Exploratory Gradient Boosting for Reinforcement Learning in Complex
  Domains</title>
    <summary>  High-dimensional observations and complex real-world dynamics present major
challenges in reinforcement learning for both function approximation and
exploration. We address both of these challenges with two complementary
techniques: First, we develop a gradient-boosting style, non-parametric
function approximator for learning on $Q$-function residuals. And second, we
propose an exploration strategy inspired by the principles of state abstraction
and information acquisition under uncertainty. We demonstrate the empirical
effectiveness of these techniques, first, as a preliminary check, on two
standard tasks (Blackjack and $n$-Chain), and then on two much larger and more
realistic tasks with high-dimensional observation spaces. Specifically, we
introduce two benchmarks built within the game Minecraft where the observations
are pixel arrays of the agent's visual field. A combination of our two
algorithmic techniques performs competitively on the standard
reinforcement-learning tasks while consistently and substantially outperforming
baselines on the two tasks with high-dimensional observation spaces. The new
function approximator, exploration strategy, and evaluation benchmarks are each
of independent interest in the pursuit of reinforcement-learning methods that
scale to real-world domains.
</summary>
    <author>
      <name>David Abel</name>
    </author>
    <author>
      <name>Alekh Agarwal</name>
    </author>
    <author>
      <name>Fernando Diaz</name>
    </author>
    <author>
      <name>Akshay Krishnamurthy</name>
    </author>
    <author>
      <name>Robert E. Schapire</name>
    </author>
    <link href="http://arxiv.org/abs/1603.04119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04118v2</id>
    <updated>2016-06-22T16:48:58Z</updated>
    <published>2016-03-14T03:08:24Z</published>
    <title>Active Algorithms For Preference Learning Problems with Multiple
  Populations</title>
    <summary>  In this paper we model the problem of learning preferences of a population as
an active learning problem. We propose an algorithm can adaptively choose pairs
of items to show to users coming from a heterogeneous population, and use the
obtained reward to decide which pair of items to show next. We provide
computationally efficient algorithms with provable sample complexity guarantees
for this problem in both the noiseless and noisy cases. In the process of
establishing sample complexity guarantees for our algorithms, we establish new
results using a Nystr{\"o}m-like method which can be of independent interest.
We supplement our theoretical results with experimental comparisons.
</summary>
    <author>
      <name>Aniruddha Bhargava</name>
    </author>
    <author>
      <name>Ravi Ganti</name>
    </author>
    <author>
      <name>Robert Nowak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04118v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04118v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04110v2</id>
    <updated>2016-05-16T20:24:07Z</updated>
    <published>2016-03-14T01:52:28Z</published>
    <title>Geometry of Interest (GOI): Spatio-Temporal Destination Extraction and
  Partitioning in GPS Trajectory Data</title>
    <summary>  Nowadays large amounts of GPS trajectory data is being continuously collected
by GPS-enabled devices such as vehicles navigation systems and mobile phones.
GPS trajectory data is useful for applications such as traffic management,
location forecasting, and itinerary planning. Such applications often need to
extract the time-stamped Sequence of Visited Locations (SVLs) of the mobile
objects. The nearest neighbor query (NNQ) is the most applied method for
labeling the visited locations based on the IDs of the POIs in the process of
SVL generation. NNQ in some scenarios is not accurate enough. To improve the
quality of the extracted SVLs, instead of using NNQ, we label the visited
locations as the IDs of the POIs which geometrically intersect with the GPS
observations. Intersection operator requires the accurate geometry of the
points of interest which we refer to them as the Geometries of Interest (GOIs).
In some application domains (e.g. movement trajectories of animals), adequate
information about the POIs and their GOIs may not be available a priori, or
they may not be publicly accessible and, therefore, they need to be derived
from GPS trajectory data. In this paper we propose a novel method for
estimating the POIs and their GOIs, which consists of three phases: (i)
extracting the geometries of the stay regions; (ii) constructing the geometry
of destination regions based on the extracted stay regions; and (iii)
constructing the GOIs based on the geometries of the destination regions. Using
the geometric similarity to known GOIs as the major evaluation criterion, the
experiments we performed using long-term GPS trajectory data show that our
method outperforms the existing approaches.
</summary>
    <author>
      <name>Seyed Morteza Mousavi</name>
    </author>
    <author>
      <name>Aaron Harwood</name>
    </author>
    <author>
      <name>Shanika Karunasekera</name>
    </author>
    <author>
      <name>Mojtaba Maghrebi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A version of this technical report has been submitted to the Springer
  Journal of Ambient Intelligence and Humanized Computing and it is under
  review</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04110v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04110v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04068v2</id>
    <updated>2016-06-22T14:18:52Z</updated>
    <published>2016-03-13T19:28:22Z</published>
    <title>A Signaling Game Approach to Databases Querying and Interaction</title>
    <summary>  As most database users cannot precisely express their information needs, it
is challenging for database management systems to understand them. We propose a
novel formal framework for representing and understanding information needs in
database querying and exploration. Our framework considers querying as a
collaboration between the user and the database management system to establish
a it mutual language for representing information needs. We formalize this
collaboration as a signaling game, where each mutual language is an equilibrium
for the game. A query interface is more effective if it establishes a less
ambiguous mutual language faster. We discuss some equilibria, strategies, and
the convergence in this game. In particular, we propose a reinforcement
learning mechanism and analyze it within our framework. We prove that this
adaptation mechanism for the query interface improves the effectiveness of
answering queries stochastically speaking, and converges almost surely. We
extend out results for the cases that the user also modifies her strategy
during the interaction.
</summary>
    <author>
      <name>Ben McCamish</name>
    </author>
    <author>
      <name>Vinod Ramaswamy</name>
    </author>
    <author>
      <name>Arash Termehchy</name>
    </author>
    <author>
      <name>Behrouz Touri</name>
    </author>
    <author>
      <name>Eric Frew</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04068v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04068v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03980v1</id>
    <updated>2016-03-13T01:53:40Z</updated>
    <published>2016-03-13T01:53:40Z</published>
    <title>On Learning High Dimensional Structured Single Index Models</title>
    <summary>  Single Index Models (SIMs) are simple yet flexible semi-parametric models for
classification and regression, where response variables are modeled as a
nonlinear, monotonic function of a linear combination of features. Estimation
in this context requires learning both the feature weights and the nonlinear
function that relates features to observations. While methods have been
described to learn SIMs in the low dimensional regime, a method that can
efficiently learn SIMs in high dimensions, and under general structural
assumptions, has not been forthcoming. In this paper, we propose
computationally efficient algorithms for SIM inference in high dimensions using
atomic norm regularization. This general approach to imposing structure in
high-dimensional modeling specializes to sparsity, group sparsity, and low-rank
assumptions among others. We also provide a scalable, stochastic version of the
method. Experiments show that the method we propose enjoys superior predictive
performance when compared to generalized linear models such as logistic
regression, on several real-world datasets.
</summary>
    <author>
      <name>Nikhil Rao</name>
    </author>
    <author>
      <name>Ravi Ganti</name>
    </author>
    <author>
      <name>Laura Balzano</name>
    </author>
    <author>
      <name>Rebecca Willett</name>
    </author>
    <author>
      <name>Robert Nowak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 tables, 2 Figures, substantial text overlap with
  arXiv:1506.08910</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06459v1</id>
    <updated>2016-03-12T12:38:32Z</updated>
    <published>2016-03-12T12:38:32Z</published>
    <title>Characterization of neighborhood behaviours in a multi-neighborhood
  local search algorithm</title>
    <summary>  We consider a multi-neighborhood local search algorithm with a large number
of possible neighborhoods. Each neighborhood is accompanied by a weight value
which represents the probability of being chosen at each iteration. These
weights are fixed before the algorithm runs, and are considered as parameters
of the algorithm. Given a set of instances, off-line tuning of the algorithm's
parameters can be done by automated algorithm configuration tools (e.g., SMAC).
However, the large number of neighborhoods can make the tuning expensive and
difficult even when the number of parameters has been reduced by some
intuition. In this work, we propose a systematic method to characterize each
neighborhood's behaviours, representing them as a feature vector, and using
cluster analysis to form similar groups of neighborhoods. The novelty of our
characterization method is the ability of reflecting changes of behaviours
according to hardness of different solution quality regions. We show that using
neighborhood clusters instead of individual neighborhoods helps to reduce the
parameter configuration space without misleading the search of the tuning
procedure. Moreover, this method is problem-independent and potentially can be
applied in similar contexts.
</summary>
    <author>
      <name>Nguyen Thi Thanh Dang</name>
    </author>
    <author>
      <name>Patrick De Causmaecker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03884v1</id>
    <updated>2016-03-12T10:22:13Z</updated>
    <published>2016-03-12T10:22:13Z</published>
    <title>Grounding Recursive Aggregates: Preliminary Report</title>
    <summary>  Problem solving in Answer Set Programming consists of two steps, a first
grounding phase, systematically replacing all variables by terms, and a second
solving phase computing the stable models of the obtained ground program. An
intricate part of both phases is the treatment of aggregates, which are popular
language constructs that allow for expressing properties over sets. In this
paper, we elaborate upon the treatment of aggregates during grounding in Gringo
series 4. Consequently, our approach is applicable to grounding based on
semi-naive database evaluation techniques. In particular, we provide a series
of algorithms detailing the treatment of recursive aggregates and illustrate
this by a running example.
</summary>
    <author>
      <name>Martin Gebser</name>
    </author>
    <author>
      <name>Roland Kaminski</name>
    </author>
    <author>
      <name>Torsten Schaub</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 7 figures, preliminary version appeared at GTTV'15</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03827v1</id>
    <updated>2016-03-12T00:02:51Z</updated>
    <published>2016-03-12T00:02:51Z</published>
    <title>Sequential Short-Text Classification with Recurrent and Convolutional
  Neural Networks</title>
    <summary>  Recent approaches based on artificial neural networks (ANNs) have shown
promising results for short-text classification. However, many short texts
occur in sequences (e.g., sentences in a document or utterances in a dialog),
and most existing ANN-based systems do not leverage the preceding short texts
when classifying a subsequent one. In this work, we present a model based on
recurrent neural networks and convolutional neural networks that incorporates
the preceding short texts. Our model achieves state-of-the-art results on three
different datasets for dialog act prediction.
</summary>
    <author>
      <name>Ji Young Lee</name>
    </author>
    <author>
      <name>Franck Dernoncourt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a conference paper at NAACL 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03814v1</id>
    <updated>2016-03-11T22:54:28Z</updated>
    <published>2016-03-11T22:54:28Z</published>
    <title>Solving MaxSAT by Successive Calls to a SAT Solver</title>
    <summary>  The Maximum Satisfiability (MaxSAT) problem is the problem of finding a truth
assignment that maximizes the number of satisfied clauses of a given Boolean
formula in Conjunctive Normal Form (CNF). Many exact solvers for MaxSAT have
been developed during recent years, and many of them were presented in the
well-known SAT conference. Algorithms for MaxSAT generally fall into two
categories: (1) branch and bound algorithms and (2) algorithms that use
successive calls to a SAT solver (SAT- based), which this paper in on. In
practical problems, SAT-based algorithms have been shown to be more efficient.
This paper provides an experimental investigation to compare the performance of
recent SAT-based and branch and bound algorithms on the benchmarks of the
MaxSAT Evaluations.
</summary>
    <author>
      <name>Mohamed El Halaby</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Survey, 46 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03795v1</id>
    <updated>2016-03-11T21:36:27Z</updated>
    <published>2016-03-11T21:36:27Z</published>
    <title>Demonstrating the Feasibility of Automatic Game Balancing</title>
    <summary>  Game balancing is an important part of the (computer) game design process, in
which designers adapt a game prototype so that the resulting gameplay is as
entertaining as possible. In industry, the evaluation of a game is often based
on costly playtests with human players. It suggests itself to automate this
process using surrogate models for the prediction of gameplay and outcome. In
this paper, the feasibility of automatic balancing using simulation- and
deck-based objectives is investigated for the card game top trumps.
Additionally, the necessity of a multi-objective approach is asserted by a
comparison with the only known (single-objective) method. We apply a
multi-objective evolutionary algorithm to obtain decks that optimise
objectives, e.g. win rate and average number of tricks, developed to express
the fairness and the excitement of a game of top trumps. The results are
compared with decks from published top trumps decks using simulation-based
objectives. The possibility to generate decks better or at least as good as
decks from published top trumps decks in terms of these objectives is
demonstrated. Our results indicate that automatic balancing with the presented
approach is feasible even for more complex games such as real-time strategy
games.
</summary>
    <author>
      <name>Vanessa Volz</name>
    </author>
    <author>
      <name>Günter Rudolph</name>
    </author>
    <author>
      <name>Boris Naujoks</name>
    </author>
    <link href="http://arxiv.org/abs/1603.03795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03518v2</id>
    <updated>2016-03-21T02:06:09Z</updated>
    <published>2016-03-11T04:50:59Z</published>
    <title>High-dimensional Black-box Optimization via Divide and Approximate
  Conquer</title>
    <summary>  Divide and Conquer (DC) is conceptually well suited to high-dimensional
optimization by decomposing a problem into multiple small-scale sub-problems.
However, appealing performance can be seldom observed when the sub-problems are
interdependent. This paper suggests that the major difficulty of tackling
interdependent sub-problems lies in the precise evaluation of a partial
solution (to a sub-problem), which can be overwhelmingly costly and thus makes
sub-problems non-trivial to conquer. Thus, we propose an approximation
approach, named Divide and Approximate Conquer (DAC), which reduces the cost of
partial solution evaluation from exponential time to polynomial time.
Meanwhile, the convergence to the global optimum (of the original problem) is
still guaranteed. The effectiveness of DAC is demonstrated empirically on two
sets of non-separable high-dimensional problems.
</summary>
    <author>
      <name>Peng Yang</name>
    </author>
    <author>
      <name>Ke Tang</name>
    </author>
    <author>
      <name>Xin Yao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03518v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03518v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03515v1</id>
    <updated>2016-03-11T04:18:48Z</updated>
    <published>2016-03-11T04:18:48Z</published>
    <title>Dimension Coupling: Optimal Active Learning of Halfspaces via Query
  Synthesis</title>
    <summary>  In this paper, we consider the problem of actively learning a linear
classifier through query synthesis where the learner can construct artificial
queries in order to estimate the true decision boundaries. This problem has
recently gained a lot of interest in automated science and adversarial reverse
engineering for which only heuristic algorithms are known. In such
applications, queries can be constructed de novo to elicit information (e.g.,
automated science) or to evade detection with minimal cost (e.g., adversarial
reverse engineering).
  We develop a general framework, called dimension coupling (DC), that 1)
reduces a d-dimensional learning problem to d-1 low-dimensional sub-problems,
2) solves each sub-problem efficiently, and 3) appropriately aggregates the
results and outputs a linear classifier. We consider the three most common
scenarios in the literature: idealized noise-free, independent noise
realizations, and agnostic settings. We show that the DC framework avoids the
curse of dimensionality: its computational complexity in all three cases scales
linearly with the dimension. Moreover, in the noiseless and noisy cases, we
show that the query complexity of DC is near optimal (within a constant factor
of the optimum algorithm). We also develop an agnostic variant of DC for which
we provide strong theoretical guarantees. To further support our theoretical
analysis, we compare the performance of DC with the existing work in all three
settings. We observe that DC consistently outperforms the prior arts in terms
of query complexity while often running orders of magnitude faster.
</summary>
    <author>
      <name>Lin Chen</name>
    </author>
    <author>
      <name>Hamed Hassani</name>
    </author>
    <author>
      <name>Amin Karbasi</name>
    </author>
    <link href="http://arxiv.org/abs/1603.03515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03511v1</id>
    <updated>2016-03-11T03:22:12Z</updated>
    <published>2016-03-11T03:22:12Z</published>
    <title>A Set Theoretic Approach for Knowledge Representation: the
  Representation Part</title>
    <summary>  In this paper, we propose a set theoretic approach for knowledge
representation. While the syntax of an application domain is captured by set
theoretic constructs including individuals, concepts and operators, knowledge
is formalized by equality assertions. We first present a primitive form that
uses minimal assumed knowledge and constructs. Then, assuming naive set theory,
we extend it by definitions, which are special kinds of knowledge.
Interestingly, we show that the primitive form is expressive enough to define
logic operators, not only propositional connectives but also quantifiers.
</summary>
    <author>
      <name>Yi Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper targets an ambitious goal to rebuild a foundation of
  knowledge representation based on set theory rather than classical logic. Any
  comments are welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03491v1</id>
    <updated>2016-03-10T23:50:51Z</updated>
    <published>2016-03-10T23:50:51Z</published>
    <title>Bayesian Opponent Exploitation in Imperfect-Information Games</title>
    <summary>  The two most fundamental problems in computational game theory are computing
a Nash equilibrium and learning to exploit opponents given observations of
their play (aka opponent exploitation). The latter is perhaps even more
important than the former: Nash equilibrium does not have a compelling
theoretical justification in game classes other than two-player zero-sum, and
furthermore for all games one can potentially do better by exploiting perceived
weaknesses of the opponent than by following a static equilibrium strategy
throughout the match. The natural setting for opponent exploitation is the
Bayesian setting where we have a prior model that is integrated with
observations to create a posterior opponent model that we respond to. The most
natural, and a well-studied prior distribution is the Dirichlet distribution.
An exact polynomial-time algorithm is known for best-responding to the
posterior distribution for an opponent assuming a Dirichlet prior with
multinomial sampling in the case of normal-form games; however, for the case of
imperfect-information games the best known algorithm is a sampling algorithm
based on approximating an infinite integral without theoretical guarantees. The
main result is the first exact algorithm for accomplishing this in
imperfect-information games. We also present an algorithm for another natural
setting where the prior is the uniform distribution over a polyhedron.
</summary>
    <author>
      <name>Sam Ganzfried</name>
    </author>
    <link href="http://arxiv.org/abs/1603.03491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
